{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AxF11X9tHvo"
   },
   "source": [
    "\n",
    "## MACHINE LEARNING IN FINANCE\n",
    "MODULE 7 | LESSON 1\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4ceGgiItOf0"
   },
   "source": [
    "# **MODEL SELECTION AND HYPERPARAMETER TUNING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MELzJ6aRtXsV"
   },
   "source": [
    "\n",
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  50 minutes |\n",
    "|**Prior Knowledge** | Machine learning   |\n",
    "|**Keywords** |Hyperparameter, grid search, random search, Cross validation  |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mg7hqOaYt6Be"
   },
   "source": [
    "*In the previous modules, we have trained and evaluated the performance of machine learning models. Machine learning models have a number of parameters that the data learn; model parameters are found by fitting our model on the training dataset. Hyperparameters on the other hand cannot be learned by our normal training process and are fixed before we begin the training process. In this module, we will learn more about hyperparameters. In this lesson, we will begin by discussing how to select the best performing model and the various hyperparameter tuning strategies.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGjNe9H40SNf"
   },
   "source": [
    "## **1. Improving Model Performance**\n",
    "\n",
    "Before putting our machine learning model(s) to production, we should make sure that the model gives the best possible solution devoid of bias and can generalize well on new unseen data.\n",
    "\n",
    "We will cover the following topics in this section.\n",
    "- Overfitting and underfitting.\n",
    "- Train, validation, and test sets.\n",
    "- Cross-validation.\n",
    "\n",
    "We begin with data splitting into train, test, and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdUAMQRBupj9"
   },
   "source": [
    "### **1.1 Train, validation and tests sets**\n",
    "\n",
    "It is advisable to split the data into training, testing, and validation sets especially if we want to tune our hyperparameters. The validation set helps us get an unbiased evaluation of our model performance and can therefore tune our model hyperparameters before passing the model to the test dataset. \n",
    "\n",
    "Given a dataset, we can get the train, validation, and test set by using one of the following:\n",
    " - Random Split\n",
    " - Stratified Split\n",
    "\n",
    "Random split involves splitting the original data into pre-defined proportions representing the train, validation, and test sets by first shuffling the data so as to overcome the challenge of there existing a pattern based on the data indices. \n",
    "\n",
    "Stratified split splits the train, validation, and test sets according to the proportion of samples in the target variable. Stratified split is often used when our data is imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prHFENboAXbO"
   },
   "source": [
    "Let us see an example of this using the California housing dataset whose target variable is the house price median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pGFSN1gi1hkF"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "X, y = housing.data, housing.target * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "wx3PgdyO1jmU",
    "outputId": "452c818f-ead1-4777-fc7c-7ecd60e45b57"
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n8vpZH2y1jpS",
    "outputId": "23d143b5-4d2b-41ed-fa75-7cb71f2ead3f"
   },
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEdIrUSpjmEw"
   },
   "source": [
    "We will use the decision tree regressor to predict the median house price with an interest in getting the generalization of our model. So, we start by training the model on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qvd-MJgH1jsF",
    "outputId": "7bb33e40-5d0c-46db-b471-8e19689be924"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eE3ZCkfOj5wD"
   },
   "source": [
    "We also evaluate the model on the dataset to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wM1xwFHU1juo",
    "outputId": "b8a8fc20-53a1-4d94-9253-e4f492f64b6b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "target_predicted = regressor.predict(X)\n",
    "score = mean_absolute_error(y, target_predicted)\n",
    "print(f\"On average, our regressor makes an error of {score:.3f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5i-pnrgkB3C"
   },
   "source": [
    "The resulting mean absolute error is actually very impressive: it says that our model actually got the median price of each house accurately. So, the question is should we pick this model? The simple answer is not so fast! Remember we trained and evaluated our model on the same dataset and the decision tree regressor could have memorized the dataset and would therefore give us the wrong notion about its ability to generalize on unseen data.\n",
    "\n",
    "When training our model, we are interested in it minimizing the error on unseen data, and this is the idea behind us splitting the data into train and test sets.\n",
    "\n",
    "So, we can begin by splitting our dataset as shown below. Again, we train our model on the training set and evaluate its performance on the same data and record the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xn9cJPUy1jxN"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4b5atgQzW_1"
   },
   "source": [
    "We again train the model on our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzuzadUv1jz7",
    "outputId": "51e26ffb-5223-4600-f1f7-a9184590af0e"
   },
   "outputs": [],
   "source": [
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhHv0vOIzbmP"
   },
   "source": [
    "We predict the data on our training data and look at the resulting training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WMxysW3P1j2V",
    "outputId": "7feaeb9e-3e44-405a-9bdb-b4bd2d1e6520"
   },
   "outputs": [],
   "source": [
    "y_predicted = regressor.predict(X_train)\n",
    "score = mean_absolute_error(y_train, y_predicted)\n",
    "print(f\"The training error of our model is {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsOnhnZZlngs"
   },
   "source": [
    "The output tells us that the model was able to accurately predict the actual house value. To check the validity of the model's strength, we use the test data, which has not been 'seen' by the model, to predict the house median price.\n",
    "<span style='color: transparent; font-size:1%'>All rights reserved WQU WorldQuant University QQQQ</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nYG2TxKk2XXP",
    "outputId": "16ed25b7-6ceb-418b-e584-10439768f6aa"
   },
   "outputs": [],
   "source": [
    "y_predicted = regressor.predict(X_test)\n",
    "score = mean_absolute_error(y_test, y_predicted)\n",
    "print(f\"The testing error of our model is {score:.3f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nn1fuAr6lypx"
   },
   "source": [
    "We now see that there is a bigger error on the testing dataset as compared to the training set and hence signs that our model is *overfitting*. To avoid issues with overfitting, we can use cross-validation that splits the data into smaller chunks iteratively so that it covers the entire train data, which is alternatively split into train and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6IaUjNFjC5C"
   },
   "source": [
    "### **1.2 Cross-Validation**\n",
    "\n",
    "In this technique, the train data is split into train and validation sets multiple times, and each split is called a **Fold**. In this subsection, we will discuss the k-fold cross-validation.\n",
    "\n",
    "K-fold cross-validation splits the data into $k$ segments where $k-1$ segments are used for training and the remaining segment used for validation as shown in the diagram below.\n",
    "\n",
    "**Fig 1: K-Fold Cross Validation**\n",
    "\n",
    "<center><img src=\"https://ars.els-cdn.com/content/image/1-s2.0-S2666827021000906-gr5.jpg\n",
    "\" alt=\"DDSP Tone Transfer\" width=\"500\"></center>\n",
    "\n",
    "##### Source: [Srivastava, Amiy, et al. \"Ensemble Prediction of Mean Bubble Size in a Continuous Casting Mold Using Data Driven Modeling Techniques.\" *Machine Learning with Applications*, vol. 6, 2021.](https://www.sciencedirect.com/science/article/pii/S2666827021000906)\n",
    "\n",
    "\n",
    "Below are the steps involved in performing a cross-validation:\n",
    "\n",
    "1. Shuffle the dataset.\n",
    "2. Hold out the test data.\n",
    "3. Perform k-fold cross validation on the train dataset.\n",
    "4. Get the best evaluation score, which is the mean of all the folds.\n",
    "5. Perform model evaluation on the test data.\n",
    "\n",
    "Cross-validation allows us to estimate the robustness of our model by splitting the dataset repetitively a given number of times.\n",
    "\n",
    "The next cells demonstrate how to apply cross-validation on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-_EXTPz2XaO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit, cross_validate\n",
    "\n",
    "cv = ShuffleSplit(n_splits=40, test_size=0.3, random_state=0)\n",
    "cv_results = cross_validate(regressor, X, y, cv=cv, scoring=\"neg_mean_absolute_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0B8M-Kcc0JA"
   },
   "source": [
    "We convert the `cv_results` Python dictionary into a dataframe for easier visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Gj22hQxy2XdA",
    "outputId": "2c77e7fb-fafa-426b-9aa0-794222eac587"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1wnQjbtdNZX"
   },
   "source": [
    "We negate the `test_score` to get the actual test errors because the parameter `scoring=\"neg_mean_absolute_error\"` gives negative values of the mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNhpxi7f2Xfo"
   },
   "outputs": [],
   "source": [
    "cv_results[\"test_error\"] = -cv_results[\"test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "xBu3dD-m2Xh-",
    "outputId": "cf9b4e6e-511a-4fdc-96bc-c831f0ec31f8"
   },
   "outputs": [],
   "source": [
    "cv_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKn4-9uTdulG"
   },
   "source": [
    "The plot below shows how the testing errors are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "5buz_Czo2zc0",
    "outputId": "b78f93e6-f717-4fa1-d278-891cf1b3cd20"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(cv_results[\"test_error\"], density=True)\n",
    "# cv_results[\"test_error\"].plot.hist(bins=10, edgecolor=\"black\")\n",
    "plt.xlabel(\"Mean absolute error\")\n",
    "plt.ylabel(\"Frequency Percentage %\")\n",
    "plt.suptitle(\n",
    "    \"Fig. 2: Test Errors Distribution.\", fontweight=\"bold\", horizontalalignment=\"right\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_PKwkJGi2zfm",
    "outputId": "c1418c6b-11b9-4c90-bcfe-4fcfb711b08a"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"The mean cross-validated testing error is: \"\n",
    "    f\"{cv_results['test_error'].mean():.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glnX7VTNOdfn"
   },
   "source": [
    "The error has reduced when compared to the initial error without using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_XplH4mn27Ip",
    "outputId": "248f625b-ca1b-4c3d-b04b-4a5eff418cf6"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"The standard deviation of the testing error is: \"\n",
    "    f\"{cv_results['test_error'].std():.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6KEDvRrd4D1"
   },
   "source": [
    "Cross-validation reduces the error on our model albeit at a smaller fraction as compared to the case when we did not use cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_EeoHpU0v4H"
   },
   "source": [
    "A problem tends to occur in our bid to improve model performance: we can find ourselves solving one problem (underfitting) while introducing another problem (overfitting). In the next subsection, let's discuss the overfitting and underfitting concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykKnsSJ6j0OR"
   },
   "source": [
    "### **1.3 Overfitting and Underfitting**\n",
    "\n",
    "When the model memorizes the train data, then we say overfitting has occurred. In such a case, we get a high evaluation score on our train dataset but a not-so-impressive score on the test data set; therefore, the model fails to generalize well on unseen data.\n",
    "\n",
    "Models that overfit respond to random noise in the training set and thus become sensitive to small fluctuations in the data. Overfitting is a combination of low bias and high variance.\n",
    "\n",
    "Underfitting on the other hand is characterized by a low performance score on both the training dataset and the validation dataset. Underfitting is caused by simple algorithms that are unable to learn complex patterns existing in our data. Underfitting is also referred to as high bias and often low variance.\n",
    "\n",
    "A right fit model neither underfits nor overfits, and the model performance in the training set is comparable to the test set. Linear machine learning algorithms such as logistic regression tend to underfit on the dataset while non-linear models like polynomial regression tend to overfit a dataset.\n",
    "\n",
    "In case our model underfits, to improve its performance, we can\n",
    "1. Use non-linear algorithms.\n",
    "2. Increase the complexity of the model by tweaking its parameters.\n",
    "3. Use non-parameterized algorithms.\n",
    "\n",
    "To curb overfitting of our models, we\n",
    "1. Increase the data size used for training to improve the generalizing ability of the model.\n",
    "2. Use regularization techniques like the L1 and L2 norms.\n",
    "3. Tune the hyperparameters of our model.\n",
    "4. Reduce the number of features in our dataset.\n",
    "5. Reduce the complexity of the model.\n",
    "6. Apply cross-validation techniques. \n",
    "\n",
    "**Fig 3: Overfitting and Underfitting Examples**\n",
    "\n",
    "<center><img src=\"https://www.researchgate.net/publication/339680577/figure/fig2/AS:865364518924290@1583330387982/llustration-of-the-underfitting-overfitting-issue-on-a-simple-regression-case-Data.png\" alt=\"DDSP Tone Transfer\" width=\"500\"></center>\n",
    "\n",
    "##### Source: [Badillo, Solveig, et al. \"An Introduction to Machine Learning.\" *Clinical Pharmacology & Therapeutics*, vol. 107, no. 4, 2020, pp. 871-885.](https://ascpt.onlinelibrary.wiley.com/doi/full/10.1002/cpt.1796)\n",
    "\n",
    "In the next cells, we evaluate our model and determine whether it is a good fit, underfitting or overfitting on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udI1YStmj-9Z"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import ShuffleSplit, cross_validate\n",
    "\n",
    "cv = ShuffleSplit(n_splits=30, test_size=0.2)\n",
    "cv_results = cross_validate(\n",
    "    regressor,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    return_train_score=True,\n",
    "    n_jobs=2,\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63RFmWAslNFc"
   },
   "source": [
    "As discussed in the previous subsection, we negate the scores to get the actual train and test errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IO_C8Tebj_DR"
   },
   "outputs": [],
   "source": [
    "scores = pd.DataFrame()\n",
    "scores[[\"train error\", \"test error\"]] = -cv_results[[\"train_score\", \"test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Izn5DC4leqT"
   },
   "source": [
    "The next cell plots the distribution of the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "OKtU6Ff3j_Hw",
    "outputId": "4520a5e2-84e4-45cd-bba1-4322a0bfc321"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scores.plot.hist(bins=50, edgecolor=\"black\")\n",
    "plt.xlabel(\"Mean absolute error\")\n",
    "plt.suptitle(\n",
    "    \"Fig. 4: Train and Test Errors Distribution via Cross-validation.\",\n",
    "    fontweight=\"bold\",\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P381TCH0luyg"
   },
   "source": [
    "We can observe that the training error is zero, implying that our model is not underfitting. The test error is larger, and therefore, our model seems to have memorized the noise in our dataset. Hence, its generalization abilities have been compromised.\n",
    "\n",
    "One of the challenges with hyperparameter tuning is that we can easily shift from a case of a model underfitting into another case of the model overfitting on our dataset. To overcome such challenges, we need to consider **validation curves**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xft3oju4XNj"
   },
   "source": [
    "### **1.4 Validation Curves**\n",
    "\n",
    "The validation curve varies the values of the hyperparameter(s) and gives the intuition of how the model behaves given different hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1i5YF_T4fTQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "max_depth = [1, 3, 5, 10, 15, 20, 25, 30]\n",
    "train_scores, test_scores = validation_curve(\n",
    "    regressor,\n",
    "    X,\n",
    "    y,\n",
    "    param_name=\"max_depth\",\n",
    "    param_range=max_depth,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=2,\n",
    ")\n",
    "train_errors, test_errors = -train_scores, -test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVDjVIc1ovQZ"
   },
   "source": [
    "In the example, we will restrict ourselves to one of the hyperparameters, the `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "D97wkXFc4nXE",
    "outputId": "910b6f5e-749e-4c07-efb7-e9bb2c88ae3e"
   },
   "outputs": [],
   "source": [
    "plt.plot(max_depth, train_errors.mean(axis=1), label=\"Training error\")\n",
    "plt.plot(max_depth, test_errors.mean(axis=1), label=\"Testing error\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"Maximum depth of decision tree\")\n",
    "plt.ylabel(\"Mean absolute error\")\n",
    "plt.suptitle(\n",
    "    \"Fig. 5: Validation Curve for Decision Tree\",\n",
    "    fontweight=\"bold\",\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAqqK_AUo9vz"
   },
   "source": [
    "From the above plot, we see that for lower values of maximum depth, the model seems to underfit while as the maximum depth of trees increases, the model overfits on our dataset. So, for the values of 10, we can say that our model will provide a good fit and hence better generalization on an unseen dataset. The plot below repeats the above plot but on training errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "u5SbuFGd4ner",
    "outputId": "6a037a80-7f6f-4bba-d112-68ee5da4e5d0"
   },
   "outputs": [],
   "source": [
    "plt.errorbar(\n",
    "    max_depth,\n",
    "    train_errors.mean(axis=1),\n",
    "    yerr=train_errors.std(axis=1),\n",
    "    label=\"Training error\",\n",
    ")\n",
    "plt.errorbar(\n",
    "    max_depth,\n",
    "    test_errors.mean(axis=1),\n",
    "    yerr=test_errors.std(axis=1),\n",
    "    label=\"Testing error\",\n",
    ")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"Maximum depth of decision tree\")\n",
    "plt.ylabel(\"Mean absolute error\")\n",
    "plt.suptitle(\n",
    "    \"Fig. 6: Validation Curve for Decision Tree using Train Errors\",\n",
    "    fontweight=\"bold\",\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5nLqflL5R7N"
   },
   "source": [
    "Our target when running machine learning models is to get an unbiased estimate of our errors. This is possible when we understand how the concept of bias-variance tradeoff relates to underfitting and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7hQsXdq6rYc"
   },
   "source": [
    "### **1.5 Bias-Variance Tradeoff**\n",
    "\n",
    "We can divide the prediction errors from machine learning algorithms into two:\n",
    "- Reducible parts\n",
    "- Irreducible parts.\n",
    "\n",
    "The irreducible parts are caused by stochastic noise as a result of missing important features or measurement errors and can't be reduced even with very good models. The reducible part, on the other hand, constitutes errors due to:\n",
    "- Bias\n",
    "- Variance\n",
    "\n",
    "**Error due to Bias** - Bias is the difference between our predicted values and the actual values we are trying to predict. These errors occur when the algorithm is too simple (less complex) to capture the functional relationship between our features. Models having high bias tend to oversimplify the model by not learning a lot from the training data. This causes the model to make some systematic mistakes, and therefore, the predictions will be biased. This is also referred to as underfitting.\n",
    "\n",
    "**Error due to Variance** - We define variance as the measure of spread in our dataset. These errors occur when the algorithm is so complex that it extracts patterns from the noise in the dataset. Models with high variance perform very well on training data but do not generalize well on unseen data. This is what we will refer to as overfitting, as discussed above.\n",
    "\n",
    "So, we see two issues here:\n",
    "1. If our model is too simple and has few parameters, then there is a tendency that we risk having a model with high bias and low variance.\n",
    "2. If our model is too complex with many parameters, then it is highly likely the model will overfit on the dataset due to high variance and low bias on the dataset.\n",
    "\n",
    "This is what introduces the bias-variance tradeoff since it is impossible to have a model that will be both complex and simple at the same time.\n",
    "\n",
    "Building accurate models requires understanding these errors so as to avoid our model underfitting or overfitting. \n",
    "\n",
    "In the next few cells, we will demonstrate the concept of bias and variance tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qdHqqrA4aZ-I",
    "outputId": "c89c7e88-a74b-49cf-fc52-0813dbdbd1df"
   },
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import bias_variance_decomp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyObT-D1jRZc"
   },
   "source": [
    "We use the data as above and the same algorithm to demonstrate the concept of bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ic9gWZkQdSXH"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "X, y = housing.data.values, housing.target.values * 10\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YF-fshzjh2S"
   },
   "source": [
    "In the next cell, we will define the hyperparameter that will define an increase in model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mv4MduAYY6F9"
   },
   "outputs": [],
   "source": [
    "max_levels = list(range(1, 50))\n",
    "levels = []\n",
    "squared_bias_plus_variance = []\n",
    "for level in max_levels:\n",
    "    model = DecisionTreeRegressor(max_depth=level)\n",
    "    model.fit(X_train, y_train)\n",
    "    mse, bias, var = bias_variance_decomp(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        loss=\"mse\",\n",
    "        num_rounds=200,\n",
    "        random_seed=1,\n",
    "    )\n",
    "    score = model.score(X_test, y_test)\n",
    "    squared_bias_plus_variance.append(bias**2 + var)\n",
    "    levels.append(level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzYlvyTDizoM"
   },
   "source": [
    "In the next cell, we plot the bias-variance tradeoff chart that shows how an increase in model complexity affects bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qs9jcMM9Y71H",
    "outputId": "256d38d1-7ddb-4a1c-a9f9-7c4d8a82ce19"
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "scatter = go.Scatter(x=levels, y=squared_bias_plus_variance)\n",
    "layout = go.Layout(\n",
    "    title=\"Fig. 7. Bias variance tradeoff\",\n",
    "    xaxis=dict(title=\"levels\"),\n",
    "    yaxis=dict(title=\"bias^2+variance\"),\n",
    ")\n",
    "go.Figure(data=[scatter], layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPen1B8RjwNb"
   },
   "source": [
    "It can be seen that as the model complexity increases, the error reduces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qi4notRdYk8E"
   },
   "source": [
    "Now that we have a good understanding of how to improve our model performance, we can go ahead and learn how to tweak the hyperparameters of our models.\n",
    "\n",
    "## **2. Hyperparameter Tuning**\n",
    "\n",
    "Hyperparameter tuning involves searching over a given set of values and finding the optimal values of a hyperparameter in a machine learning model. We expect that after performing hyperparameter tuning, the result of the model training will give us a better score without overfitting on the dataset.\n",
    "\n",
    "Now let's highlight the difference between a parameter and a hyperparameter.\n",
    "\n",
    "While a parameter is learned from the data during model training, a hyperparameter is set by the user before we begin training our model. \n",
    "\n",
    "An example of a parameter is the coefficients we obtain from training a linear regression model while the number of epochs and batch size are examples of hyperparameters. We should note that some models like linear regression only have parameters while others like the K-Nearest Neighbors (KNN) contain only hyperparameters.\n",
    "\n",
    "Before commencing on to hyperparameter tuning, we first define the **hyperparameter space**, which is the set of all possible hyperparameter values. Hyperparameters can either be discrete or continuous values. When defining the hyperparameter space, we also define their underlying distribution.\n",
    "\n",
    "In the next subsections of this lesson, we intend to discuss the various types of hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hUWQu6abh1G"
   },
   "source": [
    "### **2.1 Manual Search**\n",
    "\n",
    "The manual search method entails defining the search space and looping all the possible hyperparameters to get a combination that will give us the best outcome. We follow these steps when performing manual search:\n",
    "\n",
    "1. Split the train data into train and test splits.\n",
    "2. Define the initial hyperparameter values.\n",
    "3. Perform k-fold cross validation on the training data.\n",
    "4. Get the validation score.\n",
    "5. Choose another set of hyperparameter values.\n",
    "6. Repeat steps 3-5 until we get satisfactory performance.\n",
    "7. Train the model on the full train set with the chosen hyperparameter value.\n",
    "8. Evaluate this performance on the test set.\n",
    "\n",
    "In the example below, we show how we can get and set the value(s) of model hyperparameters. We will consider the [blood transfusion data](https://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eb8QzR_U1dBC"
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dIV4t3eIbDV"
   },
   "source": [
    "We load the data and display the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FJjYYRXT1n8N",
    "outputId": "809d633f-d341-429b-e7e0-027b2cf19909"
   },
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data\"\n",
    "data_df = pd.read_csv(url)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7z-oXKwRIm8O"
   },
   "source": [
    "The next step involves renaming the columns for easier handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8T9ObqdI14eM"
   },
   "outputs": [],
   "source": [
    "cols = [c.lower().split()[0] for c in data_df.columns]\n",
    "cols[-1] = \"class_name\"\n",
    "data_df.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SR_uTMYi14he",
    "outputId": "65c8be8c-a455-459c-ffb6-b119aedfc805"
   },
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBGBvoGZJDS_"
   },
   "source": [
    "We then define the target class that will set the stage for the type of machine learning problem we are supposed to solve. In our case, we are presented with a binary classification problem that we will not delve into too much as we are only interested in learning the hyperparameter tuning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EH5ZlUkp2ors"
   },
   "outputs": [],
   "source": [
    "target_name = \"class_name\"\n",
    "feature_columns = [\"recency\", \"frequency\", \"monetary\"]\n",
    "\n",
    "target = data_df[target_name]\n",
    "data = data_df[feature_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ct1OJjtCOJb5"
   },
   "source": [
    "We transform the data by rescaling it using `StandardScaler` that removes the mean of the feature and scales to a unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UbqhYzux2ouP"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "model = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", StandardScaler()),\n",
    "        (\"classifier\", HistGradientBoostingClassifier(random_state=42)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JMXaye1cNo3"
   },
   "source": [
    "As discussed in the previous section, we evaluate the performance of the model using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "myrK1bsW2oxo",
    "outputId": "d00cf3f9-0cfc-4995-8613-5ad5d4298d5f"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = cross_validate(model, data, target)\n",
    "scores = cv_results[\"test_score\"]\n",
    "print(\n",
    "    f\"Accuracy score via cross-validation:\\n\"\n",
    "    f\"{scores.mean():.3f} ± {scores.std():.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JL-2VzksdnK6"
   },
   "source": [
    "In the demonstrations that will follow in this lesson, we will try to tune the hyperparameters `learning rate` and `max_leaf_nodes` of the `HistGradientBoostingClassifier` model. We start first by displaying their default values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ILpFYVS6dkxy",
    "outputId": "71c432b4-b4c9-4c9d-c643-7df4bcb3b1d7"
   },
   "outputs": [],
   "source": [
    "print(\"learning rate default value\", model.get_params()[\"classifier__learning_rate\"])\n",
    "print(\"max_leaf_nodes default value\", model.get_params()[\"classifier__max_leaf_nodes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhDqa--8Ysoj"
   },
   "source": [
    "Without specifying the value of the hyperparameters, the model uses the default parameters, for example, the default value for C is 1. We can manually tune the hyperparameter, in this case, change the value of C as shown in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CO7JygpR3V66",
    "outputId": "035a4ad4-3d05-41b8-fe3a-0e7d3af430bb"
   },
   "outputs": [],
   "source": [
    "model.set_params(classifier__learning_rate=1e-3)\n",
    "model.set_params(classifier__max_leaf_nodes=20)\n",
    "cv_results = cross_validate(model, data, target)\n",
    "scores = cv_results[\"test_score\"]\n",
    "print(\n",
    "    f\"Model accuracy score with cross-validation:\\n\"\n",
    "    f\"{scores.mean():.3f} ± {scores.std():.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtDMP3fiaJI1"
   },
   "source": [
    "The hyperparameters are stored in `model.get_params` and we can explore them one by one. For demonstration purposes we will consider tuning the hyperparameter `C` in the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yv2ds16ncj9r"
   },
   "source": [
    "The cell below displays the values of the hyperparameters we have set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MkIGoVpR3WAz",
    "outputId": "66027d73-9a06-4895-b886-75fa520715d7"
   },
   "outputs": [],
   "source": [
    "print(\"learning rate default value\", model.get_params()[\"classifier__learning_rate\"])\n",
    "print(\"max_leaf_nodes default value\", model.get_params()[\"classifier__max_leaf_nodes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oK2l2l2jZSH"
   },
   "source": [
    "In the next cell, we manually set a couple of values for C and choose the one that gives us better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5I40BFIX3r5W",
    "outputId": "1903222d-54ea-4b45-cedb-c8b65618a998"
   },
   "outputs": [],
   "source": [
    "for learning_rate in [1e-3, 1e-2, 1e-1, 1, 10]:\n",
    "    for max_leaf_nodes in [3, 5, 10, 15, 20, 30]:\n",
    "        model.set_params(classifier__learning_rate=learning_rate)\n",
    "        model.set_params(classifier__max_leaf_nodes=max_leaf_nodes)\n",
    "        cv_results = cross_validate(model, data, target)\n",
    "        scores = cv_results[\"test_score\"]\n",
    "        print(\n",
    "            f\"Model accuracy score with cross-validation:\\n learning_rate={learning_rate} and max_leaf_nodes={max_leaf_nodes}:\\n\"\n",
    "            f\"{scores.mean():.3f} ± {scores.std():.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BU3vqT4aWIiT"
   },
   "source": [
    "From the cell above, we see that the value of `C=0.01` gives better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbYTz7fiNG4d"
   },
   "source": [
    "### **2.2 Grid Search**\n",
    "\n",
    "Given a hyperparameter space, grid search looks for all the possible combinations that improve the performance of the model. In this method, the underlying distribution of the hyperparameters need not be known.\n",
    "\n",
    "So, the steps involved in grid search are as follows.\n",
    "1. Split the data into train and test sets.\n",
    "2. Define the hyperparameter space.\n",
    "3. Construct a nested loop of the number of hyperparameters.\n",
    "4. Perform cross validation on the train set and store the resulting score in each loop.\n",
    "5. Train the model on the train set and evaluate it on the test set.\n",
    "\n",
    "The output of the grid search will be equal to the output of the manual search since the model is evaluated at each pair of hyperparameters in our grid.\n",
    "\n",
    "Just like in the manual tuning section above, we demonstrate how to apply grid search in hyperparameter tuning.\n",
    "\n",
    "We start by splitting our dataset into training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nudvsaiFbFaS"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6voqor_lH7Tw"
   },
   "source": [
    "We use the histogram gradient classifier model, which is a tree-based classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WLFcg6hCPLGp",
    "outputId": "328681e3-b485-40cf-ffe6-adc99d1b036d"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "model = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", StandardScaler()),\n",
    "        (\"classifier\", HistGradientBoostingClassifier(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10g4Qok0Q9w_"
   },
   "source": [
    "Grid search is costly as it finds a combination of all the hyperparameters in our hyperparameter space. In this example, we share only limited nodes, and we choose to select only two hyperparameters to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Sfbu_49PLJO",
    "outputId": "6e3ad4fd-acc5-4567-dca5-56427f0351f2"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"classifier__learning_rate\": (0.001, 0.01, 0.1, 1, 10),\n",
    "    \"classifier__max_leaf_nodes\": (3, 5, 10, 15, 20, 30),\n",
    "}\n",
    "model_grid_search = GridSearchCV(model, param_grid=param_grid, n_jobs=2, cv=2)\n",
    "model_grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqcHuuXjRzHR"
   },
   "source": [
    "We then evaluate the accuracy of our model using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dZaBaMyAPLLq",
    "outputId": "c46746b4-c385-4da6-c99a-13a23c19ddce"
   },
   "outputs": [],
   "source": [
    "accuracy = model_grid_search.score(X_test, y_test)\n",
    "print(f\"The test accuracy score of the grid-searched pipeline is: \" f\"{accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "146Xj_BbSojb"
   },
   "source": [
    "The grid search takes the hyperparameter space with all the hyperparameter values in it and finds all the possible combinations of the hyperparameters. Like in our case where we have four learning_rate and max_leaf_nodes values, there will be $4\\times 4 = 16$ combination of hyperparameters, and we select the pair that gives us the best result. This makes the process expensive because for any value we add, the process becomes more and more computationally unsustainable. We can actually see that Grid Search is similar to the manual search we saw in the previous subsection.\n",
    "\n",
    "From the selected hyperparameters, we can now predict the first five targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P9TRKjzHPLPD",
    "outputId": "284b4946-e56c-4384-bf69-e8c73962add4"
   },
   "outputs": [],
   "source": [
    "model_grid_search.predict(X_test.iloc[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j42uf5UoUSof"
   },
   "source": [
    "We now visualize the original values in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Un_7LWOzd35y",
    "outputId": "b8219efd-3883-473b-9205-8730086cc03d"
   },
   "outputs": [],
   "source": [
    "y_test.iloc[0:5].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnR56hywUdV7"
   },
   "source": [
    "By using the `best_params_` attribute, we can see the selected hyperparameters that gives us the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "muZf7rGgd38L",
    "outputId": "c4663125-a49d-4425-f307-9edb01fa319e"
   },
   "outputs": [],
   "source": [
    "print(f\"The best set of parameters is: \" f\"{model_grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyC4YMDyasz7"
   },
   "source": [
    "We inspect all the values stored in cv_results_ using a dataframe as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GR8GchS6d3-x",
    "outputId": "a7c41e84-45b4-48ef-83dc-d73aa7758306"
   },
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(model_grid_search.cv_results_).sort_values(\n",
    "    \"mean_test_score\", ascending=False\n",
    ")\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAy0YVHHbtP0"
   },
   "source": [
    "We shorten the headers to make the dataframe more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EaKrAIapd4BM"
   },
   "outputs": [],
   "source": [
    "# get the parameter names\n",
    "column_results = [f\"param_{name}\" for name in param_grid.keys()]\n",
    "column_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n",
    "cv_results = cv_results[column_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xSO9N9svd4Et",
    "outputId": "9733f242-d64b-4edb-a304-fb0b5b4a4aa8"
   },
   "outputs": [],
   "source": [
    "def shorten_param(param_name):\n",
    "    if \"__\" in param_name:\n",
    "        return param_name.rsplit(\"__\", 1)[1]\n",
    "    return param_name\n",
    "\n",
    "\n",
    "cv_results = cv_results.rename(shorten_param, axis=1)\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIKm7n5vcA0X"
   },
   "source": [
    "Since we are only interested in our two hyperparameters, the table below displays the model accuracy for each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DeS3JAwYgair",
    "outputId": "b4513cbb-cbfb-41b3-c3f7-9cda3b219eb4"
   },
   "outputs": [],
   "source": [
    "pivoted_cv_results = cv_results.pivot_table(\n",
    "    values=\"mean_test_score\", index=[\"learning_rate\"], columns=[\"max_leaf_nodes\"]\n",
    ")\n",
    "\n",
    "pivoted_cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_8pHlo3ctaH"
   },
   "source": [
    "We can visualize the table above in a heatmap as shown in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZ1cMNxOgamH",
    "outputId": "4754f5c0-b620-4137-9ab2-86cda4f384cf"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.heatmap(pivoted_cv_results, annot=True, cmap=\"YlGnBu\", vmin=0.0, vmax=0.9)\n",
    "ax.invert_yaxis()\n",
    "plt.suptitle(\n",
    "    \"Fig. 8: Heatmap of the Model Results\",\n",
    "    fontweight=\"bold\",\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reKNUmJj1Uoa"
   },
   "source": [
    "The biggest challenge we have with the two methods we have seen above is that they are very computationally intensive especially when our hyperparameter space is large. In the next subsection, we will discuss the random search method that tries to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KowRvMscohmF"
   },
   "source": [
    "### **2.3 Random Search**\n",
    "\n",
    "This method randomly selects the hyperparameter for each iteration. It outperforms the grid search when the data is small or we have no idea of the range of hyperparameter space. It is also much more efficient when compared to grid search. In random search, the hyperparameter distribution must be specified together with the hyperparameter space, which is wider as compared to the grid search case.\n",
    "\n",
    "The steps involved in random search are:\n",
    "1. Split the data into train and test sets.\n",
    "2. Define the number of trials and set a random seed.\n",
    "3. Define the hyperparameter space with its distribution.\n",
    "4. Define an iterator consisting of random hyperparameter combinations.\n",
    "5. Loop through the iterator.\n",
    "6. After getting the best possible combination of hyperparameters, we train the model on the full train set and evaluate the model performance.\n",
    "\n",
    "\n",
    "In the following cells, we demonstrate the random search method for tuning the hyperparameters on our blood transfusion data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ov4ObCcgXtH"
   },
   "outputs": [],
   "source": [
    "model = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", StandardScaler()),\n",
    "        (\"classifier\", HistGradientBoostingClassifier(random_state=42)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8xFEwWaqBIK"
   },
   "source": [
    "We can view the default hyperparameter values in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lm3ey6VzgYDu",
    "outputId": "316f2365-3cf4-48c5-9c50-f317d5b3d288"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(\"Default parameters:\\n\")\n",
    "pprint(model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRC_3Dstqmgd"
   },
   "source": [
    "The next cell defines the hyperparameter search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hk5UOZPrgYJt"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform\n",
    "\n",
    "\n",
    "class loguniform_int:\n",
    "    \"\"\"Integer valued version of the log-uniform distribution\"\"\"\n",
    "\n",
    "    def __init__(self, a, b):\n",
    "        self._distribution = loguniform(a, b)\n",
    "\n",
    "    def rvs(self, *args, **kwargs):\n",
    "        \"\"\"Random variable sample\"\"\"\n",
    "        return self._distribution.rvs(*args, **kwargs).astype(int)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = {\n",
    "    \"classifier__learning_rate\": loguniform(1e-6, 1e3),\n",
    "    \"classifier__max_leaf_nodes\": loguniform_int(3, 100),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9b4M-Ifq7HO"
   },
   "source": [
    "We can now train the model on our training data using cross-validation that executes 100 iterations of 5 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "egUqnglSgYNi",
    "outputId": "0cd1fb04-ccc4-40b2-85ed-2b6c44a59089"
   },
   "outputs": [],
   "source": [
    "model_random_search = RandomizedSearchCV(\n",
    "    model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=10,\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True,\n",
    ")\n",
    "model_random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_5q7U1hrkpF"
   },
   "source": [
    "The random search method for hyperparameter tuning selects the hyperparameter combinations randomly as opposed to grid search where each value in our hyperparameter space was used explicitly. In random search, the choice of one hyperparameter value is independent from the other ones. As opposed to grid search, where our hyperparameter space consisted of hyperparameter values, random search allows the use of sampling distributions.\n",
    "\n",
    "The next cell displays the best parameters from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OHV27NpXgYS5",
    "outputId": "cc90d5e0-002f-4f31-f175-7deb74fe4ebf"
   },
   "outputs": [],
   "source": [
    "model_random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "im_6QchEsCWl"
   },
   "source": [
    "We evaluate our model performance on the test dataset; however, we do not expect good results as the hyperparameters were arbitrarily chosen for the purposes of demonstrating the random search method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BqX112dngYaV",
    "outputId": "9b68ba47-6cdf-45d4-e5ee-de90b194d645"
   },
   "outputs": [],
   "source": [
    "accuracy = model_random_search.score(X_test, y_test)\n",
    "\n",
    "print(f\"The test accuracy score of the best model is \" f\"{accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4QCwIRRsrbW"
   },
   "source": [
    "Just like the grid search process above, we inspect the cv results in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l9WJLtesnDAn",
    "outputId": "8224bdc7-8319-4076-fab8-f2499ef3e24d"
   },
   "outputs": [],
   "source": [
    "cv_results_rs = pd.DataFrame(model_random_search.cv_results_).sort_values(\n",
    "    \"mean_test_score\", ascending=False\n",
    ")\n",
    "cv_results_rs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1C7wI0mn5dT",
    "outputId": "3144b3c3-369d-473a-862a-b38a14f95ebb"
   },
   "outputs": [],
   "source": [
    "def shorten_param(param_name):\n",
    "    if \"__\" in param_name:\n",
    "        return param_name.rsplit(\"__\", 1)[1]\n",
    "    return param_name\n",
    "\n",
    "\n",
    "cv_results_rs = cv_results_rs.rename(shorten_param, axis=1)\n",
    "cv_results_rs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5faNdN4oSsU"
   },
   "source": [
    "The table below displays the pairs that the random search tuning picked by the random search algorithm. Since `n_iter=10`, random search will pick 10 pairs at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aSas8mpsnpFu",
    "outputId": "94e40c48-de8d-4c48-9bfa-6f3b028defcf"
   },
   "outputs": [],
   "source": [
    "pivoted_cv_results_rs = cv_results_rs.pivot_table(\n",
    "    values=\"mean_test_score\", index=[\"learning_rate\"], columns=[\"max_leaf_nodes\"]\n",
    ")\n",
    "\n",
    "pivoted_cv_results_rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjlJOlYAo3GX"
   },
   "source": [
    "We can observe that hyperparameter tuning using random search does not evaluate the model at all pairs but randomly selects the best possible combination among pairs. Since random search does not pick all possible pairs, it tends to be faster than manual search and grid search, and its accuracy is nearly equal to grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEJx7ZGMgS70"
   },
   "source": [
    "### **2.4 Bayesian Optimization**\n",
    "\n",
    "Bayesian optimization improves search space based on what it learns from the previous iterations. In this kind of hyperparameter selection, we first build the probability model of the objective function and then choose the optimal hyperparameters.\n",
    "\n",
    "There are several techniques of the Bayesian Optimization listed below.\n",
    "- Gaussian Process (GP).\n",
    "- Sequential model-based optimization\n",
    "- Tree Parzen Estimator (TPE)\n",
    "\n",
    "Bayesian optimization memorizes the model's past performance, which it uses to find the probability of a score given a parameter, that is, $P(\\text{score}|\\text{hyperparameters})$.\n",
    "\n",
    "The model can be optimized easily and used to find the next hyperparameter(s).\n",
    "\n",
    "The steps of a Bayesian optimization are:\n",
    "1. Find the probability model of a score given a hyperparameter.\n",
    "2. Find the optimal hyperparameters of the model in (1).\n",
    "3. Plug the hyperparameters into the objective function.\n",
    "4. Update the probability model given the new results.\n",
    "5. Repeat steps to 1 to 4 until we get a satisfactory result.\n",
    "\n",
    "We will discuss more on Bayesian optimization in lesson 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ybKRBzRuBvj"
   },
   "source": [
    "## **3. Conclusion**\n",
    "\n",
    "In this lesson, we have seen the various techniques for improving model performance and the impact of hyperparameters on model performance. We have also discussed how to automate the search for hyperparameters using grid search and randomized search. In the next lesson, we will apply these methods to a real-world example. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-X-AarjuF9Y"
   },
   "source": [
    "**References**\n",
    "\n",
    "1. Badillo, Solveig, et al. \"An Introduction to Machine Learning.\" *Clinical Pharmacology & Therapeutics*, vol. 107, no. 4, 2020, pp. 871-885.\n",
    "2. Srivastava, Amiy, et al. \"Ensemble Prediction of Mean Bubble Size in a Continuous Casting Mold Using Data Driven Modeling Techniques.\" *Machine Learning with Applications*, vol. 6, 2021.\n",
    "3. Yang, Li, and Abdallah Shami. \"On Hyperparameter Optimization of Machine Learning Algorithms: Theory and Practice.\" *Neurocomputing*, vol. 415, 2020, pp. 295-316."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "Copyright 2023 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 3000
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
