{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5AxF11X9tHvo"
   },
   "source": [
    "\n",
    "## MACHINE LEARNING IN FINANCE\n",
    "MODULE 7 | LESSON 2\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "J4ceGgiItOf0"
   },
   "source": [
    "# **HOUSE PREDICTION AND HYPERPARAMETER TUNING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MELzJ6aRtXsV"
   },
   "source": [
    "\n",
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  35 minutes |\n",
    "|**Prior Knowledge** | Grid Search, Random Search  |\n",
    "|**Keywords** |Grid search  |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "mg7hqOaYt6Be"
   },
   "source": [
    "*In the last lesson, we introduced hyperparameter tuning in machine learning and the merits and demerits of the various algorithms used. In this lesson, we will apply the skills learned to a house prediction exercise.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7CpsX5UM0hhb"
   },
   "source": [
    "## **1. Introduction**\n",
    "\n",
    "In this lesson we will use the housing data from the [Geo Data and Lab](https://geodacenter.github.io/data-and-lab/KingCounty-HouseSales2015/) website to predict the housing prices using Random Forest Regression and later on fine-tune its hyperparameters so as to improve the model performance.\n",
    "\n",
    "We begin by first loading the relevant packages below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "X7D-zucfwUeJ"
   },
   "outputs": [],
   "source": [
    "# data manipulation and plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from Scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "SOw2peO-2LjR"
   },
   "source": [
    "We then load the downloaded file to our working space and read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "ZIGryJ0Owao-"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"kc_house_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "deletable": false,
    "id": "TdtVJQxo1JEt",
    "outputId": "1f615bf0-8076-4c8a-f99a-35e4ac5da674"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dktJ9V012Y4B"
   },
   "source": [
    "Since the data is already processed, we will lightly touch on the exploratory data analysis phase and concentrate mostly on the modeling phase.\n",
    "\n",
    "Below we can see the size of the data and examine missing values.\n",
    "\n",
    "## **2. Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "98_WmTpDhuZw",
    "outputId": "45858b6b-3b31-410b-ae7c-8b2533323c63"
   },
   "outputs": [],
   "source": [
    "print(f\"The dataset contains {df.shape[0]} samples and \" f\"{df.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "ztZKor2khwz1",
    "outputId": "2ad184c4-eb9e-48a7-c36c-740e098b3936"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "69kzZPJp2unv"
   },
   "source": [
    "We then drop the id and date column from our data as they will not be useful for our prediction exercise. We also separate the target variable from the predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "P3Bn4RRqHV-o"
   },
   "outputs": [],
   "source": [
    "X = df.drop([\"id\", \"price\", \"date\"], axis=1)\n",
    "y = df[\"price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "oU_dCYif3qrq"
   },
   "source": [
    "The next step is to pick out categorical variables from numerical variables using column selector from `sklearn`'s compose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "Hdn6TA-FHLcf",
    "outputId": "bce99887-26ca-4a8e-efed-3eec3e192b0f"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "categorical_columns = categorical_columns_selector(X)\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "NqIgjt0LHLgB",
    "outputId": "521f5bf0-caf2-4143-885c-fc6e2a9374de"
   },
   "outputs": [],
   "source": [
    "# now let's identify the numerical variables\n",
    "\n",
    "num_vars = [var for var in X.columns if var not in categorical_columns]\n",
    "\n",
    "# number of numerical variables\n",
    "print(len(num_vars))\n",
    "print(num_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8iOdr3g035I6"
   },
   "source": [
    "We note that some categorical variables are numerical and we can put them into their correct list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "WiI7ejbIH7Lv",
    "outputId": "efb3dc64-f540-415f-8bff-15502582de6a"
   },
   "outputs": [],
   "source": [
    "#  let's make a list of discrete variables\n",
    "categorical_vars = [var for var in num_vars if len(X[var].unique()) < 20]\n",
    "\n",
    "\n",
    "print(\"Number of categorical variables: \", len(categorical_vars))\n",
    "print(categorical_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "deletable": false,
    "id": "CxmmVki3ID4k",
    "outputId": "091b14ed-17b9-4d7b-d7eb-3bd180d0158f"
   },
   "outputs": [],
   "source": [
    "# let's visualize the categorical variables\n",
    "\n",
    "X[categorical_vars].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Lp-0RMgU4QBa"
   },
   "source": [
    "Therefore, the remaining numerical variables will now be classified as continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "eyDjqDx1G-wJ",
    "outputId": "2bd75968-780c-4079-dd09-c5aa84a7b046"
   },
   "outputs": [],
   "source": [
    "# make list of continuous variables\n",
    "cont_vars = [var for var in num_vars if var not in categorical_vars]\n",
    "\n",
    "print(\"Number of continuous variables: \", len(cont_vars))\n",
    "print(cont_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "deletable": false,
    "id": "ZIO6CrU1G_E2",
    "outputId": "4941d518-f80b-4d6e-f319-ec9e4e6215b7"
   },
   "outputs": [],
   "source": [
    "# let's visualize the continuous variables\n",
    "\n",
    "X[cont_vars].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ebuWlhpc4mil"
   },
   "source": [
    "We now visualize the variables' distribution using histograms as shown below.<span style='color: transparent; font-size:1%'>All rights reserved WQU WorldQuant University QQQQ</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 971
    },
    "deletable": false,
    "id": "xiqnu0LbHiSC",
    "outputId": "5f0fd390-7204-4680-b960-3dd849ec9ee3"
   },
   "outputs": [],
   "source": [
    "# lets plot histograms for all continuous variables\n",
    "\n",
    "X[cont_vars].hist(bins=30, figsize=(15, 15))\n",
    "plt.suptitle(\"Fig. 1: Histogram Plot\", fontweight=\"bold\", horizontalalignment=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gYokXCEU4vdJ"
   },
   "source": [
    "We can now standardize the continuous variables and encode the discrete variables to eliminate possible bias towards larger values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "JnbrGxXjAMjZ"
   },
   "source": [
    "## **3. Modeling Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "bxujM1-kIo6B"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "numerical_preprocessor = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "md5ATXLlIp1S"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"one-hot-encoder\", categorical_preprocessor, categorical_vars),\n",
    "        (\"standard_scaler\", numerical_preprocessor, cont_vars),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "P-KSCzAw5IV_"
   },
   "source": [
    "We now create a pipeline of our regression model as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "nQ4Nm8BiIp41"
   },
   "outputs": [],
   "source": [
    "model = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"regressor\", RandomForestRegressor(random_state=42)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "deletable": false,
    "id": "240XNz3LJNaE",
    "outputId": "835e2584-88d2-40e3-c284-b69820685b3c"
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5HeBDJGR5a2e"
   },
   "source": [
    "The data is now split into training data and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "EQVfjjmXcMKv",
    "outputId": "c1fc8a90-120a-47ea-ea2f-f3a725a210af"
   },
   "outputs": [],
   "source": [
    "# Let's separate into train and test set\n",
    "# Remember to set the seed (random_state for this `sklearn` function)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,  # predictive variables\n",
    "    y,  # target\n",
    "    test_size=0.3,  # portion of dataset to allocate to test set\n",
    "    random_state=0,  # we are setting the seed here\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "BUbUMfch5kL4"
   },
   "source": [
    "We can note below that the target variable is skewed to the left and our model will have a tendency to predict values to the most common value. Therefore, we transform it to make it more normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "deletable": false,
    "id": "GkJ--R6e7ZlS",
    "outputId": "c05b0740-4f41-4f0d-e24c-d1f89fabcd3d"
   },
   "outputs": [],
   "source": [
    "# histogram to evaluate target distribution\n",
    "\n",
    "df[\"price\"].hist(bins=50, density=True)\n",
    "plt.ylabel(\"Number of houses\")\n",
    "plt.xlabel(\"Sale price\")\n",
    "plt.suptitle(\n",
    "    \"Fig. 2: Target Variable Distribution\",\n",
    "    fontweight=\"bold\",\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "763fBUrc55u4"
   },
   "source": [
    "After log transforming the target variable, it now looks more normally distributed and will therefore use the log transform variable as our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "deletable": false,
    "id": "X_NA9ysp7bbf",
    "outputId": "12b295fe-7043-4585-ce1b-6b52bb146317"
   },
   "outputs": [],
   "source": [
    "# let's transform the target using the logarithm\n",
    "\n",
    "np.log(df[\"price\"]).hist(bins=50, density=True)\n",
    "plt.ylabel(\"Number of houses\")\n",
    "plt.xlabel(\"Log of Sale Price\")\n",
    "plt.suptitle(\n",
    "    \"Fig. 3: Log Transformed Target\", fontweight=\"bold\", horizontalalignment=\"right\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "FloP7gwjihfU"
   },
   "outputs": [],
   "source": [
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "deletable": false,
    "id": "o_Ntc07FGHoO",
    "outputId": "0e25df7b-b40b-4b79-9f02-7c34de8860af"
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "H-iIqnOS6Qtl"
   },
   "source": [
    "We now train the model using the pipeline we formulated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "9-84VlsFHiqp"
   },
   "outputs": [],
   "source": [
    "_ = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "deletable": false,
    "id": "OV2PEHJLHiuE",
    "outputId": "71dd71c4-45da-43fa-8785-67fa435cae1b"
   },
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "_lyyPVB36aAQ"
   },
   "source": [
    "We then use the model to make predictions about unseen test data and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "5xCyaRyjMnAL",
    "outputId": "67567080-db16-4985-bbae-3f8de18afa0a"
   },
   "outputs": [],
   "source": [
    "target_predicted = model.predict(X_test)\n",
    "\n",
    "print(\n",
    "    f\"Mean squared error on the testing set: \"\n",
    "    f\"{mean_squared_error(y_test, target_predicted):.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "XimWNngO7C4_"
   },
   "source": [
    "## **4. Regression Metrics**\n",
    "\n",
    "There are various metrics we can use to evaluate the performance of a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "kbYlmB-JMnC3",
    "outputId": "c2695585-f67e-4d6d-a192-fff7c654ae8d"
   },
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "XvDr0gYQ6lWF"
   },
   "source": [
    "The model performed really well, and we will now want to see if we can improve this performance using hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "hPBsfrjbMnGc",
    "outputId": "8ce3640b-eb60-4bb5-aba2-c00368924873"
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "dummy_regressor = DummyRegressor(strategy=\"mean\")\n",
    "dummy_regressor.fit(X_train, y_train)\n",
    "print(\n",
    "    f\"R2 score for a regressor predicting the mean:\"\n",
    "    f\"{dummy_regressor.score(X_test, y_test):.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "KH62n-InOCuE",
    "outputId": "80356e60-cc4c-4cbb-bdce-b1e88dd7530c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(f\"Mean absolute error: \" f\"{mean_absolute_error(y_test, target_predicted):.3f} $\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "RA_drZLA7-Ej"
   },
   "source": [
    "## **5. Hyperparameter Tuning**\n",
    "\n",
    "### **5.1 Random Search**\n",
    "\n",
    "We will need to define the hyperparameter grid to be able to use random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "0ZXx5WxuRnNZ",
    "outputId": "ba46d0b4-67b1-4321-c830-869d18e22ec6"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print(\"Parameters currently in use:\\n\")\n",
    "pprint(model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gb1zdDUY9SM1"
   },
   "source": [
    "Random search has the advantage of being fast as it does not use all the values in the hyperparameter space but rather randomly selects variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "xHRGpKxdS2uj",
    "outputId": "7c0f041d-449d-4723-ea44-aae5a89fd21a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=10, stop=90, num=10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = [\"auto\", \"sqrt\"]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 50, num=21)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "    \"regressor__n_estimators\": n_estimators,\n",
    "    \"regressor__max_features\": max_features,\n",
    "    \"regressor__max_depth\": max_depth,\n",
    "    \"regressor__min_samples_split\": min_samples_split,\n",
    "    \"regressor__min_samples_leaf\": min_samples_leaf,\n",
    "    \"regressor__bootstrap\": bootstrap,\n",
    "}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "LZspvISp9mOD"
   },
   "source": [
    "We now create a model with all the parameters specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "deletable": false,
    "id": "go6K_SyETX29",
    "outputId": "2c25f583-4c09-433f-820b-30190acd439b"
   },
   "outputs": [],
   "source": [
    "model_random_search = RandomizedSearchCV(\n",
    "    model,\n",
    "    param_distributions=random_grid,\n",
    "    n_iter=10,\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    ")\n",
    "model_random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "qurh3IJF9zNc"
   },
   "source": [
    "We can now visualize the best hyperparameters that we will fit back to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "a5xSHQJ7TBk8",
    "outputId": "e55bfbcb-d24c-41a0-853b-dc041bed2fc6"
   },
   "outputs": [],
   "source": [
    "model_random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "JyuSBz5j99t7"
   },
   "source": [
    "By using this hyperparameter values, we now evaluate our algorithm's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "otMEbUi9ViZA",
    "outputId": "0553e845-4f0f-456e-cfaf-0e2f117eb521"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print(\"Model Performance\")\n",
    "    print(\"Average Error: {:0.4f} dollars.\".format(np.mean(errors)))\n",
    "    print(\"Accuracy = {:0.2f}%.\".format(accuracy))\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "base_model = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "base_model.fit(X_train, y_train)\n",
    "base_accuracy = evaluate(base_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "nHvQK7A9Vqy1",
    "outputId": "fc94a575-5857-45fa-d934-e5434c9b31d6"
   },
   "outputs": [],
   "source": [
    "best_random = model_random_search.best_estimator_\n",
    "random_accuracy = evaluate(best_random, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "RacywKbv-I_i"
   },
   "source": [
    "Comparing the performance of our model without and with hyperparameter tuning show us a marginal increase in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "SMaxkFPTV-I_",
    "outputId": "3f560379-14b5-47e4-f846-57983d3facc8"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Improvement of {:0.2f}%.\".format(\n",
    "        100 * (random_accuracy - base_accuracy) / base_accuracy\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hjK4rWzqMyc-"
   },
   "source": [
    "### **5.2 Grid Search**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "TwoWk4Av7W-a"
   },
   "source": [
    "We now provide a framework for performing grid search on our model hyperparameters. Note that we restricted the hyperparameters to a smaller space since the grid search algorithm is very heavy. We therefore do not expect a better performance. Students are allowed to experiment with bigger spaces and see how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "-WO1TRa6Mxef"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid based on the results of random search\n",
    "param_grid = {\n",
    "    \"regressor__bootstrap\": [True],\n",
    "    \"regressor__max_depth\": [int(x) for x in np.linspace(10, 20, num=6)],\n",
    "    \"regressor__max_features\": [2, 3],\n",
    "    \"regressor__min_samples_leaf\": [3, 4, 5],\n",
    "    \"regressor__min_samples_split\": [8, 10, 12],\n",
    "    \"regressor__n_estimators\": [\n",
    "        int(x) for x in np.linspace(start=70, stop=100, num=11)\n",
    "    ],\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "XkekNsPzSLvk",
    "outputId": "a6c8fca5-a42c-4948-d652-62c8e27c1977"
   },
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "LHE3ATrWilE3",
    "outputId": "f00d576c-c131-4212-ecd0-19e9ddd028eb"
   },
   "outputs": [],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "grid_accuracy = evaluate(best_grid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Fk7duXF--bNW"
   },
   "source": [
    "We also compare the performance of the grid search algorithm with that of the original random forest algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "259pA-IbilWV",
    "outputId": "610e1f94-6de0-4e99-9e48-4c53034d3c74"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Improvement of {:0.2f}%.\".format(\n",
    "        100 * (grid_accuracy - base_accuracy) / base_accuracy\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "D-8clgDj-0Mi"
   },
   "source": [
    "## **Conclusion**\n",
    "\n",
    "In this lesson, we have seen how to apply hyperparameter tuning to a regression problem. In the next lesson, we will study objective functions and other techniques used to evaluate machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7-X-AarjuF9Y"
   },
   "source": [
    "**References**\n",
    "\n",
    "1. Breiman, Leo. \"Random Forests.\" *Machine Learning*, vol. 45, no. 1, 2001, pp 5-32.\n",
    "2. GeoDa Data and Lab. \"2014-15 Home Sales in King County, WA.\" https://geodacenter.github.io/data-and-lab/KingCounty-HouseSales2015/\n",
    "3. Ramadhan, Muhammad Murtadha, et al. \"Parameter Tuning in Random Forest based on Grid Search Method for Gender Classification based on Voice Frequency.\" *DEStech Transactions on Computer Science and Engineering*, vol. 10, 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "Copyright 2023 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 3000
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
