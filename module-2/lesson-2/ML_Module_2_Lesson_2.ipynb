{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5AxF11X9tHvo"
   },
   "source": [
    "\n",
    "## MACHINE LEARNING IN FINANCE\n",
    "MODULE 2 | LESSON 2\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "J4ceGgiItOf0"
   },
   "source": [
    "# **HIERARCHICAL CLUSTERING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MELzJ6aRtXsV"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  70 minutes |\n",
    "|**Prior Knowledge** | Clustering, unsupervised  |\n",
    "|**Keywords** |dendrogram, hierarchical  |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "S_bWQ2CBDjyY"
   },
   "source": [
    "*In the previous lesson, we explored unsupervised machine learning and saw how it differs from supervised machine learning. We also studied different clustering techniques like the k-means algorithm to find patterns in a dataset. In this lesson, we will go further using hierarchical clustering and implement it in clustering foreign exchange rates.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "K3YECTyle2bB"
   },
   "source": [
    "## **1. Introduction**\n",
    "\n",
    "Hierarchical  clustering is an unsupervised learning model that clusters data points into hierarchies. In the last lesson, we saw how the k-means algorithm operates, and one of its limitation was that we need to know the number of clusters beforehand. In hierarchical clustering, it is not necessary to know the number of clusters before the modeling process can begin.\n",
    "\n",
    "Hierarchical clustering can be divided into two types:\n",
    "1. Agglomerative hierarchical clustering\n",
    "2. Divisive hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "RKduAbjq-2vM"
   },
   "source": [
    "## **2. Agglomerative Hierarchical Clustering**\n",
    "\n",
    "Agglomerative hierarchical clustering is the most widely used (in the hierarchical family) to cluster data points based on their similarity. It uses a bottom-up approach where each data point starts as an individual cluster, and at each iteration, similar clusters are merged as we move up the hierarchy until we obtain one cluster or we identify K clusters.\n",
    "\n",
    "### **2.1 Algorithm**\n",
    "\n",
    "1. Assume we have the dataset scattered in a two dimensional plane.\n",
    "\n",
    "2. Each data point forms its own cluster at the beginning.\n",
    "\n",
    "\n",
    "3. Similar clusters are merged.\n",
    "\n",
    "\n",
    "4. Repeat step 2 above until a single cluster or K clusters remain(s).\n",
    "\n",
    "\n",
    "Two clusters are considered similar if the distance between them is small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "VH679rWMACBA"
   },
   "source": [
    "### **2.2 Measures of Distance (Similarity)**\n",
    "\n",
    "Euclidean distance is defined as the distance between two points in a straight line. For example, if $(x_1, y_1)$ and $(x_2, y_2)$ are the points in a 2-dimensional space, then the Euclidean distance would be \n",
    "$$\\text{Euclidean Distance} = (x_2-x_1)^2 + (y_2 - y_1)^2$$.\n",
    "Euclidean Distance is applied when the variables are continuous.\n",
    "\n",
    "Other methods of measuring distance between two points include;\n",
    "\n",
    "- Hamming distance\n",
    "\n",
    "We use hamming distance for text and non-numerical data, and it works by counting the number of times the coordinates in the two sets differ.\n",
    "- Manhattan distance\n",
    "\n",
    "Manhattan distance is suitable when finding the distance between different locations like finding the distance between schools. It is given by $$|a-b|_1 = \\sum_i |a_i - b_i|$$\n",
    "\n",
    "- Minkowski distance\n",
    "\n",
    "Minkowski distance is given by $$\\left(\\sum_{i=1}^n |X_i - Y_i|^p\\right)^{1/p}$$\n",
    "When $p=1$ the Minkowski distance is equivalent to Manhattan distance, for $p=2$ the Minkowski distance is equivalent to Euclidean distance.\n",
    "- Maximum distance\n",
    "\n",
    "Maximum distance is the maximum distance we can get between two sets, that is, $$|a-b|_{\\infty} = \\max_i |a_i - b_i|$$\n",
    "- Canberra distance\n",
    "\n",
    "Canberra distance is expressed as $$\\sum_i \\frac{|x_i - y_i|}{|x_i + y_i|}$$\n",
    "and terms where the denominator is zero are omitted. Canberra distance is used to compare ranked lists.\n",
    "\n",
    "The choice of distance metric to use depends on the problem we are trying to solve.\n",
    "\n",
    "### **2.3 Linkage Criterion**\n",
    " \n",
    "Once we have settled on a distance metric to apply, we need to decide where the distance will be computed. This is referred to as **linkage**.\n",
    "\n",
    "The commonly used linkage methods are:\n",
    "\n",
    " **1. Single linkage / nearest linkage:**\n",
    "\n",
    "This is the minimum distance between a pair of data points in two different clusters given by\n",
    "$$d(u,v) = \\min(dist(u[i], v[j]))$$ for data points $i$ in cluster $u$ and data points $j$ in cluster $v$. Consider the example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "X = [[i] for i in [2, 8, 0, 4, 1, 9, 9, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "From the given dataset, we obtain a single linkage using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "Z = linkage(X, \"single\")\n",
    "fig = plt.figure(figsize=(25, 5))\n",
    "plt.suptitle(\n",
    "    \"Fig. 1: Dendrogram with Single Linkage\",\n",
    "    fontweight=\"bold\",\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "dn = dendrogram(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**2. Complete Linkage:**\n",
    "\n",
    "This is the maximum distance we can get between data points in two different clusters. The mathematical representation for this is $$d(u,v) = \\max(dist(u[i], v[j]))$$ for data points $i$ in cluster $u$ and data points $j$ in cluster $v$. We define the complete linkage in a code as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "Z = linkage(X, \"complete\")\n",
    "fig = plt.figure(figsize=(25, 5))\n",
    "plt.suptitle(\n",
    "    \"Fig. 2: Dendrogram with Complete Linkage\",\n",
    "    fontweight=\"bold\",\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "dn = dendrogram(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "**3. Average Linkage**: \n",
    "\n",
    "This is the average of the distance between each data point in one cluster to the data point in the next cluster. Its mathematical representation is as shown below\n",
    "$$d(u,v) = \\sum_{i,j}\\frac{d(u[i],v[j])}{(|u|\\times|v|)}$$\n",
    "\n",
    "and just like the two linkages above we specify linkage as average in python code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "Z = linkage(X, \"average\")\n",
    "fig = plt.figure(figsize=(25, 5))\n",
    "plt.suptitle(\n",
    "    \"Fig. 3: Dendrogram with Average Linkage\",\n",
    "    fontweight=\"bold\",\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "dn = dendrogram(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "**4.  Centroid Linkage**: \n",
    "\n",
    "This is the distance between the cluster centers and is given by $$d(u,v) = ||c_u - c_v||_2$$ with the linkage specified as in the preceding charts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "Z = linkage(X, \"centroid\")\n",
    "fig = plt.figure(figsize=(25, 5))\n",
    "plt.suptitle(\n",
    "    \"Fig. 4: Dendrogram with Centroid Linkage\",\n",
    "    fontweight=\"bold\",\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "dn = dendrogram(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "**5. Ward's Linkage**: \n",
    "\n",
    "Ward's linkage minimizes the increase in the sum of square error at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "Z = linkage(X, \"ward\")\n",
    "fig = plt.figure(figsize=(25, 5))\n",
    "plt.suptitle(\n",
    "    \"Fig. 5: Dendrogram with Ward Linkage\",\n",
    "    fontweight=\"bold\",\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "dn = dendrogram(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The diagrams we have seen above are called Dendrograms and we chose the number of clusters in hierarchical clustering with the help of a dendrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Hkq-myq5NqKK"
   },
   "source": [
    "\n",
    "### **2.4 Dendrogram**\n",
    "\n",
    "A dendrogram is a diagram that shows the hierarchical relationship between clusters. It shows how clusters are formed at each step. \n",
    "\n",
    "Once we have drawn the dendrogram, we slice it horizontally at the desired level then the branches that appear below the cut form individual clusters and its associated membership. Below is an example of a dendrogram with horizontal slices at different levels. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "Z = linkage(X)\n",
    "fig = plt.figure(figsize=(25, 5))\n",
    "plt.axhline(1.2, color=\"red\", linestyle=\"--\")\n",
    "plt.axhline(2.2, color=\"red\", linestyle=\"--\")\n",
    "plt.axhline(0.8, color=\"red\", linestyle=\"--\")\n",
    "plt.suptitle(\n",
    "    \"Fig. 6: Dendrogram with Different Horizontal Lines\",\n",
    "    fontweight=\"bold\",\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "dn = dendrogram(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "PzNsaUIKUdwI"
   },
   "source": [
    "## **3. Divisive Hierarchical Clustering**\n",
    "\n",
    "Divisive hierarchical clustering works in the opposite of agglomerative hierarchical clustering; that is, we start with a single cluster containing all the data points. Then, at each iteration, we split the data into smaller clusters until we have each data point in its own cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "-2cAQCuNWTyd"
   },
   "source": [
    "## **4. Implementation of Hierarchical Clustering**\n",
    "\n",
    "In this demonstration, we use a matrix of daily returns of foreign exchange rates and perform hierarchical clustering to determine if the currencies form clusters that are related to their geographical categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "T2xI2FelaRmW"
   },
   "source": [
    "### **4.1 Data Scraping**\n",
    "\n",
    "We start by installing the Yahoo finance API that will be used to download the forex data. We also import libraries for data manipulation and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "MfE18g3e_q9V"
   },
   "outputs": [],
   "source": [
    "# For data manipulation\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# To fetch financial data\n",
    "import yfinance as yf\n",
    "\n",
    "plt.style.use(\"seaborn-darkgrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.cluster.hierarchy import cophenet, dendrogram, linkage\n",
    "\n",
    "# to perform hierarchical clustering, compute cophenetic correlation, and create dendrograms\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "# Removes the limit for the number of displayed columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Sets the limit for the number of displayed rows\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "\n",
    "# to compute distances\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "# to scale the data using z-score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "_ZCYaPHivimh"
   },
   "source": [
    "We now download the foreign exchange rates rendered to one U.S. dollar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "TPFKX09u_tWj",
    "outputId": "fbafe9a2-1264-4614-e251-5f645bf854e2"
   },
   "outputs": [],
   "source": [
    "# Set the ticker as 'EURUSD=X'\n",
    "forex_data = yf.download(\"USDEUR=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data = forex_data.reset_index()\n",
    "euro_df = forex_data[[\"Date\", \"Adj Close\"]]\n",
    "euro_df.rename(columns={\"Adj Close\": \"euro\"}, inplace=True)\n",
    "# Set the index to a datetime object\n",
    "\n",
    "forex_data1 = yf.download(\"USDRUB=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data1 = forex_data1.reset_index()\n",
    "rub_df = forex_data1[[\"Date\", \"Adj Close\"]]\n",
    "rub_df.rename(columns={\"Adj Close\": \"rub\"}, inplace=True)\n",
    "\n",
    "forex_data2 = yf.download(\"USDGBP=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data2 = forex_data2.reset_index()\n",
    "gbp_df = forex_data2[[\"Date\", \"Adj Close\"]]\n",
    "gbp_df.rename(columns={\"Adj Close\": \"gbp\"}, inplace=True)\n",
    "\n",
    "forex_data3 = yf.download(\"USDJPY=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data3 = forex_data3.reset_index()\n",
    "jpy_df = forex_data3[[\"Date\", \"Adj Close\"]]\n",
    "jpy_df.rename(columns={\"Adj Close\": \"jpy\"}, inplace=True)\n",
    "\n",
    "forex_data4 = yf.download(\"USDKES=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data4 = forex_data4.reset_index()\n",
    "kes_df = forex_data4[[\"Date\", \"Adj Close\"]]\n",
    "kes_df.rename(columns={\"Adj Close\": \"kes\"}, inplace=True)\n",
    "\n",
    "forex_data5 = yf.download(\"USDCNY=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data5 = forex_data5.reset_index()\n",
    "cny_df = forex_data5[[\"Date\", \"Adj Close\"]]\n",
    "cny_df.rename(columns={\"Adj Close\": \"cny\"}, inplace=True)\n",
    "\n",
    "forex_data6 = yf.download(\"USDKRW=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data6 = forex_data6.reset_index()\n",
    "krw_df = forex_data6[[\"Date\", \"Adj Close\"]]\n",
    "krw_df.rename(columns={\"Adj Close\": \"krw\"}, inplace=True)\n",
    "\n",
    "forex_data7 = yf.download(\"USDSGD=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data7 = forex_data7.reset_index()\n",
    "sgd_df = forex_data7[[\"Date\", \"Adj Close\"]]\n",
    "sgd_df.rename(columns={\"Adj Close\": \"sgd\"}, inplace=True)\n",
    "\n",
    "forex_data8 = yf.download(\"USDTWD=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data8 = forex_data8.reset_index()\n",
    "twd_df = forex_data8[[\"Date\", \"Adj Close\"]]\n",
    "twd_df.rename(columns={\"Adj Close\": \"twd\"}, inplace=True)\n",
    "\n",
    "forex_data9 = yf.download(\"USDNGN=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data9 = forex_data9.reset_index()\n",
    "ngn_df = forex_data9[[\"Date\", \"Adj Close\"]]\n",
    "ngn_df.rename(columns={\"Adj Close\": \"ngn\"}, inplace=True)\n",
    "\n",
    "forex_data10 = yf.download(\"USDZAR=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data10 = forex_data10.reset_index()\n",
    "zar_df = forex_data10[[\"Date\", \"Adj Close\"]]\n",
    "zar_df.rename(columns={\"Adj Close\": \"zar\"}, inplace=True)\n",
    "\n",
    "forex_data11 = yf.download(\"USDMYR=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data11 = forex_data11.reset_index()\n",
    "myr_df = forex_data11[[\"Date\", \"Adj Close\"]]\n",
    "myr_df.rename(columns={\"Adj Close\": \"myr\"}, inplace=True)\n",
    "\n",
    "forex_data12 = yf.download(\"USDIDR=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data12 = forex_data12.reset_index()\n",
    "idr_df = forex_data12[[\"Date\", \"Adj Close\"]]\n",
    "idr_df.rename(columns={\"Adj Close\": \"idr\"}, inplace=True)\n",
    "\n",
    "forex_data13 = yf.download(\"USDTHB=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data13 = forex_data13.reset_index()\n",
    "thb_df = forex_data13[[\"Date\", \"Adj Close\"]]\n",
    "thb_df.rename(columns={\"Adj Close\": \"thb\"}, inplace=True)\n",
    "\n",
    "forex_data14 = yf.download(\"USDAUD=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data14 = forex_data14.reset_index()\n",
    "aud_df = forex_data14[[\"Date\", \"Adj Close\"]]\n",
    "aud_df.rename(columns={\"Adj Close\": \"aud\"}, inplace=True)\n",
    "\n",
    "forex_data15 = yf.download(\"USDNZD=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data15 = forex_data15.reset_index()\n",
    "nzd_df = forex_data15[[\"Date\", \"Adj Close\"]]\n",
    "nzd_df.rename(columns={\"Adj Close\": \"nzd\"}, inplace=True)\n",
    "\n",
    "forex_data16 = yf.download(\"USDCAD=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data16 = forex_data16.reset_index()\n",
    "cad_df = forex_data16[[\"Date\", \"Adj Close\"]]\n",
    "cad_df.rename(columns={\"Adj Close\": \"cad\"}, inplace=True)\n",
    "\n",
    "forex_data17 = yf.download(\"USDCHF=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data17 = forex_data17.reset_index()\n",
    "chf_df = forex_data17[[\"Date\", \"Adj Close\"]]\n",
    "chf_df.rename(columns={\"Adj Close\": \"chf\"}, inplace=True)\n",
    "\n",
    "forex_data18 = yf.download(\"USDNOK=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data18 = forex_data18.reset_index()\n",
    "nok_df = forex_data18[[\"Date\", \"Adj Close\"]]\n",
    "nok_df.rename(columns={\"Adj Close\": \"nok\"}, inplace=True)\n",
    "\n",
    "forex_data19 = yf.download(\"USDAUD=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data19 = forex_data19.reset_index()\n",
    "sek_df = forex_data19[[\"Date\", \"Adj Close\"]]\n",
    "sek_df.rename(columns={\"Adj Close\": \"sek\"}, inplace=True)\n",
    "\n",
    "forex_data20 = yf.download(\"USDARS=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data20 = forex_data20.reset_index()\n",
    "ars_df = forex_data20[[\"Date\", \"Adj Close\"]]\n",
    "ars_df.rename(columns={\"Adj Close\": \"ars\"}, inplace=True)\n",
    "\n",
    "forex_data21 = yf.download(\"USDPLN=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data21 = forex_data21.reset_index()\n",
    "pln_df = forex_data21[[\"Date\", \"Adj Close\"]]\n",
    "pln_df.rename(columns={\"Adj Close\": \"pln\"}, inplace=True)\n",
    "\n",
    "forex_data22 = yf.download(\"USDPHP=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data22 = forex_data22.reset_index()\n",
    "php_df = forex_data22[[\"Date\", \"Adj Close\"]]\n",
    "php_df.rename(columns={\"Adj Close\": \"php\"}, inplace=True)\n",
    "\n",
    "forex_data23 = yf.download(\"USDRON=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data23 = forex_data23.reset_index()\n",
    "ron_df = forex_data23[[\"Date\", \"Adj Close\"]]\n",
    "ron_df.rename(columns={\"Adj Close\": \"ron\"}, inplace=True)\n",
    "\n",
    "forex_data24 = yf.download(\"USDHUF=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data24 = forex_data24.reset_index()\n",
    "huf_df = forex_data24[[\"Date\", \"Adj Close\"]]\n",
    "huf_df.rename(columns={\"Adj Close\": \"huf\"}, inplace=True)\n",
    "\n",
    "forex_data25 = yf.download(\"USDBRL=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data25 = forex_data25.reset_index()\n",
    "brl_df = forex_data25[[\"Date\", \"Adj Close\"]]\n",
    "brl_df.rename(columns={\"Adj Close\": \"brl\"}, inplace=True)\n",
    "\n",
    "forex_data26 = yf.download(\"USDCLP=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data26 = forex_data26.reset_index()\n",
    "clp_df = forex_data26[[\"Date\", \"Adj Close\"]]\n",
    "clp_df.rename(columns={\"Adj Close\": \"clp\"}, inplace=True)\n",
    "\n",
    "forex_data27 = yf.download(\"USDMXN=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data27 = forex_data27.reset_index()\n",
    "mxn_df = forex_data27[[\"Date\", \"Adj Close\"]]\n",
    "mxn_df.rename(columns={\"Adj Close\": \"mxn\"}, inplace=True)\n",
    "\n",
    "forex_data28 = yf.download(\"USDCOP=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data28 = forex_data28.reset_index()\n",
    "cop_df = forex_data28[[\"Date\", \"Adj Close\"]]\n",
    "cop_df.rename(columns={\"Adj Close\": \"cop\"}, inplace=True)\n",
    "\n",
    "forex_data29 = yf.download(\"USDILS=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data29 = forex_data29.reset_index()\n",
    "ils_df = forex_data29[[\"Date\", \"Adj Close\"]]\n",
    "ils_df.rename(columns={\"Adj Close\": \"ils\"}, inplace=True)\n",
    "\n",
    "forex_data30 = yf.download(\"USDTRY=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data30 = forex_data30.reset_index()\n",
    "try_df = forex_data30[[\"Date\", \"Adj Close\"]]\n",
    "try_df.rename(columns={\"Adj Close\": \"try\"}, inplace=True)\n",
    "\n",
    "forex_data31 = yf.download(\"USDINR=X\", start=\"2019-01-02\", end=\"2022-06-30\")\n",
    "forex_data31 = forex_data31.reset_index()\n",
    "inr_df = forex_data31[[\"Date\", \"Adj Close\"]]\n",
    "inr_df.rename(columns={\"Adj Close\": \"inr\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "QAieFJHq1QvH"
   },
   "source": [
    "The currencies are combined into one dataframe that will form the basis of our modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "deletable": false,
    "id": "uyRFBrawQh1F",
    "outputId": "117ceac1-2ea2-4214-d120-700cf3054fe1"
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "df_currencies = reduce(\n",
    "    lambda x, y: pd.merge(x, y, on=\"Date\", how=\"outer\"),\n",
    "    [\n",
    "        kes_df,\n",
    "        ars_df,\n",
    "        php_df,\n",
    "        myr_df,\n",
    "        ils_df,\n",
    "        cop_df,\n",
    "        euro_df,\n",
    "        ngn_df,\n",
    "        huf_df,\n",
    "        ron_df,\n",
    "        cny_df,\n",
    "        rub_df,\n",
    "        clp_df,\n",
    "        sgd_df,\n",
    "        twd_df,\n",
    "        krw_df,\n",
    "        idr_df,\n",
    "        thb_df,\n",
    "        inr_df,\n",
    "        pln_df,\n",
    "        try_df,\n",
    "        brl_df,\n",
    "        mxn_df,\n",
    "        zar_df,\n",
    "        gbp_df,\n",
    "        jpy_df,\n",
    "        aud_df,\n",
    "        nzd_df,\n",
    "        cad_df,\n",
    "        chf_df,\n",
    "        nok_df,\n",
    "        sek_df,\n",
    "    ],\n",
    ")\n",
    "df_currencies.set_index(\"Date\", inplace=True)\n",
    "\n",
    "df_currencies.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "RxuCd84X1k-c"
   },
   "source": [
    "We have 911 rows of daily rates and 32 currencies in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "Z5sdhixzjNlr",
    "outputId": "5c64ce48-e7a2-49d3-caf8-c5651e9178ea"
   },
   "outputs": [],
   "source": [
    "df_currencies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "FHL5Wi8N9_8W",
    "outputId": "8d6f3771-408c-415c-953e-e0f4d82d0cdb"
   },
   "outputs": [],
   "source": [
    "df_currencies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "deletable": false,
    "id": "tuyhAokY1_Or",
    "outputId": "98abea84-1cca-4ceb-a29b-01cc6f092c0f"
   },
   "outputs": [],
   "source": [
    "df_currencies.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "bSXrRBk-2eob"
   },
   "source": [
    "The next step is to find missing values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "I3JFwfBx-JD-",
    "outputId": "00801dcc-ac1d-4aaa-b1ce-49ea937417d5"
   },
   "outputs": [],
   "source": [
    "df_currencies.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MmqJ9ZBB3Oqi"
   },
   "source": [
    "We impute the missing value by propagating the observations from the last day to the missing day data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "qeVSWQ0H-b7j",
    "outputId": "cc35c773-b51a-4cca-8317-29e7334af480"
   },
   "outputs": [],
   "source": [
    "df_currencies.fillna(method=\"ffill\", inplace=True)\n",
    "df_currencies.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Qx29w1TL3g4C"
   },
   "source": [
    "Since the currencies differ a lot in value range, we scale them to ensure they have the same mean and unit variance.<span style='color: transparent; font-size:1%'>All rights reserved WQU WorldQuant University QQQQ</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "deletable": false,
    "id": "XIqNrI9kOOy9",
    "outputId": "96672257-4f82-4a04-d09f-2beec20a8a1a"
   },
   "outputs": [],
   "source": [
    "# Scaling the data to bring it to the same scale\n",
    "\n",
    "sc = StandardScaler()\n",
    "subset_scaled_df = pd.DataFrame(\n",
    "    sc.fit_transform(df_currencies),\n",
    "    columns=df_currencies.columns,\n",
    ")\n",
    "subset_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "deletable": false,
    "id": "Mi664gZXnO4D",
    "outputId": "754b6fd7-094b-47b4-bb02-75912ae6cf43"
   },
   "outputs": [],
   "source": [
    "# Generate simple percentage returns to calculate the respective betas\n",
    "\n",
    "df_returns = (df_currencies / df_currencies.shift(-1)) - 1\n",
    "df_returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "6G_c6JNHEsQv"
   },
   "source": [
    "### **4.2 Model Training**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "gwc4h0_xG95o"
   },
   "outputs": [],
   "source": [
    "subset_scaled_df.drop(subset_scaled_df.tail(1).index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "deletable": false,
    "id": "CMu4hI2Yn8AJ",
    "outputId": "fc9df477-bfa2-48c6-ab85-d46e3e5e9990"
   },
   "outputs": [],
   "source": [
    "subset_scaled_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "wrYGF959suhw"
   },
   "source": [
    "We now find the **cophenetic correlation** using a combination of distance metrics and linkage methods. A cophenetic correlation measures the dendrogram's ability to preserve the initial unmodeled data points. The higher the cophenetic correlation, the better the performance of a dendogram in preserving the original distance. \n",
    "\n",
    "The cophenetic correlation is defined as:\n",
    "$$c = \\frac{\\sum_{i<j} (Y_{ij} - y)(Z_{ij} - z)}{\\sum_{i<j}{(Y_{ij} - y)\\sum_{i<j}(Z_{ij} - z)}}$$\n",
    "\n",
    "where\n",
    "\n",
    "- $Y_{ij}$ is the distance between $i$ and $j$ in $Y$.\n",
    "- $Z_{ij}$ is the cophenetic distance between $i$ and $j$.\n",
    "- $y$ and $z$ are the mean of $Y$ and $Z$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "NXGdHs3iUQUk",
    "outputId": "1c47cc21-9dc9-46dd-8d97-49e1dcbb84a5"
   },
   "outputs": [],
   "source": [
    "# list of distance metrics\n",
    "distance_metrics = [\"euclidean\", \"chebyshev\", \"cityblock\"]\n",
    "\n",
    "# list of linkage methods\n",
    "linkage_methods = [\"single\", \"complete\", \"average\", \"weighted\"]\n",
    "\n",
    "high_cophenet_corr = 0\n",
    "high_dm_lm = [0, 0]\n",
    "\n",
    "for dm in distance_metrics:\n",
    "    for lm in linkage_methods:\n",
    "        Z = linkage(subset_scaled_df.T, metric=dm, method=lm)\n",
    "        c, coph_dists = cophenet(Z, pdist(subset_scaled_df.T))\n",
    "        print(\n",
    "            \"Cophenetic correlation for {} distance and {} linkage is {}\".format(\n",
    "                dm.capitalize(), lm, c\n",
    "            )\n",
    "        )\n",
    "        if high_cophenet_corr < c:\n",
    "            high_cophenet_corr = c\n",
    "            high_dm_lm[0] = dm\n",
    "            high_dm_lm[1] = lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "SOKMWB65BXAx"
   },
   "source": [
    "We now identify the distance metric and linkage method that give the highest cophenetic correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "aSUotj4QY3le",
    "outputId": "3a0b632f-8d26-423e-b7ed-fe47e9ee93cc"
   },
   "outputs": [],
   "source": [
    "# printing the combination of distance metric and linkage method with the highest cophenetic correlation\n",
    "print(\n",
    "    \"Highest cophenetic correlation is {}, which is obtained with {} distance and {} linkage\".format(\n",
    "        high_cophenet_corr, high_dm_lm[0].capitalize(), high_dm_lm[1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5BYmnunICBZG"
   },
   "source": [
    "Since the Euclidean distance gives us the best performance, we combine it with different linkage methods and evaluate which combination will improve our performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "TyW6wln1Y8_c",
    "outputId": "115ff646-fe8f-4f8c-9554-3706d578a365"
   },
   "outputs": [],
   "source": [
    "# list of linkage methods\n",
    "linkage_methods = [\"single\", \"complete\", \"average\", \"centroid\", \"ward\", \"weighted\"]\n",
    "\n",
    "high_cophenet_corr = 0\n",
    "high_dm_lm = [0, 0]\n",
    "\n",
    "for lm in linkage_methods:\n",
    "    Z = linkage(subset_scaled_df.T, metric=\"euclidean\", method=lm)\n",
    "    c, coph_dists = cophenet(Z, pdist(subset_scaled_df.T))\n",
    "    print(\"Cophenetic correlation for {} linkage is {}\".format(lm, c))\n",
    "    if high_cophenet_corr < c:\n",
    "        high_cophenet_corr = c\n",
    "        high_dm_lm[0] = \"euclidean\"\n",
    "        high_dm_lm[1] = lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4vYc44u4CR04"
   },
   "source": [
    "Again, we print the linkage method that gives the highest cophenetic correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "1Zw-QgiacYNr",
    "outputId": "3cb38f88-9529-4a73-d434-d4f5f993d43c"
   },
   "outputs": [],
   "source": [
    "# printing the combination of distance metric and linkage method with the highest cophenetic correlation\n",
    "print(\n",
    "    \"Highest cophenetic correlation is {}, which is obtained with {} linkage\".format(\n",
    "        high_cophenet_corr, high_dm_lm[1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "sJKHf4P_Cd4_"
   },
   "source": [
    "Let us plot the graphs of different dendrograms to visualize how clusters vary with different linkage methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "deletable": false,
    "id": "vKyMvItUcYT3",
    "outputId": "3d13863e-c4e8-4168-ce60-2a54d54f0c95"
   },
   "outputs": [],
   "source": [
    "# list of linkage methods\n",
    "linkage_methods = [\"single\", \"complete\", \"average\", \"centroid\", \"ward\", \"weighted\"]\n",
    "\n",
    "# lists to save results of cophenetic correlation calculation\n",
    "compare_cols = [\"Linkage\", \"Cophenetic Coefficient\"]\n",
    "\n",
    "# to create a subplot image\n",
    "fig, axs = plt.subplots(len(linkage_methods), 1, figsize=(15, 30))\n",
    "# `plt.suptitle`()\n",
    "\n",
    "# We will enumerate through the list of linkage methods above\n",
    "# For each linkage method, we will plot the dendrogram and calculate the cophenetic correlation\n",
    "for i, method in enumerate(linkage_methods):\n",
    "    Z = linkage(subset_scaled_df.T, metric=\"euclidean\", method=method)\n",
    "\n",
    "    dendrogram(Z, ax=axs[i])\n",
    "    axs[i].set_title(f\"Dendrogram ({method.capitalize()} Linkage)\")\n",
    "    fig.suptitle(\n",
    "        \"Fig. 11: Dendrograms Plots\", fontweight=\"bold\", horizontalalignment=\"right\"\n",
    "    )\n",
    "\n",
    "    coph_corr, coph_dist = cophenet(Z, pdist(subset_scaled_df.T))\n",
    "    axs[i].annotate(\n",
    "        f\"Cophenetic\\nCorrelation\\n{coph_corr:0.2f}\",\n",
    "        (0.80, 0.80),\n",
    "        xycoords=\"axes fraction\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "jo7dvjrTE1ao"
   },
   "source": [
    "To find the components of each cluster, we can train the data as shown below, then find the components as demonstrated in the steps below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "qe2FpVmNfDPy",
    "outputId": "562e2613-2317-44ab-ad8c-92141a0a5e35"
   },
   "outputs": [],
   "source": [
    "HCmodel = AgglomerativeClustering(n_clusters=3, affinity=\"euclidean\", linkage=\"average\")\n",
    "HCmodel.fit(subset_scaled_df.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "trsxlVZyfDbo"
   },
   "outputs": [],
   "source": [
    "df_currencies_t = df_currencies.T\n",
    "subset_scaled_df_t = subset_scaled_df.T\n",
    "\n",
    "subset_scaled_df_t[\"HC_Clusters\"] = HCmodel.labels_\n",
    "df_currencies_t[\"HC_Clusters\"] = HCmodel.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "zLXFxNoZtjzo"
   },
   "outputs": [],
   "source": [
    "cluster_profile = df_currencies_t.groupby(\"HC_Clusters\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "deletable": false,
    "id": "nfChq9wwwBdY",
    "outputId": "fdc92be6-7122-4750-8c26-1b22799657e9"
   },
   "outputs": [],
   "source": [
    "cluster_profile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "deletable": false,
    "id": "G57hbZXNt7mu",
    "outputId": "8f5369b4-b52b-419d-8b0b-f0ecd2c9fd49"
   },
   "outputs": [],
   "source": [
    "cluster_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gMReCQE_C398"
   },
   "source": [
    "We finally see the composition of 3 different clusters and the outlier which will be the column 'HC Cluster' that was engineered above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "sqe3b4UYtj2-",
    "outputId": "18076120-969d-431d-b4e4-3d28a8f05004"
   },
   "outputs": [],
   "source": [
    "# let's see the names of the securities in each cluster\n",
    "for cl in df_currencies_t[\"HC_Clusters\"].unique():\n",
    "    print(\n",
    "        \"The\",\n",
    "        df_currencies_t[df_currencies_t[\"HC_Clusters\"] == cl].iloc[:, 0].nunique(),\n",
    "        \"Securities in cluster\",\n",
    "        cl,\n",
    "        \"are:\",\n",
    "    )\n",
    "    print(df_currencies_t[df_currencies_t[\"HC_Clusters\"] == cl].iloc[:, 0].index)\n",
    "    print(\"-\" * 100, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "T0QVD-Y3sXMD",
    "outputId": "3aa64c18-1630-43c5-fbec-0ff7c3b6329e"
   },
   "outputs": [],
   "source": [
    "df_currencies_t[df_currencies_t[\"HC_Clusters\"] == 0].iloc[:, 0].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "UM6qQRY3FXT6"
   },
   "source": [
    "Cluster one has the Kenya shilling, Argentine peso, Colombian peso, Nigerian naira, Russian ruble, Chilean peso, Indonesian rupiah, Indian rupee, Turkish lira, Mexican peso, and South African rand.\n",
    "\n",
    "Cluster two has the Israeli shekel, Chinese yuan, Singaporean dollar, Taiwanese dollar, British pound, Australian dollar, New Zealand dollar, Canadian dollar, Swiss franc, Norwegian krone, and Swedish krona.\n",
    "\n",
    "Cluster three has the Philippine peso, Malaysian ringgt, Euro, Hungarian forint, Romanian leu, South Korean won, Thai bhat, Poland zlohty and Japanese yen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5F6h6gqhXxRn"
   },
   "source": [
    "We can see from the clusters that most developing countries are in the first cluster while developed nations are mostly in the second cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "PUIrX7-pC2zq"
   },
   "source": [
    "We can also draw the dendrogram using the methodology below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "h5udw-K1Vr3a"
   },
   "outputs": [],
   "source": [
    "hier_average = linkage(subset_scaled_df.T, method=\"average\", metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "LLxJ-JZ_V0ar"
   },
   "outputs": [],
   "source": [
    "hier_ward = linkage(subset_scaled_df.T, method=\"ward\", metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "a5Sfg24OV4WO"
   },
   "outputs": [],
   "source": [
    "hier_comp = linkage(subset_scaled_df.T, method=\"complete\", metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "tsSJi8yjXheZ"
   },
   "outputs": [],
   "source": [
    "# Change the chart style...\n",
    "plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "deletable": false,
    "id": "rVGv6TGyV4Zu",
    "outputId": "d9328df1-8734-4416-cea6-d3fa40ce98d9"
   },
   "outputs": [],
   "source": [
    "df_currencies_t = df_currencies.T\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\n",
    "    \"Dendrogram of FX Clusters, Jan 2019 through Jun 2022 (Complete)\", fontsize=14\n",
    ")\n",
    "plt.xlabel(\"Distance\", fontsize=10)\n",
    "plt.ylabel(\"Currency\", fontsize=10)\n",
    "plt.suptitle(\n",
    "    \"Fig. 12: Dendrogram with Complete Linkage\",\n",
    "    fontweight=\"bold\",\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "dendrogram(\n",
    "    hier_comp,\n",
    "    orientation=\"right\",\n",
    "    #     leaf_rotation=90.,\n",
    "    leaf_font_size=20,\n",
    "    labels=df_currencies_t.index.values,\n",
    "    color_threshold=3,\n",
    ")\n",
    "plt.yticks(fontsize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3ybKRBzRuBvj"
   },
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this lesson, we have studied and implemented hierarchical clustering. In the next lesson, we will focus on another unsupervised learning method that is used to reduce the number of variables in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7-X-AarjuF9Y"
   },
   "source": [
    "**References**\n",
    "1. Murtagh, Fionn, and Pedro Contreras. \"Algorithms for Hierarchical Clustering: An \n",
    "Overview.\" *Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery*, vol. 2, no. 1, 2012.\n",
    "2. Murtagh, Fionn, and Pedro Contreras. \"Algorithms for Hierarchical Clustering: An Overview, II.\" *Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery*, vol. 7, no. 6, 2017.\n",
    "3. Murtagh, Fionn, and Pedro Contreras. \"Methods of Hierarchical Clustering.\" *arXiv preprint*, arXiv:1105.0121, 2011.\n",
    "4. Nielsen, Frank. \"Hierarchical Clustering.\" *Introduction to HPC with MPI for Data Science*. Springer, 2016, pp. 195-211."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "Copyright 2023 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
