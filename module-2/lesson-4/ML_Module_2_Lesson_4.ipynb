{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5AxF11X9tHvo"
   },
   "source": [
    "\n",
    "## MACHINE LEARNING IN FINANCE\n",
    "MODULE 2 | LESSON 4\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "J4ceGgiItOf0"
   },
   "source": [
    "# **PRINCIPAL COMPONENT ANALYSIS AND INTEREST RATE MODELING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MELzJ6aRtXsV"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  30 Minutes |\n",
    "|**Prior Knowledge** | PCA, Machine learning  |\n",
    "|**Keywords** |Yield curves,  covariance, VaR, Loadings |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "mg7hqOaYt6Be"
   },
   "source": [
    "*In the previous lesson, we studied principal component analysis (PCA) and the mathematical theory behind it. In this lesson, we will implement PCA on treasury rates and use the resultant principal components to calculate the Value at Risk.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3xfyH8brg9No"
   },
   "source": [
    "##  **1. Introduction**\n",
    "\n",
    "### **1.1 Yield Curve**\n",
    "\n",
    "A yield curve is a line plot of bond interest rates of the same credit quality, having different maturities. The yield curve has two dimensions to it:\n",
    "\n",
    "- It depicts investors' average market expectations on the performance of future short-term bonds.\n",
    "- The term premium - When the yield curve is upward sloping, it shows investors' confidence in the market as they are optimistic about receiving better returns in long-term bonds. In instances where the curve is downward sloping, investors expect tough times like a recession ahead. The yield curve could also be flat depicting no change in short-term and long-term rates.\n",
    "\n",
    "These features of the yield curve can be modeled using principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "404pNkwNlw7A"
   },
   "source": [
    "### **1.2 Value at Risk (VaR)**\n",
    "\n",
    "In this subsection, we only cover a small portion of VaR that will be useful to a later topic.\n",
    "\n",
    "VaR is a useful metric to measure market risk on portfolio performance. VaR can be thought of as the largest possible loss we can incur for a given confidence interval and time period. VaR is a numerical value that distinguishes the tail (loss) of a distribution from the rest of the distribution.\n",
    "\n",
    "An individual position-level VaR is calculated by multiplying the position volatility (standard deviation) of return, the portfolio total value, the absolute of proportion and the $z-$score of our chosen confidence interval.\n",
    "\n",
    "$Z-$score is used since we assume that our returns follow a normal distribution. For example, the $z-$score of a one-tailed $99\\%$ VaR can be read in the [normal table](https://stattrek.com/online-calculator/normal) as equal to $2.32635$.\n",
    "\n",
    "In this lesson we will focus on portfolio-level VaR, which is calculated as:\n",
    "$$\\text{Portfolio Var} = \\text{Portfolio Volatility}\\times \\text{Portfolio Total Value}\\times \\text{z-score of confidence interval}$$\n",
    "where the portfolio volatility is the standard deviation of the returns in our portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "pgSrIO_C8UPh"
   },
   "source": [
    "## **2. Principal Component Analysis**\n",
    "\n",
    "Principal component analysis can be used to handle risk arising from data where variables are highly correlated. In this approach, we consider the historical data describing the market variables' movement and define factors that will explain the movements.\n",
    "\n",
    "We will consider yield data with maturities of 1 month, 3 months, 6 months, 1 year, 2 years, 3 years, 5 years, 7 years, 10 years, 20 years, and 30 years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9Y_ve68JyuGQ"
   },
   "source": [
    "### **1.1 Loading Packages**\n",
    "\n",
    "We start by loading helper packages that will help us achieve our tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "-yVoQOCGXYiy"
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "# Global Libraries\n",
    "# Disable the warnings\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import scipy as sp\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2PvZ4KxQzJhL"
   },
   "source": [
    "### **1.2 Loading Data**\n",
    "\n",
    "We download U.S. yield data ranging from 1-month to 30-year rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "rIhYe4l6dz7L"
   },
   "outputs": [],
   "source": [
    "# downloading the data\n",
    "start = datetime(2002, 6, 30)\n",
    "end = datetime(2022, 6, 30)\n",
    "data = [\n",
    "    \"DGS1MO\",\n",
    "    \"DGS3MO\",\n",
    "    \"DGS6MO\",\n",
    "    \"DGS1\",\n",
    "    \"DGS2\",\n",
    "    \"DGS3\",\n",
    "    \"DGS5\",\n",
    "    \"DGS7\",\n",
    "    \"DGS10\",\n",
    "    \"DGS20\",\n",
    "    \"DGS30\",\n",
    "]\n",
    "data = web.DataReader(data, \"fred\", start, end).dropna(how=\"all\").ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "deletable": false,
    "id": "D0NxA3HiePZI",
    "outputId": "9f6b09e0-d7e5-4d7a-8413-121395e763ad"
   },
   "outputs": [],
   "source": [
    "data.rename(\n",
    "    columns={\n",
    "        \"DGS1MO\": \"1m\",\n",
    "        \"DGS3MO\": \"3m\",\n",
    "        \"DGS6MO\": \"6m\",\n",
    "        \"DGS1\": \"1y\",\n",
    "        \"DGS2\": \"2y\",\n",
    "        \"DGS3\": \"3y\",\n",
    "        \"DGS5\": \"5y\",\n",
    "        \"DGS7\": \"7y\",\n",
    "        \"DGS10\": \"10y\",\n",
    "        \"DGS20\": \"20y\",\n",
    "        \"DGS30\": \"30y\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "df = data.copy()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9whaimEUDZKs"
   },
   "source": [
    "### **1.3 Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "EoMxQjmAeXMR",
    "outputId": "f369223f-9e0a-4680-bdaf-910ec2e355b8"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "0-Vl4PBvfSQy"
   },
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "vPhAxrbrMtEM"
   },
   "source": [
    "Now let's visualize the movement of the yield curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "deletable": false,
    "id": "iSfI837If_2i",
    "outputId": "7a201972-4e76-4f1a-a261-1cbdb2b0c931"
   },
   "outputs": [],
   "source": [
    "df.plot(figsize=(15, 8))\n",
    "pyplot.ylabel(\"Rate\")\n",
    "pyplot.legend(bbox_to_anchor=(1.01, 0.9), loc=2)\n",
    "pyplot.suptitle(\n",
    "    \"Fig. 1: Yield Curve Movement\", fontweight=\"bold\", horizontalalignment=\"right\"\n",
    ")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "lDN3RxW9qb5Y"
   },
   "source": [
    "## **2. Generating Principal Components**\n",
    "\n",
    "In this section, we study how to generate principal components from a given dataset.\n",
    "\n",
    "### **2.1 Using the Covariance Matrix**\n",
    "\n",
    "The day-to-day data from financial markets usually have two properties\n",
    "\n",
    "- Noise\n",
    "- Signal\n",
    "\n",
    "When we apply PCA to the dataset, we can extract its signal and find the minimum quantity of data that will account for the largest percentage of the whole dataset.\n",
    "\n",
    "We then detrend the data by standardizing it into z-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "deletable": false,
    "id": "IVU8TmPmZLub",
    "outputId": "0f4a8990-2557-486f-8878-3cfd897014aa"
   },
   "outputs": [],
   "source": [
    "df_std = (df - df.mean()) / df.std()\n",
    "df_std.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "xnCDhqN0MeuC"
   },
   "source": [
    "Let us derive the covariance matrix of the standardized data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "Wq067HuEZLxE",
    "outputId": "298c02e9-1e04-49fe-9571-4b97e208bd8c"
   },
   "outputs": [],
   "source": [
    "cov_matrix_array = np.array(np.cov(df_std, rowvar=False))\n",
    "cov_matrix_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "deletable": false,
    "id": "A1NL8Songe3F",
    "outputId": "c9bab209-0b8b-49ff-bbe2-8dd6efdb8ff7"
   },
   "outputs": [],
   "source": [
    "cov_df = pd.DataFrame(cov_matrix_array, columns=df.columns, index=df.columns)\n",
    "cov_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "-NaTMxcNpEDU"
   },
   "source": [
    "Then, perform eigendecomposition on the covariance matrix and find the percentage of the eigenvectors as a percentage of the total variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "592EbqG5ZLz2",
    "outputId": "42ff3873-6374-4ba7-feba-161cdbe08654"
   },
   "outputs": [],
   "source": [
    "# Perform eigendecomposition\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix_array)\n",
    "\n",
    "# Put data into a DataFrame and save to excel\n",
    "df_eigval = pd.DataFrame({\"Eigenvalues\": eigenvalues})\n",
    "\n",
    "eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "xxagmCnojV0L",
    "outputId": "6105fc20-d911-4a0c-9565-b031d02c304e"
   },
   "outputs": [],
   "source": [
    "# We calculate explained variance\n",
    "\n",
    "explained_variance = [round(variance / sum(eigenvalues), 3) for variance in eigenvalues]\n",
    "explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "kAfZzSWTZL2e",
    "outputId": "f07eff0d-9e53-4749-8df3-b2a0d1dedde2"
   },
   "outputs": [],
   "source": [
    "# Save output to Excel\n",
    "columns = [\n",
    "    \"PC1\",\n",
    "    \"PC2\",\n",
    "    \"PC3\",\n",
    "    \"PC4\",\n",
    "    \"PC5\",\n",
    "    \"PC6\",\n",
    "    \"PC7\",\n",
    "    \"PC8\",\n",
    "    \"PC9\",\n",
    "    \"PC10\",\n",
    "    \"PC11\",\n",
    "]\n",
    "df_eigvec = pd.DataFrame(eigenvectors, columns=columns, index=df.columns)\n",
    "\n",
    "df_eigvec.to_excel(\"df_eigvec.xlsx\")\n",
    "eigenvectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "deletable": false,
    "id": "zdhVpoTTte5a",
    "outputId": "a06bbea1-d4ce-4fc1-b0f7-6aef90b4fbc3"
   },
   "outputs": [],
   "source": [
    "df_eigvec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "bFNGkZMRqm_k"
   },
   "source": [
    "The indices in the table above are the maturities of the rates that we considered for this exercise while the columns in our dataframe are the factor loadings describing the rate movements.\n",
    "\n",
    "Working out the combined variation of the components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "deletable": false,
    "id": "E4brfGX_9sU3",
    "outputId": "ac2c6b09-b868-4635-bb72-e55d1ffbf0a6"
   },
   "outputs": [],
   "source": [
    "from itertools import accumulate\n",
    "\n",
    "df_eigval[\"Explained proportion\"] = df_eigval[\"Eigenvalues\"] / np.sum(\n",
    "    df_eigval[\"Eigenvalues\"]\n",
    ")\n",
    "df_eigval[\"Cumulative Explained Variance\"] = list(\n",
    "    accumulate(df_eigval[\"Explained proportion\"])\n",
    ")\n",
    "\n",
    "# Format as percentage\n",
    "df_eigval.style.format({\"Explained proportion\": \"{:.2%}\"})\n",
    "df_eigval.style.format({\"Cumulative Explained Variance\": \"{:.2%}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "deletable": false,
    "id": "MwE6rtce93FC",
    "outputId": "583b1ad3-ba33-4773-98f6-afd0b8987912"
   },
   "outputs": [],
   "source": [
    "fig, ax = pyplot.subplots()\n",
    "\n",
    "pyplot.plot(df_eigvec[\"PC1\"])\n",
    "pyplot.suptitle(\n",
    "    \"Fig. 2: The First Factor Loading\", fontweight=\"bold\", horizontalalignment=\"right\"\n",
    ")\n",
    "pyplot.ylabel(\"Factor Loadings\")\n",
    "ax.set_xticks(np.arange(11))\n",
    "ax.set_xticklabels(df.columns, rotation=\"vertical\")\n",
    "pyplot.xlabel(\"Term\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "QE64L096-hG-"
   },
   "source": [
    "The first factor loading (PC1) is the weighted combination of all the rates and represents a parallel shift in our yield curve. We can see that for one unit of the loading, the 1-month rate increases by $0.299$ basis points, the 2-month rate increases by $0.302$ basis points, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "deletable": false,
    "id": "tUpQR1JP933k",
    "outputId": "df635160-93f9-4e91-8deb-76a07d49f376"
   },
   "outputs": [],
   "source": [
    "fig, ax = pyplot.subplots()\n",
    "\n",
    "pyplot.plot(df_eigvec[\"PC2\"])\n",
    "pyplot.suptitle(\n",
    "    \"Fig. 3: The Second Factor Loading\", fontweight=\"bold\", horizontalalignment=\"right\"\n",
    ")\n",
    "pyplot.ylabel(\"Factor Loadings\")\n",
    "ax.set_xticks(np.arange(11))\n",
    "ax.set_xticklabels(df.columns, rotation=\"vertical\")\n",
    "pyplot.xlabel(\"Term\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ZIZKYNlj--NA"
   },
   "source": [
    "The second factor loading (PC2) represents a change of slope of the yield curve when the short end moves in the opposite direction of the long end zone. We can observe that the rates for 1 month to 4 years move in one direction and the rates between 5 years and 30 years move in the other direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "deletable": false,
    "id": "wshF2QZu93-6",
    "outputId": "397fe829-d83d-4489-ca7a-90d1559c952d"
   },
   "outputs": [],
   "source": [
    "fig, ax = pyplot.subplots()\n",
    "\n",
    "pyplot.plot(df_eigvec[\"PC3\"])\n",
    "pyplot.suptitle(\n",
    "    \"Fig. 4: The Third Factor Loading\", fontweight=\"bold\", horizontalalignment=\"right\"\n",
    ")\n",
    "pyplot.ylabel(\"Factor Loadings\")\n",
    "\n",
    "\n",
    "ax.set_xticklabels(df.columns, rotation=\"vertical\")\n",
    "pyplot.xlabel(\"Term\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "0qoEUh9P_m_r"
   },
   "source": [
    "The third factor loading (PC3) represents a twist of the sovereign yield curve, which happens when the short- and long-end segments move up simultaneously as the yield moves down.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "uDFHBHlb9wdN"
   },
   "source": [
    "We can now plot the scree plot to visualize the factor loadings' contribution to the variance of the dataset.\n",
    "/+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "deletable": false,
    "id": "B0ONWkOor4YJ",
    "outputId": "f566ed98-a9ee-4c60-b431-1c84c1e78c5c"
   },
   "outputs": [],
   "source": [
    "PC_values = np.arange(11)\n",
    "pyplot.plot(\n",
    "    PC_values, df_eigval[\"Explained proportion\"] * 100, \"o-\", linewidth=2, color=\"blue\"\n",
    ")\n",
    "pyplot.plot(\n",
    "    PC_values,\n",
    "    df_eigval[\"Cumulative Explained Variance\"] * 100,\n",
    "    \"o-\",\n",
    "    linewidth=2,\n",
    "    color=\"red\",\n",
    ")\n",
    "pyplot.suptitle(\"Fig. 5: Scree Plot\", fontweight=\"bold\", horizontalalignment=\"right\")\n",
    "pyplot.title(\"Scree Plot\")\n",
    "pyplot.xlabel(\"Principal Component\")\n",
    "pyplot.ylabel(\"Variance Explained\")\n",
    "pyplot.legend([\"Individual Variance\", \"Cumulative Variance\"])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "uCqAkpMWCzvX"
   },
   "source": [
    "We can see that the first two components describe more than $99\\%$ of the variance and can therefore be used to describe the yield curve movement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "NIpApBm-q_hV"
   },
   "source": [
    "Now let's calculate the principal components, which are the dot product of the standardized data and the eigenvectors.<span style='color: transparent; font-size:1%'>All rights reserved WQU WorldQuant University QQQQ</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "deletable": false,
    "id": "1nYt6Y1eqg4T",
    "outputId": "8f1faf75-7216-4aca-dd12-26d329b89801"
   },
   "outputs": [],
   "source": [
    "principal_components = df_std.dot(eigenvectors)\n",
    "principal_components.columns = df_eigvec.columns\n",
    "principal_components.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "xXg122QcsRZ8"
   },
   "source": [
    "We now plot the first three components, which account for more than 99% of the explained variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "deletable": false,
    "id": "nyoIFC_ZsFbQ",
    "outputId": "b4014475-9fbc-4bb4-bc7f-d6913bc6b762"
   },
   "outputs": [],
   "source": [
    "pyplot.plot(principal_components[[\"PC1\", \"PC2\", \"PC3\"]])\n",
    "pyplot.xlabel(\"Time (Years)\")\n",
    "pyplot.suptitle(\n",
    "    \"Fig. 6: Principal Component\", fontweight=\"bold\", horizontalalignment=\"right\"\n",
    ")\n",
    "pyplot.legend([\"PC1\", \"PC2\", \"PC3\"], bbox_to_anchor=(1.01, 0.9), loc=2)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "qKiMRxVvtXH0"
   },
   "source": [
    "The principal components do not tell us much, but from the diagram above, we can see that the first principal component is more volatile than the other two, which is expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "FIR59YxRtw0h"
   },
   "source": [
    "Below is a function that sums up the PCA process we have seen above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "CFGCp-Np0BRw"
   },
   "outputs": [],
   "source": [
    "def PCA_CALC(df, num_reconstruct):\n",
    "    df -= df.mean(axis=0) / df.std()\n",
    "    R = np.cov(df, rowvar=False)\n",
    "    eigen_values, eigen_vectors = sp.linalg.eigh(R)\n",
    "    eigen_vectors = eigen_vectors[:, np.argsort(eigen_values)[::-1]]\n",
    "    eigen_values = eigen_values[np.argsort(eigen_values)[::-1]]\n",
    "    eigen_vectors = eigen_vectors[:, :num_reconstruct]\n",
    "\n",
    "    return np.dot(eigen_vectors.T, df.T).T, eigen_values, eigen_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "TOajSWus0CMg"
   },
   "outputs": [],
   "source": [
    "scores, evals, evecs = PCA_CALC(df, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "deletable": false,
    "id": "YoEJD51P0CPQ",
    "outputId": "ecbfde0d-91ac-42f7-db85-63dfea5b89df"
   },
   "outputs": [],
   "source": [
    "fig, ax = pyplot.subplots()\n",
    "\n",
    "evecs = pd.DataFrame(evecs)\n",
    "pyplot.plot(evecs.loc[:, 0:2])\n",
    "pyplot.suptitle(\n",
    "    \"Fig. 7: The Factor Loadings\", fontweight=\"bold\", horizontalalignment=\"right\"\n",
    ")\n",
    "pyplot.legend([\"PC1\", \"PC2\", \"PC3\"], loc=\"lower right\")\n",
    "pyplot.ylabel(\"Factor Loadings\")\n",
    "ax.set_xticks(np.arange(11))\n",
    "ax.set_xticklabels(df.columns, rotation=\"vertical\")\n",
    "pyplot.xlabel(\"Term\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b8dDRJwLiXzb"
   },
   "source": [
    "As discussed in the previous subsection, PC1 reflects a (mostly) parallel shift, PC2 reflects a tilt, and PC3 reflects a twist or butterfly.\n",
    "\n",
    "We now work to reconstruct the original data from our factor loadings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "deletable": false,
    "id": "3kzmWi7b0CR0",
    "outputId": "2c851341-6787-4429-fc8e-c794d1ed9023"
   },
   "outputs": [],
   "source": [
    "reconst = pd.DataFrame(np.dot(scores, evecs.T), index=df.index, columns=df.columns)\n",
    "\n",
    "reconst.plot(figsize=(15, 8))\n",
    "pyplot.ylabel(\"Rate\")\n",
    "pyplot.xlabel(\"Years\")\n",
    "pyplot.title(\"Reconstructed Mean-Subtracted Dataset\")\n",
    "pyplot.suptitle(\n",
    "    \"Fig. 8: Reconstructed Mean-SUbtracted Dataset\",\n",
    "    fontweight=\"bold\",\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "pyplot.legend(bbox_to_anchor=(1.01, 0.9), loc=2)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "PQF0elJmvejK"
   },
   "source": [
    "We now reconstruct our original dataset from the principal components and plot the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "deletable": false,
    "id": "ltLAP60302ui",
    "outputId": "51e6aa5e-6676-43aa-dec6-bda3e73f8f9d"
   },
   "outputs": [],
   "source": [
    "for cols in reconst.columns:\n",
    "    reconst[cols] = reconst[cols] + df2.mean(axis=0)[cols]\n",
    "\n",
    "reconst.plot(figsize=(15, 8))\n",
    "pyplot.xlabel(\"Date\")\n",
    "pyplot.ylabel(\"Rates\")\n",
    "pyplot.title(\"Reconstructed Initial Dataset\")\n",
    "pyplot.suptitle(\n",
    "    \"Fig. 9: Reconstructed Initial Dataset\",\n",
    "    fontweight=\"bold\",\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "pyplot.legend(bbox_to_anchor=(1.01, 0.9), loc=2)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "q8sU_kPwvqJ-"
   },
   "source": [
    "Note that these steps can be implemented using a scikit learn module as shown below.\n",
    "\n",
    "We start by computing the correlation between the term interests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954
    },
    "deletable": false,
    "id": "RFdxXYQ_iubl",
    "outputId": "34bbcefc-5f43-4086-87b0-423b004dc6d1"
   },
   "outputs": [],
   "source": [
    "# correlation\n",
    "correlation = df.corr()\n",
    "pyplot.figure(figsize=(15, 15))\n",
    "pyplot.title(\"Correlation Matrix\")\n",
    "pyplot.suptitle(\n",
    "    \"Fig. 10: Correlation Matrix\", fontweight=\"bold\", horizontalalignment=\"right\"\n",
    ")\n",
    "sns.heatmap(correlation, vmax=1, square=True, annot=True, cmap=\"cubehelix\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hj2hYSJxv9CA"
   },
   "source": [
    "Then, standardize the dataset as explained earlier. Note that when the variable scales are different, we use a correlation matrix, and when the variable scales are similar, we use a covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "deletable": false,
    "id": "DJK09jlMjE8r",
    "outputId": "928fa054-9e36-44c3-84f2-5af1942fe292"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(df)\n",
    "rescaleddf = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "# summarize transformed data\n",
    "df.dropna(how=\"any\", inplace=True)\n",
    "rescaleddf.dropna(how=\"any\", inplace=True)\n",
    "rescaleddf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hKAIT5mKwCjT"
   },
   "source": [
    "See the visualization of the scaled dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "deletable": false,
    "id": "G-SPO3FxjVyj",
    "outputId": "6f88fae6-8446-4324-fc48-9da69daf2f12"
   },
   "outputs": [],
   "source": [
    "rescaleddf.plot(figsize=(14, 10))\n",
    "pyplot.ylabel(\"Rate\")\n",
    "pyplot.legend(bbox_to_anchor=(1.01, 0.9), loc=2)\n",
    "pyplot.suptitle(\n",
    "    \"Fig. 11: Scaled Yield Curve Plot\", fontweight=\"bold\", horizontalalignment=\"right\"\n",
    ")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gz3x0jbWwI6F"
   },
   "source": [
    "We call the PCA algorithm and fit our data to it, then data transformation takes place here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "0oKEHzFpjmLc"
   },
   "outputs": [],
   "source": [
    "model = PCA()\n",
    "PrincipalComponent = model.fit(rescaleddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "uGQR7RdHwUe0"
   },
   "source": [
    "We now visualize the explained variance of our components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "deletable": false,
    "id": "xQtBZHhikCjk",
    "outputId": "94e4753d-a4c3-4ff2-b8f7-d6c10feec6e1"
   },
   "outputs": [],
   "source": [
    "NumEigenvalues = 5\n",
    "fig, axes = pyplot.subplots(ncols=2, figsize=(14, 4))\n",
    "pd.Series(model.explained_variance_ratio_[:NumEigenvalues]).sort_values(\n",
    "    ascending=False\n",
    ").plot.bar(title=\"Explained Variance Ratio by Top Factors\", ax=axes[0])\n",
    "pd.Series(model.explained_variance_ratio_[:NumEigenvalues]).cumsum().plot(\n",
    "    ylim=(0, 1), ax=axes[1], title=\"Cumulative Explained Variance\"\n",
    ")\n",
    "\n",
    "# explained_variance\n",
    "pd.Series(np.cumsum(model.explained_variance_ratio_)).to_frame(\n",
    "    \"Explained Variance_Top 5\"\n",
    ").head(NumEigenvalues).style.format(\"{:,.2%}\".format)\n",
    "pyplot.suptitle(\"Fig. 12: Scree Plots\", fontweight=\"bold\", horizontalalignment=\"left\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "deletable": false,
    "id": "OF9i-IhIk2O-",
    "outputId": "c23be925-26a1-4916-8998-b59a88100802"
   },
   "outputs": [],
   "source": [
    "pyplot.plot(model.components_[0:3].T)\n",
    "pyplot.xlabel(\"Principal Component\")\n",
    "pyplot.suptitle(\n",
    "    \"Fig. 13: Factor Loadings\", fontweight=\"bold\", horizontalalignment=\"right\"\n",
    ")\n",
    "pyplot.legend([\"PC1\", \"PC2\", \"PC3\"], loc=\"lower right\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "JalrsiwbxSxu"
   },
   "source": [
    "Again, let's not forget that PC1 reflects a (mostly) parallel shift, PC2 reflects a tilt, and PC3 reflects a twist or butterfly.\n",
    "\n",
    "Let's create a dataframe having only the first 3 components, which is what we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "deletable": false,
    "id": "iXPvbVIY6kvC",
    "outputId": "39d3ae76-1324-4591-fff3-5a1f2960a8a9"
   },
   "outputs": [],
   "source": [
    "model = PCA().fit(rescaleddf)\n",
    "columns = [\"pca_comp_%i\" % i for i in range(11)]\n",
    "df_pca = pd.DataFrame(\n",
    "    model.transform(rescaleddf), columns=columns, index=rescaleddf.index\n",
    ")\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "_y65v60zRVcS",
    "outputId": "3273ab3f-ba2b-4949-9d92-578158ddc4b1"
   },
   "outputs": [],
   "source": [
    "# get the component variance\n",
    "# Proportion of Variance (from PC1 to PC11)\n",
    "model.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "LEz3TNv-LZ5f",
    "outputId": "b58fefe9-3a1d-4599-b51f-cc4fba976919"
   },
   "outputs": [],
   "source": [
    "# Cumulative proportion of variance (from PC1 to PC6)\n",
    "np.cumsum(model.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "deletable": false,
    "id": "AL14Dcu78JlH",
    "outputId": "8cb46edc-9117-4634-a829-a7aa9b70e225"
   },
   "outputs": [],
   "source": [
    "# component loadings or weights (correlation coefficient between original variables and the component)\n",
    "# component loadings represents the elements of the eigenvector\n",
    "# the squared loadings within the PCs always sums to 1\n",
    "loadings = model.components_\n",
    "num_pc = model.n_features_\n",
    "pc_list = [\"PC\" + str(i) for i in list(range(1, num_pc + 1))]\n",
    "loadings_df = pd.DataFrame.from_dict(dict(zip(pc_list, loadings)))\n",
    "loadings_df[\"variable\"] = df.columns.values\n",
    "loadings_df = loadings_df.set_index(\"variable\")\n",
    "loadings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "deletable": false,
    "id": "LJKNvHpjUYe5",
    "outputId": "55c65265-8957-4417-f080-e5ed2e78b7ff"
   },
   "outputs": [],
   "source": [
    "# positive and negative values in component loadings reflects the positive and negative\n",
    "# correlation of the variables with the PCs. Except A and B, all other variables have\n",
    "# positive projection on first PC.\n",
    "\n",
    "# get correlation matrix plot for loadings\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(rc={\"figure.figsize\": (11.7, 8.27)})\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = sns.heatmap(loadings_df, annot=True, cmap=\"Spectral\")\n",
    "pyplot.suptitle(\n",
    "    \"Fig. 14: Correlatin matrix for Loadings\",\n",
    "    fontweight=\"bold\",\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "MQE9J_tpUYqG",
    "outputId": "969c8a6c-f126-495d-edd9-7bb3d9fd19fa"
   },
   "outputs": [],
   "source": [
    "# get eigenvalues (variance explained by each PC)\n",
    "model.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ND5K90RUA-LL"
   },
   "source": [
    "So far, the three approaches we have applied have yielded similar results. So we want to plot the biplot of the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "OqdGKA0z-sLL"
   },
   "outputs": [],
   "source": [
    "from pca import pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "_jRe25qX-qp7",
    "outputId": "64fc8cd6-daf8-4b34-cd25-1916c6db5f53"
   },
   "outputs": [],
   "source": [
    "# Or reduce the data towards 2 PCs\n",
    "model = pca(n_components=2)\n",
    "\n",
    "# Fit transform\n",
    "results = model.fit_transform(rescaleddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "deletable": false,
    "id": "hTBfZESlPiys",
    "outputId": "415b641b-dce2-4e8d-82f6-2f9c7c6703dc"
   },
   "outputs": [],
   "source": [
    "# Make biplot with the number of features\n",
    "print(\"\\033[1m\" + \"Fig. 15: Biplot without the scores\" + \"\\033[0m\")\n",
    "fig, ax = model.biplot(cmap=None, label=False, legend=False, figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "QuVjWbA43ya1"
   },
   "source": [
    "From the biplot, we can see short-term interest rates are highly correlated, and this can also be said of the long-term interest rates, this is because the angle between them is smaller. Comparing short-term and long-term interest rates, we can conclude that they are negatively correlated as the angle between them is wider. The mid-term interest rates seem not to be correlated with both the short- and long-term interest rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "FjeunLu_ydyk"
   },
   "source": [
    "## **3. Computing Value At Risk (VaR) using PCA**\n",
    "\n",
    "In this section, we will demonstrate how we can use PCA to calculate VaR. For the purposes of this lesson, let us assume we have a portfolio with the exposures to interest rate variation as shown in the table below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "deletable": false,
    "id": "GW5UlKCjDdEZ",
    "outputId": "bce3d8c9-9551-411c-c9ba-914fe237c95e"
   },
   "outputs": [],
   "source": [
    "df_portfolio = pd.DataFrame(\n",
    "    {\n",
    "        \"2 year rate\": [10],\n",
    "        \"3 year rate\": [4],\n",
    "        \"5 year rate\": [-8],\n",
    "        \"7 year rate\": [-7],\n",
    "        \"10 year rate\": [2],\n",
    "    }\n",
    ")\n",
    "\n",
    "df_portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3PEHFdQKFUin"
   },
   "source": [
    "The table reads as follows: A $1$ basis point would see our portfolio value increase by $\\$ 10$ million considering the 2-year rate; similarly, we will see a $\\$ 4$ million portfolio increase in a a 3-year rate, and so on.\n",
    "\n",
    "From our scree plot, we saw that two-factor loadings were sufficient to model our yield rates as they account for more than $99 \\%$ of the data variance. Recall our loadings dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "deletable": false,
    "id": "mVA6tM3HILOa",
    "outputId": "cae9780d-92dc-4153-a0d4-ce34748c9df1"
   },
   "outputs": [],
   "source": [
    "df_eigvec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "NoqCcLXhIRle"
   },
   "source": [
    "We can now calculate the exposure brought about by the first factor as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "UfHKuJyiIbkk",
    "outputId": "2ac816e6-726e-46bc-89fe-9e5dea01b9aa"
   },
   "outputs": [],
   "source": [
    "10 * 0.3169 + 4 * 0.322 - 8 * 0.3231 - 7 * 0.3143 + 2 * 0.2987"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "cYpikqxsJjy1"
   },
   "source": [
    "The exposure due to the second factor will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "ffpnRvkTJiv2",
    "outputId": "24147b82-7a74-458f-ed4e-2eb5003dfdf1"
   },
   "outputs": [],
   "source": [
    "10 * 0.1856 + 4 * 0.0994 + 8 * 0.0798 + 7 * 0.2122 - 2 * 0.3285"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Amzl4gqGKrGl"
   },
   "source": [
    "The first factor has very little exposure as compared to the second factor.\n",
    "\n",
    "The change in the portfolio value becomes\n",
    "$$\\Delta P = 0.2695 f_1 + 3.7204 f_2$$\n",
    "\n",
    "Recall the loadings variance below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "deletable": false,
    "id": "yKhxlb1OHnEP",
    "outputId": "fb1d8cbf-4e88-442f-d4e9-9c2191ab5e51"
   },
   "outputs": [],
   "source": [
    "df_eigval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "OYE2tH1EL_xw"
   },
   "source": [
    "We can calculate the standard deviation (factor score) as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "deletable": false,
    "id": "swb3sW7vMHKI",
    "outputId": "e179cca9-df47-4ef8-e172-48475c9d1099"
   },
   "outputs": [],
   "source": [
    "df_eigval[\"Factor_store\"] = np.sqrt(df_eigval[\"Eigenvalues\"])\n",
    "df_eigval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "suowcEHxMU8A"
   },
   "source": [
    "The standard deviation of $\\Delta P$ will therefore be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "mgiFnMn0Mlw0",
    "outputId": "cc13f196-49c6-4294-e3dc-94576d98f3ee"
   },
   "outputs": [],
   "source": [
    "0.2695 * 3.0502 + 3.7204 * 1.2463"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "KmSjIwVPOKnw"
   },
   "source": [
    "Finally, the one day $99\\%$ VaR is given by\n",
    "$$\\sigma(\\Delta P) \\times Z_{\\alpha} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "Kjguoa66O5NT",
    "outputId": "f9e9166c-34e5-421d-cec0-28b265fcb967"
   },
   "outputs": [],
   "source": [
    "5.45876342 * 2.32635"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gGkgziE242He"
   },
   "source": [
    "## **4. Conclusion**\n",
    "\n",
    "In this lesson, we have applied principal component analysis concept on yield rate data and seen how we can arrive at the same conclusion using three different approaches. We have used results to calculate VaR for our portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7-X-AarjuF9Y"
   },
   "source": [
    "**References**\n",
    "\n",
    "1. Carcano, Nicola. \"Yield Curve Risk Management: Adjusting Principal Component Analysis for Model Errors.\" *The Journal of Risk*, vol. 12, no. 1, 2009, pp. 3–16.\n",
    "2. Oprea, Andrea. \"The Use of Principal Component Analysis (PCA) in Building Yield Curve Scenarios and Identifying Relative-Value Trading Opportunities on the Romanian Government Bond Market.\" *Journal of Risk and Financial Management*, vol. 15, no. 6, 2022. https://www.mdpi.com/1911-8074/15/6/247/htm.\n",
    "3. Stat Trek. \"Normal Distribution Calculator.\" https://stattrek.com/online-calculator/normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "Copyright 2023 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
