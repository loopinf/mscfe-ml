{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5AxF11X9tHvo"
   },
   "source": [
    "\n",
    "## MACHINE LEARNING IN FINANCE\n",
    "MODULE 2 | LESSON 1\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "J4ceGgiItOf0"
   },
   "source": [
    "# **UNSUPERVISED MACHINE LEARNING: CLUSTERING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MELzJ6aRtXsV"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  65 minutes |\n",
    "|**Prior Knowledge** | Linear Algebra, machine learning  |\n",
    "|**Keywords** |Clustering, K-means, elbow plot  |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "mg7hqOaYt6Be"
   },
   "source": [
    "*In this lesson, we will learn the difference between supervised learning and unsupervised learning and their applications. Once we have a good understanding of unsupervised learning applications, we will study its foundational techniques. At the end of the lesson, we will implement the k-means clustering algorithm using built-in Python packages.*<span style='color: transparent; font-size:1%'>All rights reserved WQU WorldQuant University QQQQ</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8gUhudI8x-84"
   },
   "source": [
    "## **1. Introduction**\n",
    "\n",
    "In supervised learning, the output (target variable) that we want to predict is known.  That allows us to assess the quality of our model by applying metrics such as recall, accuracy, mean squared error, etc., which are used to evaluate the performance of the model. In unsupervised learning, the target variable is not available. The unsupervised model helps in finding patterns in our clustered data without any associated response.\n",
    "\n",
    "The table below lists some of the differences between unsupervised and supervised models.\n",
    "\n",
    "Tables:\n",
    "\n",
    "Unsupervised       | Supervised \n",
    "-------------------|------------------\n",
    "No labels provided | Labels are provided\n",
    "Find or explore patterns in unlabeled data       | Find or explore patterns in existing structure\n",
    "Uses techniques such as clustering and dimensionality reduction | Uses techniques such as regression and classification\n",
    "\n",
    "A supervised classification algorithm can be used to categorize given data into two known groups while an unsupervised clustering algorithm will tell us how the data is structured.\n",
    "\n",
    "Unsupervised machine learning is applied under the following scenarios\n",
    "- When the data has no output/target variable.\n",
    "- When we want to understand patterns in the data.\n",
    "- To pick out essential information (a process called dimensionality reduction) from the data and use it to train a supervised model.\n",
    "\n",
    "There are a variety of applications of unsupervised machine learning in the financial industry. Here, we list just a few:\n",
    "- Asset allocation\n",
    "- Yield curve construction and interest rate modeling\n",
    "- Trading to enhance speed and accuracy\n",
    "- Pairs trading\n",
    "- Portfolio management to cluster investors\n",
    "\n",
    "In the rest of this lesson, we will study a branch of unsupervised machine learning called **clustering**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "glpt7qW3myBZ"
   },
   "source": [
    "## **2. Clustering**\n",
    "\n",
    "Clustering is a process of organizing data points into groups such that members of each group share some similar attributes. In the plot below, we can say that elements in Cluster 0 are similar to each other but different from elements in another cluster, say Cluster 1.\n",
    "\n",
    "**Fig 1: A Plot with Four Clusters**\n",
    "\n",
    "<center><img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_plusplus_001.png\" alt=\"Clustering 2\" width=\"400\"></center>\n",
    "\n",
    "##### Source: [scikit-learn](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_plusplus.html)\n",
    "\n",
    "As seen in the above plot, we can cluster (group) the data points by using the distance metrics between adjacent datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "pz-9XCNPQvgK"
   },
   "source": [
    "### **2.1 Distance-Based Clustering**\n",
    "\n",
    "Given a set of data points, we can use the distance between the points to group the points into clusters such that\n",
    "- Internal distances between the individual points (intra-cluster) should be small, that is, the points of a given cluster should be close to each other.\n",
    "- External distances between the points (inter-cluster) should be large, that is, the points in different clusters are dissimilar.\n",
    "\n",
    "### **2.2 The Goals of Clustering**\n",
    "\n",
    "Given a set of unlabeled data, we will apply clustering to find the structure and groupings that exist in the data. There exists no perfect criterion for doing clustering, and it is upon the user to supply a criterion that will ensure the result matches their needs.\n",
    "\n",
    "For any clustering method we opt to use, we need to define an appropriate **similarity measure** to be applied in finding the clusters.\n",
    "\n",
    "### **2.3 Proximity Measures**\n",
    "\n",
    "When we do clustering, a proximity measure between two points must be defined where proximity is the data similarity or dissimilarity with respect to each other. The similarity measure $S(x_i, x_k)$ is large if the points $x_i, x_k$ are similar while the dissimilarity measure $D(x_i x_k)$ is small if the points $x_i, x_k$ are similar.\n",
    "\n",
    "Below are the similarity measures that we can use:\n",
    "\n",
    "1. Vectors: Cosine Distance\n",
    "$$s(\\vec{x}, x') = \\frac{x^t x'}{||x||||x'||}$$\n",
    "\n",
    "2. Sets: Jaccard Distance\n",
    "$$J(A, B) = \\frac{|A\\cap B|}{|A\\cup B|} = \\frac{|A\\cap B|}{|A| + |B| - |A\\cap B|}$$\n",
    "If sets $A$ and $B$ are both empty, then we define $J(A, B)=1$ otherwise $0\\leq J(A, B) \\leq 1$\n",
    "\n",
    "3. Points: Euclidean Distance\n",
    "$$d(x, x') = \\left(\\sum_{k=1}^p \\left|x_k - x'_k\\right|^q\\right)^{1/q}$$\n",
    "\n",
    "\n",
    "### **2.4 Clustering Algorithms**\n",
    "\n",
    "Clustering algorithms can be categorized into four types as listed below:\n",
    "1. Exclusive Clustering\n",
    "2. Overlapping Clustering\n",
    "3. Hierarchical Clustering\n",
    "4. Probabilistic Clustering\n",
    "\n",
    "In exclusive clustering, data is clustered exclusively such that a point belonging to one cluster can not be included in another cluster. \n",
    "\n",
    "Overlapping clustering uses fuzzy sets to cluster data such that a point can belong to more than one cluster but with varying levels of membership. We therefore group data according to an appropriate membership value. \n",
    "\n",
    "In hierarchical clustering, every data point is treated as a cluster and at each iteration we find the union between the two nearest clusters until we reach the number of clusters we wanted. \n",
    "\n",
    "Finally, a probabilistic approach clusters each point based on their probability of belonging to a particular cluster.\n",
    "\n",
    "Now let's look at some of the most commonly used clustering algorithms:\n",
    "- K-means\n",
    "- Fuzzy K-means\n",
    "- Hierarchical clustering\n",
    "- Mixture of Gaussians\n",
    "\n",
    "K-means is an example of an exclusive clustering algorithm. Fuzzy K-means is an example of an overlapping clustering algorithm. Hierarchical clustering is just that, an example of a Hierarchical clustering algorithm. A mixture of Gaussians is an example of a probabilistic clustering algorithm.\n",
    "\n",
    "Let's get a better understanding of each clustering method above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "0maR2i9yQUpJ"
   },
   "source": [
    "## **3. K-Means Clustering**\n",
    "\n",
    "K-means clustering works by classifying a dataset through a defined number of clusters that is fixed a priori.\n",
    "\n",
    "Here is a walkthrough of the k-means algorithm:\n",
    "1. We start by selecting k centroids where k is the number of clusters.\n",
    "2. We then place the k centroids in our training data at different random places.\n",
    "3. We then calculate the distance from each point in the training data to each centroid.\n",
    "4. The points are then grouped based on the nearest centroid.\n",
    "5. The grouped points are isolated together with their respective centroid. Find the mean of each group, then move the centroid to the location of the mean we have calculated.\n",
    "6. We repeat the steps until we reach an optimal number of k groups.\n",
    "\n",
    "**Fig 2: A K-means clustering procedure flow chart**\n",
    "\n",
    "<center><img src='https://www.researchgate.net/profile/Alhadi-Bustamam/publication/318341309/figure/fig1/AS:514967923159041@1499789328422/Flowchart-of-k-means-clustering-algorithm.png'></center>\n",
    "\n",
    "##### Source: [Bustamam, A. et al. \"Application of K-means Clustering Algorithm in Grouping the DNA Sequences of Hepatitis B Virus (HBV).\" *AIP Conference Proceedings*, vol. 1862. no. 1. AIP Publishing LLC, 2017.](https://aip.scitation.org/doi/abs/10.1063/1.4991238)\n",
    "\n",
    "Mathematically, the objective of the K-means algorithm is to minimize an objective function which in our case is a squared error function given by \n",
    "$$J=\\sum_{j=1}^k \\sum_{i=1}^n ||x_i^{j} - C_j||^2$$\n",
    "where \n",
    "\n",
    "$$||x_i^{j} - C_j||^2$$\n",
    "is the Euclidean distance between data points $x_i$ and the centroid $C_j$.\n",
    "\n",
    "Below is the mathematical walkthrough of the algorithm:\n",
    "1. Let $X = \\{x_1, x_2, \\cdots, x_n\\}$ be our data points and $V = \\{v_1, v_2, \\cdots, v_c\\}$ be the centroids.\n",
    "2. Select centroids randomly.\n",
    "3. Then, calculate the distance between the centroids and each data point.\n",
    "4. We then assign the data points to the centroids closest to them.\n",
    "5. We recalculate the new centroids by computing the mean of the grouping,\n",
    "$$v_i = \\left(\\frac{1}{c_i}\\right)\\sum_{j=1}^{c_i} x_j$$\n",
    "where $c_i$ is the number of data points in the ith cluster.\n",
    "6. Find the distance between the new centroid and each data point.\n",
    "7. If there is no change in the composition of the clusters then we stop, or else we repeat the procedure from step 3.\n",
    "\n",
    "The issue with the K-means algorithm is that the number of clusters need to be selected before we begin modeling. We can guess the number of clusters using either the Silhoutte score or the elbow method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "sBSD1FZjfg2H"
   },
   "source": [
    "### **3.1 Silhouette Analysis**\n",
    "\n",
    "Silhoutte analysis is applied in clustering to investigate the separation distance between our clusters. We use the Silhoutte plot to visualize the distance of the points in one cluster to points in the neighboring clusters. The measure has a range of $[-1, 1]$.\n",
    "\n",
    "Silhoutte coefficients nearer to $+1$ show that the data points are far away from the adjacent clusters. A coefficient of $0$ shows that the data points are very close to the neighboring clusters. Negative coefficients shows that the data points are probably assigned to the wrong cluster.\n",
    "\n",
    "The Silhoutte coefficient is given by\n",
    "$$S = \\frac{b-a}{\\max(a,b)}$$\n",
    "An example of a Silhoutte diagram is shown below.\n",
    "\n",
    "**Fig 3: Silhoutte Analysis on Dataset with 3 Clusters**\n",
    "<center><img src = 'https://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_silhouette_analysis_002.png'></center>\n",
    "\n",
    "##### Source: [scikit-learn, Selecting the number of clusters with silhouette analysis on KMeans clustering¶](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html?highlight=silhouette)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "DUqEZhg1fmBZ"
   },
   "source": [
    "### **3.2 Elbow Method**\n",
    "\n",
    "We can construct an elbow plot to find the appropriate value of $k$ to use in our clustering.\n",
    "\n",
    "We find the optimal number of clusters (groups) by drawing a line plot of Within the Sum of Squares (WSS) versus the number of clusters. We define WSS as the sum of square difference between each point in a cluster and its cluster center. An elbow plot is shown in the figure below:\n",
    "\n",
    "**Fig 4: An Elbow Plot Showing the Number of Optimal Clusters**\n",
    "\n",
    "<center><img src = 'https://editor.analyticsvidhya.com/uploads/43191elbow_img%20(1).png'></center>\n",
    "\n",
    "##### Source: [Tripathi, Shreya et al. \"Approaches to Clustering in Customer Segmentation.\" International Journal of Engineering & Technology, 2012, vol. 7, no. 3.12, 2018, pp. 802-807.](https://www.sciencepubco.com/index.php/ijet/article/view/16505)\n",
    "\n",
    "Note that in the K-means algorithm, we do not have a general method for getting a unique optimal number of clusters.\n",
    "\n",
    "We now put K-means into action with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "_t9dtw1bxokG"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "UadhgBZX2Duu"
   },
   "source": [
    "We generate a dataset with 4 clusters from the scikit-learn library 'make_blobs' and plot it for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "deletable": false,
    "id": "jkGdcyi2zZmS",
    "outputId": "a9a10078-1804-471f-cbf4-932d075c6ad3"
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(centers=4, n_samples=2500)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.title(\"Dataset with 4 clusters\", fontsize=20)\n",
    "plt.xlabel(\"First feature\")\n",
    "plt.ylabel(\"Second feature\")\n",
    "plt.suptitle(\n",
    "    \"Fig. 5: Dataset Visualization\", fontweight=\"bold\", horizontalalignment=\"right\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "xg1VPeYB1a6x"
   },
   "source": [
    "In the plot above, we see that some clusters intersect with each other and this poses a beautiful problem for us to investigate and see if our algorithm will be able to distinguish them.\n",
    "\n",
    "let's plot the elbow curve to establish if we will get the expected 4 clusters that we have generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "deletable": false,
    "id": "bqqhsY7Tze5L",
    "outputId": "39897e9d-1f91-4458-db0f-c96b4d627717"
   },
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=i, init=\"k-means++\", max_iter=300, n_init=10, random_state=42\n",
    "    )\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 5)\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title(\"K-Means Clustering(The Elbow Method)\", fontsize=20)\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"WCSS\")\n",
    "plt.suptitle(\"Fig. 6: The Elbow Method\", fontweight=\"bold\", horizontalalignment=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "p7xB6wV72xX9"
   },
   "source": [
    "Note that after the value of $k = 4$, the WCSS decreases very slowly. Therefore, our value of $k$ is equal to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "deletable": false,
    "id": "k_ir_a3kzhJS",
    "outputId": "f9f239e2-1bad-48a0-8bc5-222c76594480"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, init=\"k-means++\", max_iter=300, n_init=10, random_state=0)\n",
    "ymeans = kmeans.fit_predict(X)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "plt.title(\"Cluster of Dataset\", fontsize=30)\n",
    "\n",
    "plt.scatter(X[ymeans == 0, 0], X[ymeans == 0, 1], s=100, c=\"pink\", label=\"Cluster 0\")\n",
    "plt.scatter(X[ymeans == 1, 0], X[ymeans == 1, 1], s=100, c=\"orange\", label=\"Cluster 1\")\n",
    "plt.scatter(\n",
    "    X[ymeans == 2, 0], X[ymeans == 2, 1], s=100, c=\"lightgreen\", label=\"Cluster 2\"\n",
    ")\n",
    "plt.scatter(X[ymeans == 3, 0], X[ymeans == 3, 1], s=100, c=\"red\", label=\"Cluster 3\")\n",
    "plt.scatter(\n",
    "    kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=50, c=\"black\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"First Feature\")\n",
    "plt.ylabel(\"Second Feature\")\n",
    "plt.legend()\n",
    "# `plt.suptitle`('Fig 10: Clustering Results', fontsize=20, y = 1, ha = 'right')\n",
    "plt.suptitle(\n",
    "    \"Fig. 7: Clustering Results\", fontweight=\"bold\", horizontalalignment=\"right\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "a_5dH5533XBq"
   },
   "source": [
    "We have trained the K-means algorithm in our dataset and we can see that the output shows clear distinction between different clusters.\n",
    "\n",
    "In the next subsection, we will discuss the expectation-maximization approach of finding the composition of the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7W7wZGIfCESl"
   },
   "source": [
    "### **3.3 k-Means Algorithm: Expectation–Maximization**\n",
    "\n",
    "The Expectation-Maximization (E-M) procedure is as follows.\n",
    "1. Randomly select the cluster centers.\n",
    "2. Repeat the following steps until convergence is reached:\n",
    "- E-Step: Points are assigned to the nearest cluster center.\n",
    "- M-Step: Find the mean of each cluster and replace the cluster center with the mean.\n",
    "\n",
    "We can visualize the E-M algorithm in action below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "deletable": false,
    "id": "LLkRHyw8FBBN",
    "outputId": "3584b35c-7aa2-4a41-f08d-5db49520f3ce"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "centers = [0, 4] + rng.randn(4, 2)\n",
    "\n",
    "\n",
    "def draw_points(ax, c, factor=1):\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=c, cmap=\"viridis\", s=50 * factor, alpha=0.3)\n",
    "\n",
    "\n",
    "def draw_centers(ax, centers, factor=1, alpha=1.0):\n",
    "    ax.scatter(\n",
    "        centers[:, 0],\n",
    "        centers[:, 1],\n",
    "        c=np.arange(4),\n",
    "        cmap=\"viridis\",\n",
    "        s=200 * factor,\n",
    "        alpha=alpha,\n",
    "    )\n",
    "    ax.scatter(centers[:, 0], centers[:, 1], c=\"black\", s=50 * factor, alpha=alpha)\n",
    "\n",
    "\n",
    "def make_ax(fig, gs):\n",
    "    ax = fig.add_subplot(gs)\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    return ax\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15, 4))\n",
    "gs = plt.GridSpec(\n",
    "    4, 15, left=0.02, right=0.98, bottom=0.05, top=0.95, wspace=0.2, hspace=0.2\n",
    ")\n",
    "ax0 = make_ax(fig, gs[:4, :4])\n",
    "ax0.text(\n",
    "    0.98,\n",
    "    0.98,\n",
    "    \"Random Initialization\",\n",
    "    transform=ax0.transAxes,\n",
    "    ha=\"right\",\n",
    "    va=\"top\",\n",
    "    size=16,\n",
    ")\n",
    "draw_points(ax0, \"gray\", factor=2)\n",
    "draw_centers(ax0, centers, factor=2)\n",
    "\n",
    "for i in range(3):\n",
    "    ax1 = make_ax(fig, gs[:2, 4 + 2 * i : 6 + 2 * i])\n",
    "    ax2 = make_ax(fig, gs[2:, 5 + 2 * i : 7 + 2 * i])\n",
    "\n",
    "    # E-step\n",
    "    y_pred = pairwise_distances_argmin(X, centers)\n",
    "    draw_points(ax1, y_pred)\n",
    "    draw_centers(ax1, centers)\n",
    "\n",
    "    # M-step\n",
    "    new_centers = np.array([X[y_pred == i].mean(0) for i in range(4)])\n",
    "    draw_points(ax2, y_pred)\n",
    "    draw_centers(ax2, centers, alpha=0.3)\n",
    "    draw_centers(ax2, new_centers)\n",
    "    for i in range(4):\n",
    "        ax2.annotate(\n",
    "            \"\",\n",
    "            new_centers[i],\n",
    "            centers[i],\n",
    "            arrowprops=dict(arrowstyle=\"->\", linewidth=1),\n",
    "        )\n",
    "\n",
    "    # Finish iteration\n",
    "    centers = new_centers\n",
    "    ax1.text(\n",
    "        0.95, 0.95, \"E-Step\", transform=ax1.transAxes, ha=\"right\", va=\"top\", size=14\n",
    "    )\n",
    "    ax2.text(\n",
    "        0.95, 0.95, \"M-Step\", transform=ax2.transAxes, ha=\"right\", va=\"top\", size=14\n",
    "    )\n",
    "\n",
    "\n",
    "# Final E-step\n",
    "y_pred = pairwise_distances_argmin(X, centers)\n",
    "axf = make_ax(fig, gs[:4, -4:])\n",
    "draw_points(axf, y_pred, factor=2)\n",
    "draw_centers(axf, centers, factor=2)\n",
    "axf.text(\n",
    "    0.98,\n",
    "    0.98,\n",
    "    \"Final Clustering\",\n",
    "    transform=axf.transAxes,\n",
    "    ha=\"right\",\n",
    "    va=\"top\",\n",
    "    size=16,\n",
    ")\n",
    "plt.suptitle(\n",
    "    \"Fig. 8: Expectation-Maximization\", fontweight=\"bold\", horizontalalignment=\"right\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "xKXnjiK6FVN0"
   },
   "source": [
    "The limitations of the E-M algorithm are:\n",
    "- We may not achieve a globally optimal solution.\n",
    "- We must choose the number of clusters before modeling.\n",
    "- K-means will be slow for big data.\n",
    "- K-means only works with linear cluster boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "CfPEDzM1goM1"
   },
   "source": [
    "## **4. Fuzzy K-Means Clustering**\n",
    "\n",
    "Fuzzy clustering is the same as the K-means algorithm with the difference being that each point has some measure (probability) of belonging to each cluster in our dataset. In this case, a point might belong to 2 or more clusters if the point is at the middle of the clusters.\n",
    "\n",
    "The other processes in K-means such as random initialization, iteration, and algorithm termination are the same in fuzzy K-means. As such, the clusters are treated as probabilistic distributions. By converting the probability function to a binary distribution giving 1 for data points close to a centroid and 0 otherwise, the fuzzy K-means becomes the usual K-means.\n",
    "\n",
    "Below is the procedure for the fuzzy K-means algorithm;\n",
    "\n",
    "- First, fix the number of clusters.\n",
    "- Initialize the centroids randomly and compute the probability of the data points belonging to the different clusters.\n",
    "- Knowing the probability of each data point belonging to a cluster, we recalculate new centroids by computing the mean of the clusters.\n",
    "$$v_k (n+1) = \\frac{\\sum_{x_i\\in k} x_i P(v_k|x_i)^b}{\\sum_{x_i\\in k}P(v_k|x_i)^b}$$\n",
    "- The algorithm is terminated when it converges or when we reach the number of specified iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ucFecdhGjL1V"
   },
   "source": [
    "## **5. Hierarchical Clustering**\n",
    "\n",
    "In hierarchical clustering we define a distance matrix using linkage with the following parameters\n",
    "- Method: To calculate the distance between clusters.\n",
    "- Metric: The distance metric to be used.\n",
    "- Optimal ordering: To order data points.\n",
    "\n",
    "The different types of methods we use in hierarchical clustering are:\n",
    "\n",
    "1. Single: Finds the distance between the closest points in different clusters.\n",
    "2. Complete: Finds the distance between the farthest points in different clusters.\n",
    "3. Average: Finds the distance between the arithmetic mean of our clusters.\n",
    "4. Centroids: Finds the distance between the geometric mean of our clusters.\n",
    "5. Median: Finds the distance between the median of our clusters.\n",
    "6. Ward: Finds distance based on our sum of squares.\n",
    "\n",
    "Below are Python scripts on how to implement the different methods.\n",
    "\n",
    "### **5.1 Ward Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "deletable": false,
    "id": "UwVS8SPVClBT",
    "outputId": "e71fc288-92a8-4914-cd8c-0b6ed4fbaf1b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X1, y1 = make_blobs(\n",
    "    n_samples=50, centers=[[4, 4], [-2, -1], [1, 1], [10, 4]], cluster_std=0.9\n",
    ")\n",
    "df = pd.DataFrame(X1, columns=[\"X\", \"y\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "deletable": false,
    "id": "g1zcURiOCyqC",
    "outputId": "cce7e92a-1849-4e7d-cc3d-ecf012a26feb"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "\n",
    "# Use the linkage()\n",
    "distance_matrix = linkage(df[[\"X\", \"y\"]], method=\"ward\", metric=\"euclidean\")\n",
    "\n",
    "# Assign cluster labels\n",
    "df[\"cluster_labels\"] = fcluster(distance_matrix, 2, criterion=\"maxclust\")\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x=\"X\", y=\"y\", hue=\"cluster_labels\", data=df)\n",
    "plt.suptitle(\"Fig. 9: Ward Method\", fontweight=\"bold\", horizontalalignment=\"right\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dUcDBB4SOlZ3"
   },
   "source": [
    "### **5.2 Single Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "deletable": false,
    "id": "fytvhQsLGSm1",
    "outputId": "94af5dfd-1009-4d68-a1ad-3426d747a3cc"
   },
   "outputs": [],
   "source": [
    "# Use the linkage()\n",
    "distance_matrix = linkage(df[[\"X\", \"y\"]], method=\"single\", metric=\"euclidean\")\n",
    "\n",
    "# Assign cluster labels\n",
    "df[\"cluster_labels_2\"] = fcluster(distance_matrix, 2, criterion=\"maxclust\")\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x=\"X\", y=\"y\", hue=\"cluster_labels_2\", data=df)\n",
    "plt.suptitle(\"Fig. 10: Single Method\", fontweight=\"bold\", horizontalalignment=\"right\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "XRdvHWYVOrxq"
   },
   "source": [
    "### **5.3 Complete Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "deletable": false,
    "id": "Hh8kIRkTGSsz",
    "outputId": "294bb987-a324-4979-a33f-6f491f6c1617"
   },
   "outputs": [],
   "source": [
    "# Use the linkage()\n",
    "distance_matrix = linkage(df[[\"X\", \"y\"]], method=\"complete\", metric=\"euclidean\")\n",
    "\n",
    "# Assign cluster labels\n",
    "df[\"cluster_labels_3\"] = fcluster(distance_matrix, 2, criterion=\"maxclust\")\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x=\"X\", y=\"y\", hue=\"cluster_labels\", data=df)\n",
    "plt.suptitle(\"Fig. 11: Complete Method\", fontweight=\"bold\", horizontalalignment=\"right\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "VBj83GLmODo1"
   },
   "source": [
    "### **5.4 Dendrogram**\n",
    "\n",
    "Dendrograms help us visualize how clusters are formed, and from them, we can choose the number of clusters in our dataset. Below is simple code that gives us a dendrogram diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "deletable": false,
    "id": "x9Df7vmhGTF0",
    "outputId": "19e2849e-c061-47d9-ebfb-5ec264915ef4"
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "# Create a dendrogram\n",
    "dn = dendrogram(distance_matrix)\n",
    "plt.suptitle(\"Fig. 11: Dendrogram\", fontweight=\"bold\", horizontalalignment=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "yOIGpcyrOJEF"
   },
   "source": [
    "In summary, the following are steps taken by hierarchical clustering:\n",
    "1. We treat each point in our dataset as its own cluster.\n",
    "2. We then calculate the Euclidean distance between the centroids of all clusters in our dataset.\n",
    "3. We join the closest clusters together.\n",
    "4. We repeat steps 2 and 3 until we get only one cluster that contains all the data points.\n",
    "5. A dendogram that shows the evolution of the hierarchical structure is plotted showing how clusters are arranged from top to bottom.\n",
    "6. It is now up to us to decide at which level we want to create our clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "tlZwktgzku15"
   },
   "source": [
    "## **6. Gaussian Mixture Models (GMM)**\n",
    "\n",
    "This is a model-based approach that uses several models to generate clusters and attempts to optimize the model fit on the data.\n",
    "\n",
    "The advantages of this type of clustering includes that the possibility of one point belonging to several clusters, a density approximation for each cluster, the availability of robust statistical inference methods to do our clustering, and the freedom to choose the component distribution of the model.\n",
    "\n",
    "The K component distributions that make up a mixture model are collectively combined to give $$f(x) = \\sum_{k= 1}^K a_k f_k(x)$$\n",
    "where $a_k$ is the $k$-th component contribution in building $f(x)$.\n",
    "\n",
    "Like the K-means algorithm above, GMM is also used to find the number of clusters in a dataset. Consider the example below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "ylQJGr_-88Ca"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate some data\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "sns.set()\n",
    "\n",
    "X, y_true = make_blobs(n_samples=400, centers=4, cluster_std=0.60, random_state=0)\n",
    "X = X[:, ::-1]  # flip axes for better plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "pHxSLBOp9hkq"
   },
   "source": [
    "\n",
    "\n",
    "To visualize the number of clusters, we fit our Gaussian mixture model with a pre-specified number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "deletable": false,
    "id": "Gt0WY_U8-NUS",
    "outputId": "626f6c4f-3523-41e4-a41a-68049d29d38d"
   },
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "\n",
    "gmm = mixture.GaussianMixture(n_components=4).fit(X)\n",
    "labels = gmm.predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap=\"viridis\")\n",
    "plt.suptitle(\n",
    "    \"Fig. 12: Clusters from GMM\", fontweight=\"bold\", horizontalalignment=\"right\"\n",
    ")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1nGqW2eD_DaW"
   },
   "source": [
    "The Gaussian mixture model contains a probabilistic model that gives the probability of a point belonging to a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "bD758o7G_BZi",
    "outputId": "ea356820-568e-4b52-81b4-46c0160f5c58"
   },
   "outputs": [],
   "source": [
    "probs = gmm.predict_proba(X)\n",
    "print(probs[:5].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "N2egLS1Q88nc"
   },
   "source": [
    "The Gaussian mixture model like the k-means algorithm uses the expectation-maximization approach below.\n",
    "1. We choose a random starting point.\n",
    "2. Repeat the following until convergence.\n",
    "- E-Step: Find the weight of each point belonging to a given cluster.\n",
    "- M-Step: Update the location and shape of each cluster based on the data point weights.\n",
    "\n",
    "GMM will at times miss the globally optimal solution, and it is therefore recommended to use different random initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "mh0t-87lBCXn"
   },
   "source": [
    "### **6.1 Number of Clusters**\n",
    "\n",
    "GMM being a generative model will provide us with a natural technique of finding the optimal number of clusters. Since a generative model provides probability distribution for a given dataset, we can use analytic criterion techniques like *Akaike Information Criterion (AIC)* or *Bayesian Information Criterion (BIC)* to reduce the chances of our model overfitting. We can also use cross-validation to achieve the same.\n",
    "\n",
    "In the example below, we use AIC and BIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "deletable": false,
    "id": "BHh1xwtbAQ07",
    "outputId": "0fc4470a-b90a-4ed0-a8f7-b5b404e6b7b3"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "Xmoon, ymoon = make_moons(200, noise=0.05, random_state=0)\n",
    "plt.scatter(Xmoon[:, 0], Xmoon[:, 1])\n",
    "\n",
    "n_components = np.arange(1, 21)\n",
    "models = [\n",
    "    mixture.GaussianMixture(n, covariance_type=\"full\", random_state=0).fit(Xmoon)\n",
    "    for n in n_components\n",
    "]\n",
    "\n",
    "plt.plot(n_components, [m.bic(Xmoon) for m in models], label=\"BIC\")\n",
    "plt.plot(n_components, [m.aic(Xmoon) for m in models], label=\"AIC\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"n_components\")\n",
    "plt.suptitle(\n",
    "    \"Fig. 13: Optimal Clusters\", fontweight=\"bold\", horizontalalignment=\"right\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "kHG9OUoRUu-Z"
   },
   "source": [
    "## **7. DBSCAN**\n",
    "\n",
    "DBSCAN is short for Density-Based Spatial Clustering of Applications with Noise. It works by identifying regions that are densely populated from sparsely populated regions. The logic follows that points belonging to a cluster should be close to the points in the same cluster.\n",
    "\n",
    "It is defined by two parameters:\n",
    "1. **Epsilon** that finds the radius, which has enough points within.\n",
    "2. **Minimum Samples** that finds the minimum number of points we want to be in a cluster.\n",
    "\n",
    "Below is an implementation of the DBSCAN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "17kvrBZOVsB7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "Skir3PXxVumV"
   },
   "outputs": [],
   "source": [
    "def createDataPoints(centroidLocation, numSamples, clusterDeviation):\n",
    "    # Create random data and store in feature matrix X and response vector y.\n",
    "    X, y = make_blobs(\n",
    "        n_samples=numSamples, centers=centroidLocation, cluster_std=clusterDeviation\n",
    "    )\n",
    "\n",
    "    # Standardize features by removing the mean and scaling to unit variance\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "YHWdNvonVup0"
   },
   "outputs": [],
   "source": [
    "X, y = createDataPoints([[4, 3], [2, -1], [-1, 4]], 1500, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "k8K5OBEDV5VF",
    "outputId": "8dfff3c5-3046-4abc-b7cd-db25714e460c"
   },
   "outputs": [],
   "source": [
    "epsilon = 0.3\n",
    "minimumSamples = 7\n",
    "db = DBSCAN(eps=epsilon, min_samples=minimumSamples).fit(X)\n",
    "labels = db.labels_\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "bNdom_n6V5Xe",
    "outputId": "85c7a738-b944-4629-fe39-28968d8a46dc"
   },
   "outputs": [],
   "source": [
    "# First, create an array of booleans using the labels from db.\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "core_samples_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "F8ZiPMVHV5a0",
    "outputId": "cf5349d7-218e-4d11-93c8-ede86022d9e1"
   },
   "outputs": [],
   "source": [
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_clusters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "BaLsQ1aQWPcO",
    "outputId": "6b72b5e0-0bde-4480-c04d-37c8760aff89"
   },
   "outputs": [],
   "source": [
    "# Remove repetition in labels by turning it into a set.\n",
    "unique_labels = set(labels)\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "5BpMXQZaWVu8",
    "outputId": "4b6b709d-ba07-473b-e249-7d132ead7494"
   },
   "outputs": [],
   "source": [
    "# Create colors for the clusters.\n",
    "colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "deletable": false,
    "id": "VOzc7zVaWPfn",
    "outputId": "c4e878dd-8610-40f8-e933-67a0bdb3564f"
   },
   "outputs": [],
   "source": [
    "# Plot the points with colors\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = \"k\"\n",
    "\n",
    "    class_member_mask = labels == k\n",
    "\n",
    "    # Plot the datapoints that are clustered\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.scatter(xy[:, 0], xy[:, 1], s=50, c=col, marker=\"o\", alpha=0.5)\n",
    "\n",
    "    # Plot the outliers\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.scatter(xy[:, 0], xy[:, 1], s=50, c=col, marker=\"o\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3ybKRBzRuBvj"
   },
   "source": [
    "## **8. Conclusion**\n",
    "\n",
    "We have introduced the concept of clustering and have given an overview of the clustering techniques applied to a dataset. We also illustrated a simple application of K-means clustering to datasets. In the next lesson, we are going to apply hierarchical clustering in a real world dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7-X-AarjuF9Y"
   },
   "source": [
    "**References**\n",
    "\n",
    "1. Chang, Chih-Tang, Jim ZC Lai, and Mu-Der Jeng. \"A fuzzy k-means clustering algorithm using cluster center displacement.\" J. Inf. Sci. Eng. 27.3 (2011): 995-1009.\n",
    "2. Cohn, Ryan, and Elizabeth Holm. \"Unsupervised Machine Learning via Transfer Learning and k-means Clustering to Classify Materials Image Data.\" *Integrating Materials and Manufacturing Innovation*, vol. 10, no. 2, 2021, pp. 231-244.\n",
    "3. Davis, Damek, Mateo Diaz, and Kaizheng Wang. \"Clustering a mixture of gaussians with unknown covariance.\" arXiv preprint arXiv:2110.01602 (2021).\n",
    "4. Ghahramani, Zoubin. \"Unsupervised Learning.\" Advanced Lectures on Machine Learning, edited by Oliver Bousquet, Ulrike von Luxburg, and Gunnar Rätsch, Springer, 2003, pp. 72-112.\n",
    "5. Goldberger, Jacob, and Sam Roweis. \"Hierarchical Clustering of a Mixture Model.\" *Advances in Neural Information Processing Systems*, vol. 17, 2004.\n",
    "6. Murtagh, Fionn, and Pedro Contreras. \"Algorithms for Hierarchical Clustering: An Overview.\" *Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery*, vol. 2, no. 1, 2012, pp. 86-97.\n",
    "7. Nielsen, Frank. *Introduction to HPC with MPI for Data Science*. Springer, 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "Copyright 2024 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
