{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ojFr1lDqtO-A"
   },
   "source": [
    "\n",
    "## MACHINE LEARNING IN FINANCE\n",
    "MODULE 5 | LESSON 1\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "zSLS-GqxtWB5"
   },
   "source": [
    "# **SUPPORT VECTOR MACHINES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "it2RrrWEtdky"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  20 minutes |\n",
    "|**Prior Knowledge** | Machine Learning classification, linear and non-linear data, bias and variance  |\n",
    "|**Keywords** |Support Vector, Margin, Kernel  |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7XxvGPFv-V6z"
   },
   "source": [
    "*In the previous module, we covered two types of classification methods, namely the Naïve Bayes and Decision Trees approach. In this lesson, we will cover another popular method called Support Vector Machines.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "cJ6LlEZu-GD7"
   },
   "source": [
    "## **1. Introduction**\n",
    "\n",
    "Support vector machines is an approach to solving classification problems and has grown in popularity since its introduction in the 1990s. Like logistic regression, it is effective for linearly separable data, or data that can be separated by a linear boundary. However, there are hyperparameters that can be used to adjust the boundary to deal with non-linear data. This boundary is what we call the separating hyperplane. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "xBA--Loz-0_e"
   },
   "source": [
    "## **2. Separating Hyperplane**\n",
    "\n",
    "A hyperplane is simply a flat linear plane for any $p$-dimensional space. An example would be the straight line in a 2-dimensional space as can be seen in fig. 1, i.e., for $p=2$ dimensions, the hyperplane is a flat $p-1=1$ dimensional subspace. In figure 1 below, the blue line is a hyperplane in a two-dimensional space.\n",
    "\n",
    "**Figure 1:** A Linear Boundary Separating a Binary Target Class Data\n",
    "\n",
    "![](../../images/CustomLinSepData.png)\n",
    "\n",
    "\n",
    "Mathematically, in this example, the hyperplane can be represented by the equation of a straight line,\n",
    "$\n",
    "\\begin{equation} \n",
    "\\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2}= 0,\n",
    "\\tag{1} \n",
    "\\end{equation}\n",
    "$\n",
    "hence, in general for $p$-dimensions,\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\cdots +\\beta_{p}X_{p}= 0, \n",
    "\\tag{2}\n",
    "\\end{equation}\n",
    "$$\n",
    "or,\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\\sum_{j=1}^{p}\\beta_{j}X_{j} + \\beta_{0} = 0. \n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Suppose the equation of the separating hyperplane represented in fig. 1 by the blue line is given by,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "-4 + 4X_{1} + X_{2}= 0. \n",
    "\\tag{4}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Now if we had an observation with $(X_{1}, X_{2})$ coordinates of $(6, -10)$, then substituting this point into (4) would result in,\n",
    "$$\n",
    "\\begin{equation} \n",
    "-4 + 4(6) + (-10)= 10 > 0. \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This observation would be above the hyperplane, along with class 1 (red triangles). The rule for classifying target classes for any observation of $(X_{1}, X_{2})$ would be,\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "-4 + 4X_{1} + X_{2} > 0 \\text{ Then Class 1}, \n",
    "\\end{equation}\n",
    "$$\n",
    "or\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "-4 + 4X_{1} + X_{2} < 0 \\text{ Then Class 0}. \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The question you may ask is what do we do with an observation that is on the boundary, i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "-4 + 4X_{1} + X_{2} = 0 \\text{ ?} \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "To address this, we will need to look at how this hyperplane is derived, and to do this, we look at the Maximal Margin Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "nHRkoYCHNXrA",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **3. Maximal Margin Classifier**\n",
    "\n",
    "\n",
    "You will want to refer to fig. 2 to aid in your understanding of this section. In the example shown in fig. 2, the optimal classifier for two predictors is shown by the blue line. This optimal hyperplane is constructed such that there are equidistant \"buffers\" on either side shown by the dashed lines. The perpendicular distance between the dashed lines is known as the margin and the optimal hyperplane is achieved when this margin is maximized. Another way of thinking about this is the maximal margin classifier maximizes the distance to the nearest data point. You will notice that the dashed lines touch the data on either side, but it is only necessary to touch the data points on one side before the margin is achieved. These buffers shown by the dashed lines are known as support vectors.\n",
    "\n",
    "**Figure 2:** Optimal Boundary Separating Two Classes\n",
    "\n",
    "![](../../images/svm_annotate_scrs.png)\n",
    "\n",
    "\n",
    "For this binary classification problem for predictors $X_{1}$ and $X_{2}$, the optimized boundary is obtained by maximizing the margin,$M$, subject to:\n",
    "\n",
    "$$ \n",
    "\\begin{equation} \n",
    "\\sum_{j=1}^{2}\\beta_{j}^{2} = 1.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "For a higher dimension of predictors say $p$, this is extended to the constraint,\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "\\sum_{j=1}^{p}\\beta_{j}^{2} = 1.\n",
    "\\tag{4}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The constraint in equation (4) is to ensure we look at the perpendicular distance from the $j$th data point to the hyperplane. \n",
    "\n",
    "Instead of class 0 and 1 for our binary problem, let us suppose the class labels are -1 and 1 instead, i.e., $y_i$ is -1 or 1 for the ith observation. From (3), a correct classification would be\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y_{i}(\\sum_{j=1}^{p}\\beta_{j}X_{ij} + \\beta_{0}) > 0,\n",
    "\\tag{5} \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "and hence misclassifications as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y_{i}(\\sum_{j=1}^{p}\\beta_{j}X_{ij} + \\beta_{0}) \\leq 0.\n",
    "\\tag{6} \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "A method, called the Perceptron algorithm, determines the weights, $\\beta_{j}$, by looking at the misclassifications in (6) and updating the weights in an iterative way. The details of this algorithm will not be covered in this course.\n",
    "\n",
    "For an extra safeguard, think of the perpendicular distance of the hyperplane to the support vector as $D$. For this extra safeguard, we can change (5) to be\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "y_{i}(\\sum_{j=1}^{p}\\beta_{j}X_{ij} + \\beta_{0}) > D. \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "One thing to note is that SVMs do not output probabilities of the target class; however, some methods do exist, such as Platt scaling and isotonic regression, to obtain this. Thus far, we have only considered linearly separable data, but often in practice, this may not be the case. The perceptron algorithm does not perform well for non-linearly separable data. This brings us to Hard and Soft margins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Ww8G0BoIONm1"
   },
   "source": [
    "## **4. Types of Margins**\n",
    "\n",
    "Hard margins are when no misclassifications occur or are allowed. Soft margins are when the number of misclassifications can be specified. A hyperparameter called the slack variable, $c$, specifies the number of misclassifications allowed and the higher $c$, the greater the penalty. A soft margin approaches a hard margin when $c$ approaches zero, i.e., no misclassifications. The problem with using a hyperplane that perfectly separates classes in the training dataset is that it may not achieve the same performance for unseen or test data. This is the problem of over-fitting. When $c$ is close to zero, we are actually taking a very conservative approach by using a very small margin, i.e., the dashed lines in fig. 2 are much closer to the hyperplane (solid blue line). A higher $c$ pushes these margins further away since we are allowing the classifier to accommodate more misclassifications. In other words, a small $c$ enables low bias but higher variance. New data introduced into the training set can change the hyperplane; however, it is important to note that only points on the margin and on the wrong side of the hyperplane would affect a change. This quality emphasizes the insensitivity to outliers. The gamma hyperparameter can be adjusted such that nearby or far away points to the decision boundary have an influence. Low gamma takes far away points and vice versa. We finally get to the problem of non-linear data and the approach of SVM to overcome this via kernels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## **5. Kernels**\n",
    "\n",
    "Let's watch the video below to get an idea about what a Kernel does. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "\n",
    "VimeoVideo(\"781662374\", h=\"d05a258e6d\", width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "##### [Access Video Transcript here](https://drive.google.com/file/d/1HyKh9glthHdjHG-qkMHVo2JYGfxjCS3L/view?usp=share_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "kjlKqNe0OWC3"
   },
   "source": [
    "\n",
    "Suppose the target classes are distributed in the feature space between two predictors as shown in the left plot of fig. 3. \n",
    "\n",
    "**Figure 3:** Left: Non-linearly separable data. Right: Data projected into an extra dimension $Z$ such that $Z = X1^2 + X2^2$.\n",
    "\n",
    "\n",
    "![](../../images/KernelTransformData.png)\n",
    "\n",
    "\n",
    "Using the hard margin approach, a straight line would not split the classes appropriately. An approach may be to add an extra predictor that is a function of the original such that\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "Z = X1^2 + X2^2.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Projecting the feature space into an extra dimension would show the distribution of the target classes in the right plot of fig. 3. We now see that a plane can separate the target classes efficiently. This mapping into higher dimensions is done by kernels. SVM algorithms have a number of kernels to choose from, such as Polynomial Kernel and Gaussian Radial Basis Function (RBF) to name a few. For a more comprehensive list, please refer to https://data-flair.training/blogs/svm-kernel-functions/. One thing to note is that this illustrative example was merely two predictors originally; therefore, with more predictors the optimal transformation is not only difficult find but also to compute. Kernels achieve this in a more memory efficient way. The detailed mathematics behind kernels is not part of the scope of this module, but the interested reader is referred to https://www.analyticsvidhya.com/blog/2020/10/the-mathematics-behind-svm/. What we need to understand is that the solution to calculating the optimal hyperplane depends on the dot or inner product of the observations. The inner product for two vectors $a = (a_{1}, a_{2},⋯,a_{n})$ and $b = (b_{1}, b{2},⋯,b_{n})$ of equal length $n$ is\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\left<a,b\\right> = \\sum_{i=1}^{n}a_i b_i.\n",
    "\\tag{7}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "From (7), you may notice that this is 'somewhat' how we obtained the calculation of $Z$ for the example above, i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z = \\sum_{j=1}^{2} \\left<Xj_{i},Xj_{i'}\\right>\n",
    "= K(X_{i},X_{i'})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "for two different observations $i$ and $i'$. We use $K$ to denote the kernel.\n",
    "\n",
    "In general, a linear support vector can be represented as \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y(x) = \\beta_0 + \\sum_{i}^{n}\\alpha_{i}\\left<X, X_{i} \\right>\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "for some new point $X$, where the parameters $\\alpha_{i}$ are estimates over each observation $i$. This implies that inner products between the new point $X$ and all other observations need to be calculated to estimate $\\alpha$; however, $\\alpha$ is zero for all observations that are not support vectors. We therefore see that determining the $\\alpha$s is made less computationally intensive.\n",
    "\n",
    "The Gaussian RBF kernel has the below formula instead,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "K(X_{i},X_{i'}) = \\text{exp} \\left(-\\gamma ||X_{i}-X_{i'}||^2\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $||X_{i}-X_{i'}||$ is the euclidean distance between observation $X_i$ and $X_{i'}$. This may look familiar to you if you've seen the formula for a Gaussian equation $\\text{exp} \\left(-\\gamma ||X-\\mu||^2\\right)$ for mean $\\mu$. The parameter $\\gamma$ is synonymous with the width of the Gaussian distribution or standard deviation, $\\sigma$, i.e., $\\gamma = 1/2\\sigma^2$. Hence, this is a hyper parameter provided by the user and a too large $\\gamma$ can lead to over-fitting. The Gaussian Kernel is often used when the original data is non-linearly separable and is sometimes the default kernel in programming languages. It is best to know the default kernel used in the SVM model and experiment with different kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "6xh4HP91uhsj"
   },
   "source": [
    "## **6. Conclusion**\n",
    "In this lesson, we covered the mathematics and methodology behind support vector machines. We looked at the hyperplane and support vectors that form the decision boundaries for separating a binary target class. Methods to possibly overcome non-linearly separable data such as kernels were examined. In the next lesson, we will put SVM into practice with a problem in the finance field.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "F7vmlGS9unb8"
   },
   "source": [
    "**References**\n",
    "\n",
    "DataFlair. \"Kernel Functions-Introduction to SVM Kernel and Example.\" https://data-flair.training/blogs/svm-kernel-functions/\n",
    "\n",
    "James, Gareth, et al. *An Introduction to Statistical Learning: With Applications in R*. 2nd ed., Springer, 2021.\n",
    "\n",
    "Radhika. \"The Mathematics Behind Support Vector Machine Algorithm (SVM).\" *Analytic Vidhya*, 23 Oct. 2020. https://www.analyticsvidhya.com/blog/2020/10/the-mathematics-behind-svm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "Copyright © 2023 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n",
    "<span style='color: transparent; font-size:1%'>All rights reserved WQU WorldQuant University QQQQ</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6c241f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "Copyright 2023 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
