{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ojFr1lDqtO-A"
   },
   "source": [
    "\n",
    "## MACHINE LEARNING IN FINANCE\n",
    "MODULE 5 | LESSON 3\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "zSLS-GqxtWB5"
   },
   "source": [
    "# **NEURAL NETWORKS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "it2RrrWEtdky"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  20 minutes |\n",
    "|**Prior Knowledge** | Linear and Non-linear data, gradient  |\n",
    "|**Keywords** |Activation function, Loss function, Neuron, Layers  |\n",
    "\n",
    "\n",
    "---<span style='color: transparent; font-size:1%'>All rights reserved WQU WorldQuant University QQQQ</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "TPzpLVqBuIXC"
   },
   "source": [
    "*In the previous lesson, we covered support vector machines, which can handle both linear and non-linear data provided the appropriate kernel is used. In this lesson, we will cover a neural network predictor that is effective for both linear and non-linear data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "B77f1sBImdwv"
   },
   "source": [
    "## **1. Neural Network Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "yndfoTFAFtbS"
   },
   "source": [
    "Neural networks (NNs) are often described as a way to replicate the human brain with a computer. When we think about the process of identifying an image, such as an image of a dog, we first view the image with our eyes thereby sending inputs to the brain. The next step would be the brain analyzing the image based on what we've seen about dogs in the past. Finally, the brain has made a decision/conclusion that this is an image of a dog. To summarize this process, we could have:\n",
    "\n",
    "Input -> analyze -> decision.\n",
    "\n",
    "This task is quite simple for us; however, to get a computer to perform the same image recognition task for a dog is not trivial. The process is similar in that the computer undertakes a similar process:\n",
    "\n",
    "Input -> process/calculations -> output.\n",
    "\n",
    "Let's look at an example to understand the process of a neural network to discover the relationship between inputs and outputs. We'll consider the simple problem of converting mass from pounds (lbs) to kilograms (kg). Suppose we didn't know the formula for the conversion, and all we had to go on was the observation:\n",
    "\n",
    "|  |  |\n",
    "|:---|:---|\n",
    "|**kg** |  **lbs** |\n",
    "|50 | 110.23  |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "We make a critical assumption here: that the relationship between kg and lbs is linear. I know it's a good guess since we know the formula is $lbs = 2.2046 \\times kg$, but often, we don't know the relationship. So we've made our guess that the relationship is lbs = $c\\times$kg, where $c$ is the slope. This linear equation is what is called the *activation function* in neural networks. We begin by making a guess about $c=1.5$, and with this guess, our lbs estimate is lbs = 1.5(50) = 75 lbs. we therefore seem to have underestimated by 110.23 - 75 = 35.23 lbs. Okay, so we guess again, going higher for $c$ based on the feedback from our first guess, and go with $c=3$, resulting in our second estimate of 50(3) = 150 lbs. It seems we overestimated this time by 39.77 lbs. We therefore need to go with a value of $1.5 < c < 3 $. We do this iteratively until we get to a value of $c=2.2$, which results in 110 lbs. Pretty close. We can decide this is close enough, therefore settling on $c=2.2$. We can summarize the iterations below,  \n",
    "\n",
    "Table 1: Summary of Iterations to Find Optimal Slope $c$.\n",
    "\n",
    "|  |  |  |  |\n",
    "|:---|:---|:---|:---|\n",
    "|**iteration** |  $c$ |  lbs estimate|  (Actual-predicted) |\n",
    "|1 | 1.5  | 75  | 35.23  |\n",
    "|2 | 3.0  | 150  | -39.77  |\n",
    "|3 | 2.5  | 125  | -14.77  |\n",
    "|4 | 2.2  | 110  | 0.23  |\n",
    "---\n",
    "\n",
    "The last column provided insight into how far away our guess was from the actual. This is an example of a *loss function* in a NN. The magnitude of the difference between the actual and estimate signals how we need to adjust our next guess for $c$. Think of the difference being large resulting in our next guess of $c$ being largely different from the previous one. This type of feedback is what we call the *optimizer* in a NN. With this example, we more or less get the workings or flow of a NN. This can be summarized in figure 1 below. You'll notice a 'weight' component in the diagram, which we will get to in the next section. To do this, we will look at the architecture of the NN.\n",
    "\n",
    "Figure 1: Flow Diagram of Neural Network Operation\n",
    "\n",
    "![](../../images/NN_flowDiagPic.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "IeDTCY0GF5PT"
   },
   "source": [
    "## **2. Network Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gLHGzXPYir3q"
   },
   "source": [
    "\n",
    "\n",
    "A neural network's architecture is shown in figure 2. The hidden layer is where the processing takes place, and do keep in mind that we can have more than one hidden layer, but for now, we keep it simple.\n",
    "\n",
    "Figure 2: Architecture of Basic Neural Network\n",
    "\n",
    "![](../../images/499px-colored_neural_network.webp)\n",
    "\n",
    "##### Source:opensource.com, *How to use an Arduino and Raspberry Pi to Turn a Fiber Optic Neural Network into Wall Art*, June 2022, https://opensource.com/article/17/10/fiber-optic-neural-network-art)\n",
    "\n",
    "If you look at the connections (edges) between the input layer and hidden layer, you will notice all red circles (input nodes) are connected to all the blue circles (Hidden layer nodes). We call this type of connection a *dense* layer. Think of each input node as a feature feeding information into the NN, for example in a credit risk problem, one input node could be Age with another being Salary. Hence, the number of nodes in the input layer is the number of features or predictors. The number of nodes in the output layer will depend on the target variable. If the target were a binary class variable, then one node would be sufficient in the output layer since it returns the probability of being in the target class, e.g., probability of defaulting on a loan. For a regression-type problem, like the lbs to kg conversion discussed earlier, the output node would return the continuous value of the target variable, i.e., lbs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "WHlW1Ce6yllW"
   },
   "source": [
    "## **3. Forward and Backward Propagation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "nysGubidyvTH"
   },
   "source": [
    "NNs achieve predictions via a process called propagation. At each forward step, there are matrix multiplications taking place. Computers are relatively quick at doing these calculations, and it is worth understanding the process behind it. To begin, we must first identify the dimension of inputs at the start, i.e., at the input layer. We mentioned earlier that the number of nodes at the input layer corresponds to the number of predictors or features. This is the number of columns of the input matrix, and the number of rows corresponds to the number of observations, $N$ say. We thus have a $N\\times3$ matrix of inputs for figure 2. In general, let's denote the number of features as $p$ so figure 2 would have $p=3$. We shall denote the layers as follows:\n",
    "\n",
    "\n",
    "1.   Input layer: $X_0$\n",
    "2.   Hidden layer: $X_1$\n",
    "3.   Output layer: $Y$\n",
    "\n",
    "We'll touch on some linear algebra here but nothing too intense other than matrix multiplication. In the forward propagation step moving from $X_0$ to $X_1$, we have the following calculations taking place, which can be represented as matrix multiplication, \n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "X_{1} = f(X_{0} w_{0}+b_{0}),\n",
    "\\tag{1} \n",
    "\\end{equation}\n",
    "$$\n",
    "where $w_0$ is a matrix of $p\\times4$ *weights* since the hidden layer has 4 nodes, and $f$ is a user specified activation function. The term $b$ is a bias term that enables the activation function to shift, which allows more flexibility of the algorithm to fit the data. The activation function, in addition to specifying the linearity/non-linearity of data, also decides on which node to activate by controlling the value of the weights. For instance, suppose Age was not a strong predictor of defaulting on a loan, then the weight for that node would be small. To sequentially move to the output layer, we then have the following calculation,\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "Y = f(X_{1} w_{1}+b_{1})=f(f(X_{0} w_0 +b_{0}) w_1+b_{1}),\n",
    "\\tag{2} \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "by substituting (1) for $X_1$. The dimension of $w_1$ will be 4x2 since we have 4 nodes in the hidden layer and 2 nodes in the output layer. This sequential process of matrix multiplication results in the following dimension changes:\n",
    "\n",
    "\n",
    "*   $X_{0}\\times w_0$: $(N\\times p) \\times (p\\times 4) = N\\times 4$ (Think of $p$ canceling each other out)\n",
    "*   $X_{1}\\times w_1$: $(N\\times 4) \\times (4\\times 2) = N\\times 2$\n",
    "\n",
    "The dimensions of the weights and layers is necessary when specifying the architecture of the NN in a programming language, such as Python. Let's look at two videos that will help illustrate the inputs and outputs at the hidden nodes. Inputs of the hidden nodes can be shown in the video below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "\n",
    "VimeoVideo(\"782624672\", h=\"60b0362998\", width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "##### [Access Video Transcript here](https://drive.google.com/file/d/1YJEyoOkAvqG4MemvchShFvAr6XW5B76j/view?usp=share_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Outputs of hidden nodes can be show in this video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "VimeoVideo(\"782999865\", h=\"2294f6c7e5\", width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "##### [Access Video Transcript here](https://drive.google.com/file/d/1gvVkhaDNuNT68LwlFDqwv0XVWpOD3fN9/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Thus far, we have shown the forward propagation process to achieve outputs; however, it is the backward propagation process that enables the NN to learn and improve. This is the feedback process from the loss function to the optimizer, which eventually tells the algorithm how the weights should be adjusted. Ultimately, it is the weights, and some bias term, that will be adjusted to achieve optimal performance. A great benefit of NNs is that we do not have to restrict ourselves to just one hidden layer, and by increasing the layers, we increase our chances of solving non-linear data relationships. There is a problem that NNs face with this increase in layers, which is called the *vanishing gradient problem*.\n",
    "\n",
    "Refer to the video below on how the bias is optimized using derivatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "VimeoVideo(\"785293805\", h=\"a725197abc\", width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "##### [Access Video Transcript here](https://drive.google.com/file/d/16dGT4UBi12L6yan5psSk508WlCvycpdR/view?usp=share_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "TXIPhyHnVSaJ"
   },
   "source": [
    "## **4. Vanishing Gradient**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5D3j6S0lVYHu"
   },
   "source": [
    "We've briefly mentioned the role of the activation function in section 1. There are various activation functions to choose from with each having their benefits. The reader is directed to (Nwankpa 7) for a brief list of these functions. The sigmoid activation function is a popular function used in binary classification problems. It squashes the output into a small range between 0 and 1. The functional form may look familiar to you if you've worked with a logistic regression model as it is calculated as follows,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "σ(x) = \\frac{1}{1+e^{-x}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "What this tells us is that for a large change in $x$ or input, we have a small change in output. But we know that the gradient is given by,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\text{Gradient} = \\frac{\\text{change in output}}{\\text{change in input}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "hence this implies the gradient tends to a small value for the sigmoid function. The connection of the sigmoid function to the loss function, $L$ say, is that we know our output depends on the activation function. The more layers we have in our network, the more activation functions are required in the process. The idea of the backpropagation from table 1 was to minimize $L$ or the difference in the last column through feedback. Calculus is very handy when we want to minimize some function $f$ with respect to its input $x$. The process entails determining the value of $x$, say $x'$, that makes the derivative or gradient of $f$ = 0. Backpropagation thus entails finding the derivative of the initial layer by multiplying the derivatives of each layer moving from the final to the initial layer, a process known as the chain rule. Thus, the problem arises where this product tends to zero with many layers, especially with the sigmoid activation function. This prevents optimal updating of the weights and bias. \n",
    "\n",
    "Improvements to NNs have been made since 2009 to overcome this problem such as better activation functions like ReLu, better weight initialization schemes, and improved optimization schemes. NNs have come a long way since their inception, and with GPUs available, the speed of training these algorithms has been greatly improved.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "6xh4HP91uhsj"
   },
   "source": [
    "## **5. Conclusion**\n",
    "\n",
    "In this lesson, we briefly explained the process of a NN. We mentioned its components such as inputs, hidden and output layers consisting of nodes, the forward and backward propagation process, and the role of the activation, loss, and optimizer in the operation of the algorithm. Neural networks also face problems such as vanishing gradients and overfitting if not tuned correctly. We'll look at an example of this in the next lesson where we train a neural network on a financial problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "F7vmlGS9unb8"
   },
   "source": [
    "**References**\n",
    "\n",
    "Nwankpa, Chigozie. \"Activation Functions: Comparison of Trends in Practice and Research for Deep Learning.\" *arXiv Preprint arXiv:*, 1811.03378, 2018, pp. 5–8, arxiv.org/pdf/1811.03378.pdf.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "Copyright 2023 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
