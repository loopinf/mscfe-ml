{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojFr1lDqtO-A"
   },
   "source": [
    "\n",
    "## MACHINE LEARNING IN FINANCE\n",
    "MODULE 5 | LESSON 2\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSLS-GqxtWB5"
   },
   "source": [
    "# **SUPPORT VECTOR MACHINES IN PRACTICE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "it2RrrWEtdky"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  20 minutes |\n",
    "|**Prior Knowledge** | Classification, Support Vector Machines, Decision Trees.  |\n",
    "|**Keywords** |Hyperparameters, slack variable, returns, Receiver Operating Curve (ROC).  |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPzpLVqBuIXC"
   },
   "source": [
    "*In the previous lesson, we covered the methodology of Support Vector Machines. In this lesson, we apply it to a predictive analysis problem.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Q5HOgn2CAym"
   },
   "source": [
    "## **1. Introduction**\n",
    "\n",
    "In this lesson, we will implement SVM on a trading strategy for the Luxembourg index (LUXXX) based on other country indices as well as technical indicators. The strategy is to take a long/short position when a 0.25% change in LUXXX return is predicted. The long/short position will be based on an upward or downward trending market. Keep in mind that this strategy does not take into account trading costs, but the lesson is designed to show the practicality of SVM in predictive analytics for financial problems.\n",
    "\n",
    "We will begin by importing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlPRAL-LROhc"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeTW8wxwC-n9"
   },
   "source": [
    "The data containing the weekly closing price of indices can be found in the following csv file along with the dates. We load this data and thereafter view it using the head() command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhXI5QaEvMaQ"
   },
   "source": [
    "Should you store the csv file in the same location as the notebook, then use the uncommented cells below. Use the commented cells instead if you choose to store the csv in a different location. The \"loc\" variable is the path to the csv you need to specify. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "fZiDR8rSqqmH",
    "outputId": "12a3d1cd-4445-4624-9e74-c20116876204"
   },
   "outputs": [],
   "source": [
    "# loc = \"ENTER YOUR FULL PATH TO LOCATION OF DATA FILE HERE\"\n",
    "# data_df = pd.read_csv(loc+\"/MScFE 650 MLF GWP Data.csv\")\n",
    "data_df = pd.read_csv(\"../../data/MScFE 650 MLF GWP Data.csv\")\n",
    "data_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8APq2-5uDKQ4"
   },
   "source": [
    "The 'Date' column is in a string format; therefore, we convert it to a datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yB1UV-VWRXZD"
   },
   "outputs": [],
   "source": [
    "# Convert string to datetime\n",
    "data_df[\"Date\"] = pd.to_datetime(data_df[\"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfE8MrnfDV5b"
   },
   "source": [
    "Below is where we specify the target column. In our case, it is the LUXXX index. The returns of LUXXX need to be calculated since our strategy is based on the returns over the weeks. The remaining indices are converted to returns instead of their prices. This accomplishes two things, namely it scales the features into a common unit as well as captures movement in the prices instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3SWLVppRk0J"
   },
   "outputs": [],
   "source": [
    "# Set Target Index for predicting\n",
    "target_ETF = \"LUXXX\"\n",
    "\n",
    "# Use returns instead of prices for other Indices\n",
    "# Other Indices used as Index_features\n",
    "ETF_features = data_df.loc[:, ~data_df.columns.isin([\"Date\", target_ETF])].columns\n",
    "data_df[ETF_features] = data_df[ETF_features].pct_change()\n",
    "\n",
    "data_df[target_ETF + \"_returns\"] = data_df[target_ETF].pct_change()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TOS7cH8EAhA"
   },
   "source": [
    "The cell below calculates the Target column and converts it into a categorical binary variable. We denote 1 for the absolute returns of LUXXX exceeding 0.25% and 0 otherwise. In a downward or bear market, we look to short should the price decrease by more than this threshold and we take a long position in a bear market. The goal here is to predict a percent change more than a threshold. Note that we shift the target column by one week to align the predicted period with the period available for other predictors. The ML algorithm requires the label and feature/predictor values to be captured in the same row or observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NqPtNyZR2DX"
   },
   "outputs": [],
   "source": [
    "# Create Target Column.\n",
    "# Shift period for target column\n",
    "data_df[target_ETF + \"_returns\" + \"_Shift\"] = data_df[target_ETF + \"_returns\"].shift(-1)\n",
    "\n",
    "# Strategy to take long position for anticipated returns of 0.5%\n",
    "data_df[\"Target\"] = np.where(\n",
    "    (data_df[target_ETF + \"_returns_Shift\"].abs() > 0.025), 1, 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncXWFDdUEuPs"
   },
   "source": [
    "It is worth looking at any imbalance in the dataset. The proportion of 34.5% of the target class 1 does not indicate any imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3lXgOjgeSB7f",
    "outputId": "e2f4a1df-b589-4c74-8316-a504b35c28fa"
   },
   "outputs": [],
   "source": [
    "# Checking target proportion\n",
    "round(data_df[\"Target\"].sum() / len(data_df), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUcoK41FE9oH"
   },
   "source": [
    "Now that we have our Target class, we can select which indices contribute to predicting the target class. We use the feature importance tool in `Sklearn` for a Decision Tree to do this. We look at the cumulative importance for the features that make a contribution > 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9x6icInXSGQ_"
   },
   "outputs": [],
   "source": [
    "# Preparing for Train/Test split\n",
    "\n",
    "NoNaN_df = data_df.dropna()\n",
    "X = NoNaN_df[ETF_features]\n",
    "\n",
    "X = X.iloc[:, :]  # .values\n",
    "y = NoNaN_df.loc[:, \"Target\"]  # .values\n",
    "\n",
    "del NoNaN_df\n",
    "\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# USE DECISION TREE FOR FEATURE IMPORTANCE\n",
    "# Fitting Decision Tree Classification to the Training set\n",
    "\n",
    "\n",
    "DTree = DecisionTreeClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    random_state=0,\n",
    "    max_depth=8,\n",
    "    min_samples_leaf=30,\n",
    "    min_samples_split=20\n",
    "    #                               , class_weight=classweight\n",
    ")\n",
    "DTree.fit(X_train, y_train)\n",
    "\n",
    "feature_importances = pd.DataFrame(\n",
    "    DTree.feature_importances_,\n",
    "    index=X.iloc[:, :].columns,  # don't want the last Target column\n",
    "    columns=[\"importance\"],\n",
    ").sort_values(\"importance\", ascending=False)\n",
    "\n",
    "feature_importances[\"Cumul_Imp\"] = feature_importances.cumsum().iloc[:, :]\n",
    "# If you want to see only those that explain a % of variation\n",
    "pct_var = 1.0\n",
    "feature_importances = feature_importances[(feature_importances[\"Cumul_Imp\"] <= pct_var)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpawYn8IFhZv"
   },
   "source": [
    "The cumulative importance shows only 4 indices that contribute, namely MSCI KOREA, DENMARK, FRANCE, and NORWAY. This reduces the features significantly and will also reduce the run time when doing hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "AKexbx_8SXbS",
    "outputId": "b78a0047-718a-4edf-81da-7f5e7814eef2"
   },
   "outputs": [],
   "source": [
    "# PLOT IMPORTANCE\n",
    "\n",
    "# Example data\n",
    "cols = feature_importances.index\n",
    "y_pos = np.arange(len(cols))\n",
    "performance = feature_importances.importance\n",
    "\n",
    "df2plot = pd.DataFrame(data=performance, index=feature_importances.index)\n",
    "df2plot[\"Variable\"] = feature_importances.index\n",
    "df2plot = df2plot[df2plot[\"importance\"] > 0.000]\n",
    "\n",
    "print(df2plot)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "sns.barplot(\n",
    "    x=\"importance\", y=\"Variable\", data=df2plot, label=\"Variable Ranking\", color=\"b\"\n",
    ")\n",
    "\n",
    "ax.set(xlim=(0, 1), ylabel=\"\", xlabel=\"importance\")\n",
    "\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    ax.text(\n",
    "        width + 0.05,\n",
    "        p.get_y() + p.get_height() / 2.0 + 0.2,\n",
    "        \"{:1.2f}\".format(width),\n",
    "        ha=\"center\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2my_qIPf8-l"
   },
   "source": [
    "**Figure 1: Feature Importance Contribution of Four Predictors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "__gzM-V4SbXw",
    "outputId": "23287ca5-b298-4bcf-beec-b106f4db349f"
   },
   "outputs": [],
   "source": [
    "# Indices to add value in prediction\n",
    "ETF_ImpFeatures = []\n",
    "for i in df2plot[\"Variable\"].values:\n",
    "    ETF_ImpFeatures.append(i)\n",
    "\n",
    "ETF_ImpFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rCBkY8DSj5s"
   },
   "source": [
    "## **2. Create Technical Indicators**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the first technical indicators are the moving average crossovers of short- (SMA_5) and long-term (SMA_15). We use the ratio of (SMA_15/SMA_5) to capture the relationship as one feature and any crossovers. We will rely on the SVM model to capture the relationship between this feature and the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0KZHeoDS5t8"
   },
   "outputs": [],
   "source": [
    "data_df[\"SMA_5\"] = data_df[target_ETF].rolling(5).mean()\n",
    "data_df[\"SMA_15\"] = data_df[target_ETF].rolling(15).mean()\n",
    "data_df[\"SMA_ratio\"] = data_df[\"SMA_15\"] / data_df[\"SMA_5\"]\n",
    "\n",
    "# Can drop SMA columns since not needed anymore.\n",
    "data_df.drop([\"SMA_5\", \"SMA_15\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oq63GG_uTG9L"
   },
   "source": [
    "The next technical indicator we look at is the relative strength index (RSI), which is a momentum oscillator. This indicator measures the magnitude of recent price changes. The target price is usually considered overbought when the RSI > 70% and oversold when < 30%. This is based on the price change of LUXXX before the week of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19J2qICGS_lS"
   },
   "outputs": [],
   "source": [
    "# shift the price of the target by 1 unit previous in time\n",
    "data_df[\"Diff\"] = data_df[target_ETF] - data_df[target_ETF].shift(1)\n",
    "data_df[\"Up\"] = data_df[\"Diff\"]\n",
    "data_df.loc[(data_df[\"Up\"] < 0), \"Up\"] = 0\n",
    "\n",
    "data_df[\"Down\"] = data_df[\"Diff\"]\n",
    "data_df.loc[(data_df[\"Down\"] > 0), \"Down\"] = 0\n",
    "data_df[\"Down\"] = abs(data_df[\"Down\"])\n",
    "\n",
    "data_df[\"avg_5up\"] = data_df[\"Up\"].rolling(5).mean()\n",
    "data_df[\"avg_5down\"] = data_df[\"Down\"].rolling(5).mean()\n",
    "\n",
    "data_df[\"avg_15up\"] = data_df[\"Up\"].rolling(15).mean()\n",
    "data_df[\"avg_15down\"] = data_df[\"Down\"].rolling(15).mean()\n",
    "\n",
    "data_df[\"RS_5\"] = data_df[\"avg_5up\"] / data_df[\"avg_5down\"]\n",
    "data_df[\"RS_15\"] = data_df[\"avg_15up\"] / data_df[\"avg_15down\"]\n",
    "\n",
    "data_df[\"RSI_5\"] = 100 - (100 / (1 + data_df[\"RS_5\"]))\n",
    "data_df[\"RSI_15\"] = 100 - (100 / (1 + data_df[\"RS_15\"]))\n",
    "\n",
    "data_df[\"RSI_ratio\"] = data_df[\"RSI_5\"] / data_df[\"RSI_15\"]\n",
    "\n",
    "# Can drop RS Calc columns columns\n",
    "data_df.drop(\n",
    "    [\"Diff\", \"Up\", \"Down\", \"avg_5up\", \"avg_5down\", \"avg_15up\", \"avg_15down\"],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lACjMGrFTPLm"
   },
   "source": [
    "Rate of change is the percentage change in price over a 15-week period. It is a long-term rate of change and is also a momentum indicator. It is also used to spot overbought and oversold instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSo7VEd3TMYy"
   },
   "outputs": [],
   "source": [
    "data_df[\"RC\"] = data_df[target_ETF].pct_change(periods=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuhDdFgKJfWD"
   },
   "source": [
    "We now combine all features, that is the indices' price change as well as the technical indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDrh_DOvTaLY"
   },
   "outputs": [],
   "source": [
    "# all_feats\n",
    "ETF_ImpFeatures.append(\"SMA_ratio\")\n",
    "ETF_ImpFeatures.append(\"RSI_ratio\")\n",
    "ETF_ImpFeatures.append(\"RC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZL2arBy2JpRV"
   },
   "source": [
    "We are now in a position to train our model. To begin, we perform the train/test split of the data into an 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "do-5He7xTeQL"
   },
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "# Train/Test split. No NaNs in the data.\n",
    "NoNaN_df = data_df.dropna()\n",
    "X = NoNaN_df[ETF_ImpFeatures]\n",
    "\n",
    "X = X.iloc[:, :]  # .values\n",
    "y = NoNaN_df.loc[:, \"Target\"]  # .values\n",
    "\n",
    "del NoNaN_df\n",
    "\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_8tOu97J20A"
   },
   "source": [
    "To determine the best kernel and slack variable value, we do a grid search method using `GridSearchCV`. This will explore the specified hyperparameter space for the optimal values over five cross validations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PU0I2iQBTlqy",
    "outputId": "ab431fca-a0b6-4a48-eb40-d4045529584c"
   },
   "outputs": [],
   "source": [
    "# defining parameter range\n",
    "param_grid = {\n",
    "    \"C\": [0.01, 0.1, 1, 10, 100, 1000],\n",
    "    #               'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "    \"kernel\": [\"rbf\", \"linear\", \"poly\"],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(svm.SVC(), param_grid, refit=True, verbose=3, cv=5)\n",
    "\n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6h8VLceKPaE"
   },
   "source": [
    "The best hyperparameters are specified below. We can see that the best kernel to use for this dataset is the polynomial kernel with a slack variable value of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hz25IDU6r8kh",
    "outputId": "ba28135c-0186-4297-e5b0-a19b7f158c7c"
   },
   "outputs": [],
   "source": [
    "# print best parameter after tuning\n",
    "print(grid.best_params_)\n",
    "\n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Csl_8N4WKeYe"
   },
   "source": [
    "Finally, we can train our SVM model for the specified hyperparameters and compare it to an untuned decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtrkDS6csErt",
    "outputId": "6d33bbbe-a86c-4acd-afad-7eb0477673b3"
   },
   "outputs": [],
   "source": [
    "# Train with Tuned SVM\n",
    "# Create a svm Classifier\n",
    "clf_tuned = svm.SVC(\n",
    "    C=grid.best_params_[\"C\"], kernel=grid.best_params_[\"kernel\"], probability=True\n",
    ")\n",
    "\n",
    "clf_tuned.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Fitting Decision Tree Classification to the Training set as well\n",
    "\n",
    "clf_tree = DecisionTreeClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    random_state=0,\n",
    "    max_depth=8,\n",
    "    min_samples_leaf=30,  # DM\n",
    "    min_samples_split=20\n",
    "    #                               , class_weight=classweight\n",
    ")  # DM\n",
    "clf_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tSCeesoKtNu"
   },
   "source": [
    "From the results below, we see that the SVM model is significantly better than a random guess; however, the tree-based approach performed slightly better with a 4% increase in AUC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGeW0bwzvbbp"
   },
   "source": [
    "## **3. Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "YDx3zg_mTohS",
    "outputId": "f6846553-7318-431a-a006-740603406383"
   },
   "outputs": [],
   "source": [
    "# Performance\n",
    "\n",
    "# predicted probabilities generated by sklearn classifier\n",
    "y_pred_proba = clf_tuned.predict_proba(X_test)\n",
    "y_pred_probatree = clf_tree.predict_proba(X_test)\n",
    "\n",
    "# SVM ROC dependencies\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba[:, 1])\n",
    "# TREE ROC dependencies\n",
    "fpr_tree, tpr_tree, _ = roc_curve(y_test, y_pred_probatree[:, 1])\n",
    "\n",
    "auc = round(roc_auc_score(y_test, y_pred_proba[:, 1]), 4)\n",
    "auc_tree = round(roc_auc_score(y_test, y_pred_probatree[:, 1]), 4)\n",
    "\n",
    "# SVM Model\n",
    "plt.plot(fpr, tpr, label=\"SVM, auc=\" + str(auc))\n",
    "# Tree model\n",
    "plt.plot(fpr_tree, tpr_tree, label=\"Tree, auc=\" + str(auc_tree))\n",
    "# Random guess model\n",
    "plt.plot(fpr, fpr, \"-\", label=\"Random\")\n",
    "plt.title(\"ROC\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlabel(\"FPR\")\n",
    "\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEYq2t48gCq-"
   },
   "source": [
    "**Figure 2. ROC Curve for Three Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xh4HP91uhsj"
   },
   "source": [
    "## **4. Conclusion**\n",
    "This lesson focused on implementing the SVM model in Python on a predictive analytical problem, specifically a trading strategy for long positions of an index. We also included technical indicators as part of the predictors. We found that the SVM provides better results than a random guess, and the results are not too far off from those of a decision tree classifier. As an exercise, you can adjust this notebook to look at a hyperparameter-tuned decision tree classifier as well.\n",
    "In the next lesson, we will look at a model that is very effective on non-linear data, namely an artificial neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9115f10",
   "metadata": {},
   "source": [
    "---\n",
    "Copyright © 2022 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
