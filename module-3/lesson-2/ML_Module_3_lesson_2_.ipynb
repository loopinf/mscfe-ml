{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIM9Q2epGhzI"
   },
   "source": [
    "\n",
    "## MACHINE LEARNING IN FINANCE\n",
    "MODULE 3 | LESSON 2\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ijalya1GlLX"
   },
   "source": [
    "# **BAYESIAN STATISTICS IN PRACTICE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tdgnLDJGyVK"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  15 minutes |\n",
    "|**Prior Knowledge** |Naïve Bayes classifier methodology, likelihood, prior.  |\n",
    "|**Keywords** |Gaussian Distributions, Model fitting, Likelihood, Prior distribution  |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Np32o5ogH1Ke"
   },
   "source": [
    "*In the previous lesson, we looked at how Bayesian statistics can be used to assign the probability of an outcome occurring. We looked at categorical features and will now look at continuous features to predict the future one-month return of a group of stocks exceeding the median.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XywjHzrGgwe"
   },
   "source": [
    "## **1. Naïve Bayes in Practice**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1w-Ggx3xGgwg"
   },
   "source": [
    "Recall from Module 3, Lesson 1, that posterior distribution can be expressed as a product of the likelihoods of the predictors given the class and the prior distributions of the class, as in (1.4). This is based on the assumption of conditional independence. This is true for predictors that are numeric and continuous as well since these predictors also have likelihood distributions given a specific target class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9dYvGinjgWr"
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "P(Y=k | X_{1}, X_{2}, \\cdots , X_{n}) = \\frac{P(X_{1}|Y=k)P(X_{2}|Y=k)\\cdots P(X_{n}|Y=k)P(Y=k)}{\\sum_{j}P(Y=j)\\prod_{i=1}^{n}P(X_{i}|Y=j)}\n",
    "\\tag{2.1}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvqDuJLIGgwh"
   },
   "source": [
    "Let's consider a case where the likelihood distribution of the predictor follows a Gaussian distribution for each class. The predictor would be a strong explanatory variable, should the distributions differ. A strong predictor, which is often what we aim for, has very little overlap between the likelihoods for each class, thus differentiating between classes. This is shown in figure 1 for both a strong and weak predictor. The top plot shows the likelihood for a strong predictor given the target class as there is little overlap between the distributions (the two curves). The bottom plot shows a weaker predictor with significantly more overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaQH00CHqH87"
   },
   "source": [
    "**Fig 1: Strong vs. Weak Predictor Likelihoods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../images/M3Lesson2_plt1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIR_GQ1hGgwi",
    "outputId": "2b399b52-739b-4184-fec8-205931c5f14b"
   },
   "source": [
    "![](../../images/M3Lesson2_plt2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulQuu_jaGgwj"
   },
   "source": [
    "To put these ideas into perspective, let us look at an example where we apply the Naïve Bayes classifier to a stock returns problem.\n",
    "\n",
    "In this section, we use data from Coqueret and Guida (chapter 9, section 9.4), specifically the future one-month returns data for a collection of stocks and financial data. This data can be accessed via https://github.com/shokru/mlfactor.github.io/tree/master/material. For illustrative purposes, we take a subset of stocks and features to predict the future one-month price being above the median, thus a classification problem. The features we look into are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSiS-tPuGgwk"
   },
   "source": [
    "* *Mkt_Cap_12M_Usd* - Size\n",
    "* *Pb* - Price to book\n",
    "* *Vol1Y_Usd* - For share turnover\n",
    "* *Return_On_Capital*\n",
    "* *Roe* - Return on equity\n",
    "* *Pe* - Price to Earnings ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hy74gG30Ggwk"
   },
   "source": [
    "We perform the analysis using the Python statistical software. Let us load the necessary libraries and import the data. We'll work with data from the year 2000 to 2018 and only 50 stock IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Dm2Q2HhGgwk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_50 = pd.read_csv(\"../../data/mlfac_dat.csv\")\n",
    "\n",
    "# consider data from 2000 to 2018\n",
    "\n",
    "df_50 = df_50[(df_50[\"date\"] > \"1999-12-31\") & (df_50[\"date\"] < \"2019-01-01\")]\n",
    "\n",
    "# Filter on the first 50 stock ids\n",
    "\n",
    "df_50 = df_50[df_50[\"stock_id\"] <= 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqb1El8KGgwl"
   },
   "source": [
    "The objective is to predict whether the future one-month returns are above the median. Hence, we can simply flag instances when it is above the median as 1 and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNZLZ8llGgwl"
   },
   "outputs": [],
   "source": [
    "# Created a class variable for 1-Month returns if > median\n",
    "\n",
    "df_50[\"R1M_Usd_C\"] = np.where(df_50[\"R1M_Usd\"] > df_50[\"R1M_Usd\"].median(), 1, 0)\n",
    "\n",
    "# Make the response variable into integer format\n",
    "\n",
    "df_50[\"R1M_Usd_C\"] = df_50[\"R1M_Usd_C\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB0CcwRpGgwl"
   },
   "source": [
    "Specify the predictors or features used in the classifier. These features have been transformed into variables that follow a uniform distribution. In practice, we can hope for Gaussian-distributed variables. Therefore, we will transform each feature using the Box-Muller transformation (Box and Muller 610)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "do87ShdXGgwm"
   },
   "outputs": [],
   "source": [
    "# create a copy of data to store Gaussian converted data\n",
    "\n",
    "df50_norm = df_50.copy()\n",
    "\n",
    "# Features of interest\n",
    "feature_cols = [\"Mkt_Cap_12M_Usd\", \"Pb\", \"Vol1Y_Usd\", \"Return_On_Capital\", \"Roe\", \"Pe\"]\n",
    "\n",
    "# loop through Box-Muller for each data point for each predictor\n",
    "for col in feature_cols:\n",
    "\n",
    "    for i in np.arange(len(df_50)):\n",
    "        # 1. Use U1 as current data value. Generate U2, which is Unif(0, 1)\n",
    "\n",
    "        u1s, u2s = (\n",
    "            df_50.loc[i, col],\n",
    "            np.random.uniform(low=0.0, high=1.0, size=1)[0],\n",
    "        )  # X.loc[i, col]\n",
    "\n",
    "        # 2. Transform U1 to s\n",
    "\n",
    "        ss = -np.log(u1s)\n",
    "\n",
    "        # 3. Transform U2 to theta\n",
    "\n",
    "        thetas = 2 * np.pi * u2s\n",
    "\n",
    "        # 4. Convert s to r\n",
    "\n",
    "        rs = np.sqrt(2 * ss)\n",
    "\n",
    "        # 5. Calculate x and y from r and theta\n",
    "\n",
    "        xs, ys = rs * np.cos(thetas), rs * np.sin(thetas)\n",
    "\n",
    "        # 6. Store only one of the Gaussian derived values\n",
    "\n",
    "        df50_norm.loc[i, col] = ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzlqcA7MGgwm"
   },
   "source": [
    "It is worth looking at the likelihoods given the target class to gauge how strong or weak we can expect the predictors to be. We do this using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2XBOKAFGgwm",
    "outputId": "0b08486c-77a6-402c-f949-1430a3351e58"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10), sharey=True)\n",
    "\n",
    "i = 0\n",
    "\n",
    "sns.kdeplot(ax=axes[0, i], data=df50_norm, x=feature_cols[i], hue=\"R1M_Usd_C\")\n",
    "\n",
    "i = 1\n",
    "\n",
    "sns.kdeplot(ax=axes[0, i], data=df50_norm, x=feature_cols[i], hue=\"R1M_Usd_C\")\n",
    "\n",
    "i = 2\n",
    "\n",
    "sns.kdeplot(ax=axes[0, i], data=df50_norm, x=feature_cols[i], hue=\"R1M_Usd_C\")\n",
    "\n",
    "i = 0\n",
    "\n",
    "sns.kdeplot(ax=axes[1, i], data=df50_norm, x=feature_cols[i + 3], hue=\"R1M_Usd_C\")\n",
    "\n",
    "i = 1\n",
    "\n",
    "sns.kdeplot(ax=axes[1, i], data=df50_norm, x=feature_cols[i + 3], hue=\"R1M_Usd_C\")\n",
    "\n",
    "i = 2\n",
    "\n",
    "sns.kdeplot(ax=axes[1, i], data=df50_norm, x=feature_cols[i + 3], hue=\"R1M_Usd_C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYMYSh_gGgwn"
   },
   "source": [
    "**Fig. 2: Likelihood for All 6 Features Given the Respective Target Class** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-2AjmSjGgwn"
   },
   "source": [
    "Looking at the output in figure 2 and comparing it to figure 1, we can see the predictive power of the features are quite weak considering the significant overlap in the likelihoods. We can therefore expect poor model performance of the classifier. \n",
    "\n",
    "We then load the library needed to split the dataset into a training and testing dataset to train and evaluate the classifier, respectively. An 80/20 train/test split is used. We consider only the features mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHLc4IKdGgwn"
   },
   "outputs": [],
   "source": [
    "# split dataset in features and target variable\n",
    "\n",
    "X = df50_norm[feature_cols]  # Transformed Features\n",
    "\n",
    "y = df50_norm[\"R1M_Usd_C\"]  # Target variable\n",
    "\n",
    "# Split dataset into training set and test set 80/20\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUCJTgy7Ggwn"
   },
   "source": [
    "Let us fit the Naïve Bayes classifier on the training set and obtain predictions on the unseen test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKhWlyZuGgwn",
    "outputId": "cfb92cc7-cff4-4a8d-dd80-45ea2c9635ef"
   },
   "outputs": [],
   "source": [
    "# initialize and fit classifier to training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = GaussianNB()\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# get the predictions\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Extract the probabilities of Target Class = 1.\n",
    "\n",
    "y_predprobs = classifier.predict_proba(X_test)\n",
    "\n",
    "probs = y_predprobs[:, 1]\n",
    "\n",
    "# print the accuracy\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-Tis8puGgwn"
   },
   "source": [
    "The ROC curve and AUC can be obtained using the code below. It is useful to also show a no-skill or random guess classifier in the plot to compare to the Naïve Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2aXpH39Ggwo",
    "outputId": "b38b5b80-e142-4b1c-d3b4-7aee47e9b18d"
   },
   "outputs": [],
   "source": [
    "# plot the roc curve for the model\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# generate a no skill or random guess prediction\n",
    "\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "\n",
    "# calculate roc curves\n",
    "\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "\n",
    "# calculate scores\n",
    "\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)  # random guess\n",
    "\n",
    "tree_auc = roc_auc_score(y_test, probs)  # tree classifier\n",
    "\n",
    "# summarize scores\n",
    "\n",
    "print(\"No Skill: ROC AUC=%.3f\" % (ns_auc))\n",
    "\n",
    "print(\"Clasifier: ROC AUC=%.3f\" % (tree_auc))\n",
    "\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle=\"--\", linewidth=4, label=\"No Skill\")\n",
    "\n",
    "plt.plot(\n",
    "    ns_fpr,\n",
    "    ns_tpr,\n",
    "    linewidth=2,\n",
    "    label=\"NB Classifier\",\n",
    ")\n",
    "\n",
    "# axis labels\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "\n",
    "# show the legend\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# show the plot\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwjZn7p4Ggwo"
   },
   "source": [
    "**Fig. 3: ROC Curve of Naïve Bayes Classifier Performance along with a Random Guess or No-Skill Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMm-FiTkGgwo"
   },
   "source": [
    "The accuracy is ~47.9%, thus slightly worse than a random guess of 50%. The corresponding ROC curve is shown in figure 3. The AUC for this model is 50.1%, thus confirming our suspicion of its weak predictive power. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjLQAnDaGgwo"
   },
   "source": [
    "## **2. Conclusion**\n",
    "\n",
    "The example covered in this lesson helps us understand the Naïve Bayes classifier by seeing it in practice. The model performance, however, is not as good as we would hope. This lesson focused on showing how to apply Naïve Bayes to a real-world classification problem using continuous predictors and how Naïve Bayes can easily be applied to categorical variables. To obtain a better model, stronger predictors are needed, i.e., explanatory variables that have different likelihood distributions for the target classes. Explanatory data analysis, feature selection, and engineering cannot be understated when dealing with modeling. \n",
    "\n",
    "In the next section, we will look at tree-based algorithms, another modeling technique. The Naïve Bayes model assumes the features are conditionally independent of the target variable; however, tree-based algorithms do not require any assumptions about the underlying distribution. Both Naïve Bayes and trees do not require feature scaling, although tree-based algorithms are not sensitive to outliers. The prediction approach of trees are to place decision boundaries in the data whereas Naïve Bayes aims to model how the data were generated. These are known as discriminative and generative models, respectively. We will look at trees in depth in the next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xNZuKwEGgwo"
   },
   "source": [
    "**References**\n",
    "\n",
    "- Box, G. E. P., and Mervin E. Muller. “A Note on the Generation of Random Normal Deviates.” *The Annals of Mathematical Statistics*, vol. 29, no. 2, 1958, pp. 610–11, https://doi.org/10.1214/aoms/1177706645.\n",
    "\n",
    "- Coqueret, Guillaume, and Tony Guida. *Machine Learning for Factor Investing: R Version.* Financial Mathematics Series. Chapman and Hall/CRC, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRaIJ97IGgwo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
