{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqyexxheDY7f"
   },
   "source": [
    "\n",
    "## MACHINE LEARNING IN FINANCE\n",
    "MODULE 3 | LESSON 4\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLOTckMbDiu3"
   },
   "source": [
    "# **TREES IN PRACTICE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukcwOJyLDxke"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  12 minutes |\n",
    "|**Prior Knowledge** | Tree-based machine learning methodology  |\n",
    "|**Keywords** |Model fitting, Training models, Testing models, Model performance, ROC, Area Under the Curve  |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMZQZwxjHg5w"
   },
   "source": [
    "*In the previous lesson, we looked at how tree-based algorithms can be used to predict class outcomes. We now apply this theory to predict one-month returns of a group of stocks exceeding the median or not.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VojJsqT08LET"
   },
   "source": [
    "## **1. Trees in Practice**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-8_YLCX8LEU"
   },
   "source": [
    "Recall from Module 3, Lesson 3 that given a set of data with predictors and a target variable with class outcomes, we can fit a decision tree classifier that can intuitively show the rules or criteria that makes a target class highly likely. This non-black box characteristic is a benefit of decision trees as we will show in this lesson by fitting a tree to stock data in order to predict whether a future one-month return will change by more than a threshold or not, i.e., we consider the same binary class problem in lesson 2 of this module.\n",
    "\n",
    "We therefore use the same data (from Coqueret and Guida chapter 9, section 9.4), specifically the future one-month returns data for a collection of stocks and financial data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4bDuxCdRvrI"
   },
   "source": [
    "## **2. The Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8BGgNakR2TD"
   },
   "source": [
    "The data can be accessed via https://github.com/shokru/mlfactor.github.io/tree/master/material. Recall that the features we look into are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "637e-AmD8LEV"
   },
   "source": [
    "* *Mkt_Cap_12M_Usd* - Size\n",
    "* *Pb* - Price to book\n",
    "* *Vol1Y_Usd* - For share turnover\n",
    "* *Return_On_Capital*\n",
    "* *Roe* - Return on equity\n",
    "* *Pe* - Price to Earnings ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03RLEk3a8LEV"
   },
   "source": [
    "We use Python to perform the analysis and take the same data from the year 2000 to 2018 with only 50 stock IDs. Let us read in the data as in Lesson 2 and take a preview. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ofDMwZo78LEW",
    "outputId": "9797b533-8ba9-4f98-d21b-113cd26463da"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# location of the data\n",
    "\n",
    "# ENTER LOCATION OF DATA FILE BELOW\n",
    "df_50 = pd.read_csv(\"../../data/mlfac_dat.csv\")\n",
    "\n",
    "# consider data from 2000 to 2018\n",
    "\n",
    "df_50 = df_50[(df_50[\"date\"] > \"1999-12-31\") & (df_50[\"date\"] < \"2019-01-01\")]\n",
    "\n",
    "# Filter on the first 50 stock ids\n",
    "\n",
    "df_50 = df_50[df_50[\"stock_id\"] <= 50]\n",
    "\n",
    "# Created a class variable for 1-Month returns if\n",
    "# changes by more than a threshold = thresh\n",
    "\n",
    "thresh = 0.04\n",
    "df_50[\"R1M_Usd_C\"] = np.where(df_50[\"R1M_Usd\"].abs() > thresh, 1, 0)\n",
    "\n",
    "# Make the response variable into integer format\n",
    "\n",
    "df_50[\"R1M_Usd_C\"] = df_50[\"R1M_Usd_C\"].astype(int)\n",
    "\n",
    "# Features of interest\n",
    "feature_cols = [\"Mkt_Cap_12M_Usd\", \"Pb\", \"Vol1Y_Usd\", \"Return_On_Capital\", \"Roe\", \"Pe\"]\n",
    "\n",
    "\n",
    "df_50 = df_50[feature_cols + [\"R1M_Usd_C\"]]\n",
    "\n",
    "df_50.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSLwLsiH8LEY"
   },
   "source": [
    "We see 6 predictors with the target variable being the one-month future returns (R1M_Used_C), that is a binary variable taking on 1 if the return is more than 4% in either a long or short direction and 0 otherwise. Keep in mind that features have been pre-selected in this example. In practice, however, feature selection generally takes a significant time to perform and often adds much value in improving performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8s4XXg1SPw_"
   },
   "source": [
    "## **3. Train the Tree**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3mr32gwSJVL"
   },
   "source": [
    "Now that we have a view of the data, we will proceed with splitting the dataset into a train/test 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUZ1A8MX8LEZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split dataset in features and target variable\n",
    "X = df_50[feature_cols]  # Transformed Features\n",
    "\n",
    "y = df_50[\"R1M_Usd_C\"]  # Target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3PxPP0i8LEZ"
   },
   "source": [
    "We now declare our decision tree classifier using the `sklearn`package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eafJTtXB8LEa"
   },
   "outputs": [],
   "source": [
    "# Import Decision Tree Classifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create Decision Tree classifier object\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=\"gini\", max_depth=3, min_samples_leaf=25)\n",
    "\n",
    "# Train Decision Tree classifier\n",
    "\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjeHjM0_8LEb"
   },
   "source": [
    "The decision tree produces a set of rules that are easily interpretable in our case since we have used only a few features for illustrative purposes. A flow diagram of the decision tree can be shown using the code below, which will produce a flow diagram as in Figure 1. This shows that companies with turnover above 0.905 are 78% likely to have returns that change by more than 4%. This is a strong differentiating node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 687
    },
    "id": "vj3I6Z0p8LEb",
    "outputId": "ed065bde-f8ae-410e-f5f2-c2214a2e1db7"
   },
   "outputs": [],
   "source": [
    "# visualize the tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# features we used\n",
    "\n",
    "fn = feature_cols\n",
    "\n",
    "# labels of the target class\n",
    "\n",
    "cn = [\"0\", \"1\"]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(25, 20))\n",
    "_ = tree.plot_tree(clf, feature_names=fn, class_names=cn, filled=True)\n",
    "\n",
    "# save the figure to file\n",
    "\n",
    "# fig.savefig('imagename.png')\n",
    "fig.savefig(\"decistion_tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0jeW1w58LEb"
   },
   "source": [
    "**Fig. 1: Flow Diagram of Decision Tree for Predicting Price Change of More than 4% in Any Direction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ol5UVAqsd8iH"
   },
   "source": [
    "Figure 1 shows an absolute change exceeding 4% as class 1 and class 0 otherwise. The blue nodes represent cases that majority have class 1, i.e., the darker the shade of blue, the purer the node. Similarly, for Class 0, the more orange the node, the higher proportion of Class 0.\n",
    "\n",
    "It is worth pointing out that the cuts in Trees are parallel to existing axis whereas in SVM the cuts can be, but not restricted to, a linear combination of features. The sequential nature of the splitting shown in Fig 1 shows that only one feature can be used at a time. SVM on the other hand is not restricted by this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFvsRw6dS1Zl"
   },
   "source": [
    "## **4. Tree Performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v24eQ5fW8LEc"
   },
   "source": [
    "We've trained our decision tree classifier and we are now ready to test the performance. To do this, we need to run the unseen test dataset through the classifier to determine the class predictions for the target variable. It would be insightful to compare this to a random guess classifier, i.e., a model that requires no skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqZxixIz8LEc"
   },
   "outputs": [],
   "source": [
    "# Predict the response for test dataset\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Obtain the probabilities\n",
    "probs_tmp = clf.predict_proba(X_test)\n",
    "\n",
    "probs = probs_tmp[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uForPtc8LEc"
   },
   "source": [
    "Fortunately, `Sklearn` has tools to evaluate performance metrics for most machine learning algorithms. We look at the ROC curve and AUC as in Lesson 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "IkBA6ZPm8LEd",
    "outputId": "7ed6e6d6-2c9c-47e3-b2b0-a7686eb75383"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# plot the roc curve for the model\n",
    "\n",
    "# generate a no skill or random guess prediction\n",
    "\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "\n",
    "# calculate roc curves for random guess\n",
    "\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "\n",
    "# calculate roc curves for the decision tree\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, probs)\n",
    "\n",
    "# calculate scores\n",
    "\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)  # random guess\n",
    "\n",
    "tree_auc = roc_auc_score(y_test, probs)  # tree classifier\n",
    "\n",
    "# summarize scores\n",
    "\n",
    "print(\"No Skill: ROC AUC=%.3f\" % (ns_auc))\n",
    "\n",
    "print(\"Clasifier: ROC AUC=%.3f\" % (tree_auc))\n",
    "\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle=\"--\", label=\"No Skill\")\n",
    "\n",
    "plt.plot(fpr, tpr, marker=\".\", label=\"Tree Classifier\")\n",
    "\n",
    "# axis labels\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "\n",
    "# show the legend\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# show the plot\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pI_ikj158LEf"
   },
   "source": [
    "**Fig. 2: ROC Curve of the Decision Tree Classifier vs. a Random Guess or No Skill Predictor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGneGI4q8LEf"
   },
   "source": [
    "The AUC for the classifier is 60.6%, which is better than a random guess. It is worth noting that a subset of predictors was chosen to illustrate the decision tree flow diagram in Figure 1. Feature selection plays an important part in model performance and should not be overlooked. \n",
    "\n",
    "Lastly, it is worth looking at the implementation of a tree-based algorithm that uses boosting. The algorithm we consider is popular in the Kaggle community and can be easily implemented in Python using the `xgboost` package. The hyperparameters have been specified, but not optimized, instead of the default settings. For further reading on these parameters, please refer to (“`XGBoost` Parameters — `Xgboost` 1.6.1 Documentation”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZ7wr2a18LEf"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Init classifier\n",
    "xgb_cl = xgb.XGBClassifier(\n",
    "    max_depth=2,\n",
    "    eta=0.1,  # learning rate\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    ")\n",
    "\n",
    "# Fit\n",
    "xgb_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "preds_xgb = xgb_cl.predict(X_test)\n",
    "\n",
    "# Obtain the probabilities\n",
    "preds_prob_xgb_tmp = xgb_cl.predict_proba(X_test)\n",
    "\n",
    "preds_prob_xgb = preds_prob_xgb_tmp[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjO8IP5o8LEg"
   },
   "source": [
    "The results of the `xgboost` model along with the decision tree and random guess classifier is shown in figure 3 below. The `XGBoost` ties with the decision tree at 60.6%. The reader, as an exercise, can decrease the max depth of the tree to determine the change in performance. This will show the sensitivity to the hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "dKYw6WPA8LEg",
    "outputId": "7c6b2a36-5c38-4f4b-f250-217f26bd4dae"
   },
   "outputs": [],
   "source": [
    "# generate a no skill or random guess prediction\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "# calculate roc curves\n",
    "# random\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "# tree\n",
    "fpr, tpr, _ = roc_curve(y_test, probs)\n",
    "# xgb\n",
    "xgb_fpr, xgb_tpr, _ = roc_curve(y_test, preds_prob_xgb)\n",
    "\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)  # random guess\n",
    "tree_auc = roc_auc_score(y_test, probs)  # tree classifier\n",
    "xgb_auc = roc_auc_score(y_test, preds_prob_xgb)  # tree classifier\n",
    "# summarize scores\n",
    "print(\"No Skill: ROC AUC=%.3f\" % (ns_auc))\n",
    "print(\"Tree Clasifier: ROC AUC=%.3f\" % (tree_auc))\n",
    "print(\"XGB Clasifier: ROC AUC=%.3f\" % (xgb_auc))\n",
    "\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle=\"--\", label=\"No Skill\")\n",
    "plt.plot(fpr, tpr, marker=\".\", label=\"Tree Classifier\")\n",
    "plt.plot(xgb_fpr, xgb_tpr, marker=\".\", label=\"XGB Classifier\")\n",
    "# axis labels\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqU1dA6p8LEg"
   },
   "source": [
    "**Fig. 3**: ROC Curves for the Tree, `XGBoost`, and No-Skill Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoHm2cE08LEh"
   },
   "source": [
    "## **5. Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rO77Gyu-8LEh"
   },
   "source": [
    "The examples illustrated in this lesson help us understand the tree-based algorithms from the simple decision tree to the `xgboost` model. The model performance is better than a random guess thus showing added benefit. This lesson focused on showing how to apply tree-based algorithms to a real-world classification problem. A model that does better than a random guess can be easily motivated; however, keep in mind the goal of the specific classification problem, as it may require a minimum AUC to be achieved. The next module will explore support vector machines and neural networks, another class of predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pt6Pl3Kp8LEh"
   },
   "source": [
    "**References**\n",
    "\n",
    "- Coqueret, Guillaume, and Tony Guida. *Machine Learning for Factor Investing: R Version.* CRC Financial Mathematics Series, Chapman and Hall/CRC, 2022.\n",
    "\n",
    "- “`XGBoost` Parameters — `Xgboost` 1.6.1 Documentation.” `XGBoost` Parameters, https://xgboost.readthedocs.io/en/stable/parameter.html. Accessed 30 June 2022.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
