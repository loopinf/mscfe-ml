{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nqttp31vYiID"
   },
   "source": [
    "## MACHINE LEARNING IN FINANCE\n",
    "MODULE 1 | LESSON 4\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdaKlN6TYmHr"
   },
   "source": [
    "# **SUPERVISED MODELS: CLASSIFICATION CASE STUDY** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWKB5HJBZOlc"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  60 minutes |\n",
    "|**Prior Knowledge** | Logistic regression, Confusion matrix  |\n",
    "|**Keywords** | Probability of default  |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8oqygeRq9s9"
   },
   "source": [
    "*In the last lesson of Module 1, we are going to develop an application of a Machine Learning classification model to predict the probability of default of loan applicants.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6M_A90F7PxUD"
   },
   "source": [
    "## **1. Predicting the Probability of Default**\n",
    "\n",
    "The probability of default indicates the likelihood that a borrower will be unable to meet their debt payment obligations (either interest or principal payments). Default means that the lender has the legal right to attempt the recovery of their debt by seizing the borrower's assets. The higher the borrower's probability of default, the higher the interest rate charged by the lender, or the lower the amount the borrower would be eligible to obtain from the bank.\n",
    "\n",
    "We are going to exploit real-world information on default events from a bank in Israel. In this lesson, we will develop a credit scoring model to determine the probability that a loan holder will default training a model from part of the data. We will train the model by looking at historical records of loans held and the individual characteristics of the borrowers (e.g., age, educational level, debt-to-income ratio, and other variables).\n",
    "\n",
    "Let's read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1667758186704,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "JdJ7vkx-GyT1",
    "outputId": "93662690-d53c-4c94-f1d2-35bcd91cc128"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"bank.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55BrwSRvtcf9"
   },
   "source": [
    "The dataset includes a sample of several tens of thousands of previously granted loans. It covers 41,188 records and 10 fields. The data includes an individual identifier and shows whether each loan defaulted or not ($y=0$ for no default and $y=1$ for default), as well as borrowers' features: age, education level (university degree, high school, illiterate, basic education, and professional coursework), years with current employer, years in the same home, income, debt-to-income ratio, credit card debt, and other debt. \n",
    "\n",
    "We will exploit the informativeness that several variables may have at predicting the default behavior of individuals using a logistic regression model. The model will be of use to banks or credit issuers to estimate the probability of default of an individual credit applicant with certain characteristics. A good prediction model will allow the bank to provide better and personalized financial services to customers.\n",
    "\n",
    "The education variable is categorical with several categories. For estimation of the model, we are going to build a separate indicator variable (or \"dummy variable\") that shows if an individual has each education level (value of 1) or not (value of zero).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AHvtZfj9IQFl"
   },
   "outputs": [],
   "source": [
    "cat_vars = [\"education\"]\n",
    "for var in cat_vars:\n",
    "    cat_list = \"var\" + \"_\" + var\n",
    "    cat_list = pd.get_dummies(df[var], prefix=var)\n",
    "    data1 = df.join(cat_list)\n",
    "    df = data1\n",
    "cat_vars = [\"education\"]\n",
    "data_vars = df.columns.values.tolist()\n",
    "to_keep = [i for i in data_vars if i not in cat_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1667758187428,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "sX6jU27NJUtr",
    "outputId": "0b8c9072-88a3-4992-ba61-21e8fce55bad"
   },
   "outputs": [],
   "source": [
    "df_final = df[to_keep]\n",
    "df_final.drop([\"loan_applicant_id\"], axis=1, inplace=True)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zsgv7vU5w-xF"
   },
   "source": [
    "As usual, we divide the sample into a training set and a test set. The dataset includes more than 41,000 observations that we split in half to determine both samples. Default takes place in roughly 11% of the observations in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1667758187429,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "5DVBKf2HHQD-",
    "outputId": "f19d1a81-7539-4f7f-dce7-ad2052868de9"
   },
   "outputs": [],
   "source": [
    "X, y = (\n",
    "    df_final.loc[:, df_final.columns != \"y\"],\n",
    "    df_final.loc[:, df_final.columns == \"y\"],\n",
    ")\n",
    "print(X.shape, y.shape)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=int(len(y) * 0.5), shuffle=False\n",
    ")\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1667758187430,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "jb8KuPjtxPxE",
    "outputId": "d665c7e1-516c-4cdb-96ad-a936e95f4468"
   },
   "outputs": [],
   "source": [
    "print(df_final[\"y\"].value_counts())\n",
    "print(\n",
    "    \"Percentage of default: \",\n",
    "    100\n",
    "    * df_final[\"y\"].value_counts()[1]\n",
    "    / (df_final[\"y\"].value_counts()[0] + df_final[\"y\"].value_counts()[1]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33dinFO2P_fL"
   },
   "source": [
    "## **2. Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bb-2U0BDckGt"
   },
   "source": [
    "Before training ML algorithms, we should be sure that we are proficient in the information we are working with. Obtaining some prior knowledge about how the data behaves can provide useful guidance to improve the performance of the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AmyqkLwbczz"
   },
   "source": [
    "### **2.1 Summary Statistics**\n",
    "\n",
    "Computing summary statistics (also called descriptive statistics) is a useful first task that can provide us a first glimpse at the behavior of our data. Averages and medians inform us about the representative value of each variable in our instances. Ranges and variances (or standard deviations) are informative about the dispersion of the variables and may be useful to identify outliers (more on outliers below).\n",
    "\n",
    "Obtaining the main descriptive statistics of all the variables is very easy with pandas, as shown below. If we take a look at the means or medians of the data as measures of representativeness for each variable, we can see that the individuals in our sample are middle-aged, with long tenures in their jobs, have spent a relatively long period in the same home, and are relatively high-income individuals by Israeli standards, while they are lowly indebted (16% of income). Lastly, the sample is evenly distributed in terms of education across the five levels.\n",
    "\n",
    "If we take a look at other features of the distribution of input features, such as dispersion, skewness, and kurtosis, we observe that the variables with the most extreme variation are those associated with the amount of debt, both credit and other types of debt. That is, there are several individuals that have extreme levels of debt in absolute terms, relative to most of the individuals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1667758187430,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "xkpf0EwohG32",
    "outputId": "57fc1c28-6563-4871-ecf6-1c41773d1317"
   },
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1667758187431,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "WeIncmUuXOpk",
    "outputId": "34ee48be-841f-47c4-ff1e-08e388524a2e"
   },
   "outputs": [],
   "source": [
    "print(X.skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667758187431,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "jR8DU5IPXdSs",
    "outputId": "098fc29f-2bde-4068-ab3f-d09680118cdc"
   },
   "outputs": [],
   "source": [
    "print(X.kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbtY380ceG_H"
   },
   "source": [
    "Let's visualize the different aspects of the distributions of each input feature to have a better understanding of the data and explore which variables are more related to default. We are going to plot the distributions of each variable on the same scale. Thus, for better visualization, we are going to standardize the data, i.e., for each input feature, we subtract its mean and divide by its standard deviation. Remember that this is one of the rescaling options we mentioned in Lesson 3.\n",
    "\n",
    "On top of the standardization, we plot the distribution of each variable separately for those instances where we observe a default and where we do not observe it. Notice in the figure below that four variables display the most significant differences in their distributions between non-defaulting and defaulting individuals: household income, debt-to-income ratio, credit card debt, and other debt. As suggested by the descriptive statistics shown above, the level of credit card debt and other types of debt include individuals with very extreme values.\n",
    "\n",
    "Notice also that the education attained is does not seem relevant as a default predictor since all the distributions are more or less similar across all education levels. Given this observation we may opt to exclude these variables from our model since they seem to provide little predictive ability compared to the other features. However, we should keep in mind also that there may be relevant non-linear interactions among variables that may help to predict the outcome of interest, despite seeming individually irrelevant at first. Although our Logistic Regression model will ignore the interactions between variables, we should take into account this possibility in more advanced ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "executionInfo": {
     "elapsed": 3052,
     "status": "ok",
     "timestamp": 1667758190474,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "9Mx_K1rVgU9a",
    "outputId": "6b86cc45-bda4-4e75-dd93-d9c940bc2562"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "X_mean = X.mean()\n",
    "X_std = X.std()\n",
    "X_std = (X - X_mean) / X_std\n",
    "X_std[\"y\"] = y\n",
    "X_std = X_std.melt(id_vars=[\"y\"], var_name=\"Column\", value_name=\"Normalized\")\n",
    "plt.figure(figsize=(18, 6))\n",
    "ax = sns.violinplot(data=X_std, x=\"Column\", y=\"Normalized\", hue=\"y\", split=True)\n",
    "_ = ax.set_xticklabels(X_train.keys(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pxtmcDhgNQC"
   },
   "source": [
    "To further explore the relationship between default and each input feature, we can perform logistic regressions where we only include one feature, on top of the constant bias term. For better visualization and comparability, now we rescale the variables using a min-max method that constrains the value of each variable into the unit interval. Notice again that four variables generate most of the action. \n",
    "\n",
    "The analysis so far may lead us to think that we could just ignore most of the features. However, notice that the regressions above are very close to linear regressions, which neglect the presence of non-linearities in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "executionInfo": {
     "elapsed": 3109,
     "status": "ok",
     "timestamp": 1667758193581,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "qJPyIA3B5ge0",
    "outputId": "846cec9d-162e-4d13-adec-b85c773d8b63"
   },
   "outputs": [],
   "source": [
    "X_minmax = (X - X.min()) / (X.max() - X.min())\n",
    "X_minmax[\"y\"] = y\n",
    "plt.figure(figsize=(18, 6))\n",
    "g = 1\n",
    "for colname in X:\n",
    "    plt.subplot(3, 4, g)\n",
    "    sns.regplot(\n",
    "        x=colname,\n",
    "        y=\"y\",\n",
    "        data=X_minmax,\n",
    "        marker=\"\",\n",
    "        logistic=True,\n",
    "        fit_reg=True,\n",
    "        ci=None,\n",
    "        label=colname,\n",
    "    )\n",
    "    plt.title(colname, fontsize=\"large\")\n",
    "    plt.xlabel(\"\")\n",
    "    g += 1\n",
    "    # sns.lmplot(x=colname, y='y', data=X_minmax, markers = \"\", logistic = True, fit_reg=True, ci=None)\n",
    "fig = plt.gcf()\n",
    "fig.tight_layout(pad=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a7EVVuLkjqf"
   },
   "source": [
    "Another important step in data preprocessing is the analysis of correlations between features. Our prediction models may well fail if the ML algorithm is fed with many features that are strongly correlated: The optimization algorithms will face difficulties at disentangling which variable is really relevant for prediction, slowing down the training process.\n",
    "\n",
    "Below, we display the correlation matrix of the input features, which tell us the degree of covariation between each pair of features. The variables debt-to-income, credit-card debt, and other debt have correlation coefficients above 0.5 among them. Although these correlations are not a big deal, we may have to verify the performance of ML models that exclude at least one of these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667758193581,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "4dHDeF7x4SRl",
    "outputId": "6cdc09cc-2a47-44d1-ad50-1c8dde70d9a3"
   },
   "outputs": [],
   "source": [
    "X.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjUsX3oreG04"
   },
   "source": [
    "In practice, some applications may involve a number of predictors that make it unfeasible to look at these simple metrics for all the variables. However, minimal verification is recommended. To further ease the analysis:\n",
    "  * Focus on a subset of predictors that a priori should be more relevant when predicting the outcome that we want to predict, e.g., income and debt levels seem the most reasonable in our application here.\n",
    "  * Track outliers in the summary statistics (more on this below).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vX2144-1bxEy"
   },
   "source": [
    "### **2.2 Missing Data and Outlier Detection**\n",
    "\n",
    "There are mainly two ways to deal with missing data: removal and imputation. Removal is agnostic but costly if one whole instance is eliminated due to the presence of a single missing feature value. Imputation overcomes the loss of information but requires making assumptions, which are often erroneous. \n",
    "\n",
    "When facing missing data, we can opt to replace it with the medians or means computed over the cross-section of assets. This implies that the missing feature value will be located within the bulk of observed values. If many values are missing, we would substantially alter the original distribution of the feature and we should assess if it makes sense to include it in our data. \n",
    "\n",
    "In time series contexts, with views towards backtesting and prediction, the most simple imputation comes from replacing the missing information with the previous value of the time series. If we want to predict returns, however, imputation from past values should be avoided. By default, a superior choice is to set missing return indicators to zero, which is often close to the average or median. Still if a feature is highly autocorrelated, i.e., the correlation between future and past values of the variable are close to one, then imputation from the past can make sense. If not, then it should be avoided. \n",
    "\n",
    "In time series problems, such as the prediction of future returns, we should also avoid interpolation as a way to fill in missing data. Interpolation over time periods means that we are predicting a future outcome using information that is not available at the time of the prediction. For instance, if earnings figures are disclosed in April and July, interpolating May and June requires the knowledge of July's earnings. In May, we will not have information about July's earnings, so we cannot make predictions. In this setup, resorting to past values is a better way to go.\n",
    "\n",
    "Sometimes, we may also detect values of input features that are extremely different from most of the data. We can opt for several heuristic methods that deal with such situations if we are concerned about the effect on the performance of our ML algorithms. These methods involve setting thresholds that determine if a value is considered an outlier:\n",
    "\n",
    "  * Any point outside the interval $[\\mu−m\\sigma,\\mu+m\\sigma]$ can be deemed an outlier. $\\mu$ is the mean of the sample and $\\sigma$ the standard deviation. How stringent we are at labeling outliers is modulated by the multiple value $m$, which usually belongs to the set $\\{3,5,10\\}$.\n",
    "  * If the largest value is above $m$ times the second-to-largest, then it can also be classified as an outlier (the same reasoning applied for the left side of the distribution).\n",
    "  * For a given small threshold $q$, any value outside the $[q,1−q]$ quantile range can be considered an outlier. The range for $q$ is usually (0.5%,5%) with 1% and 2% being the most often used.\n",
    "\n",
    "Once we have labeled the outliers, we must determine whether to include them or how to include them in our data. If we decide to include them, the most common practice is to replace the value of the outlier with the corresponding upper or lower threshold that we have obtained following either of the methods above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZ5E-nj6b7Ml"
   },
   "source": [
    "### **2.3 Feature Selection and Scaling**\n",
    "\n",
    "If we have at our disposal a large set of predictors, it is reasonable to filter out redundant or unwanted input features. Simple methods include:\n",
    "\n",
    "* Computing the correlation matrix of all input features and filter variables so that no (absolute) value is above a threshold (say, 0.7) so that redundant variables do not pollute the learning engine.\n",
    "* Training linear regressions and removing the non-significant variables.\n",
    "* Using unsupervised methods (clustering or principal components analysis) to retain a reduced number of features to use as inputs in the algorithm.\n",
    "\n",
    "The methods above may overlook nonlinear relationships. Another approach would be to fit a decision tree (or a random forest) and retain only the features that have a high variable importance or exploit an autoencoder architecture. You will learn these methods in other modules and courses.\n",
    "\n",
    "Lastly, as we stressed in the previous lesson, rescaling is needed in many ML applications. Optimization algorithms will better learn about the relevance of each input feature if they share a common scale among them. In the data analysis we performed above, we already executed to versions of feature re-scaling: normalization and min-max rescaling in the unit interval. Using `scikitlearn`, below, we again use min-max rescaling, but in the $[-1,1]$ interval, to train our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlZdVzfn3aFF"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_input = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_input.fit(X_train)\n",
    "X_train = scaler_input.transform(X_train)\n",
    "X_test = scaler_input.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MmjGtX1QEjy"
   },
   "source": [
    "## **3. Logistic Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6bC2VUS3oYj"
   },
   "source": [
    "Now turn to train the prediction model. We do not specify any particular parameters when we define the class `logisticRegr`, meaning that we use the default optimization options. This implies that the model is regularized using ridge regularization, where the parameter $\\alpha$ is set to 1. Please review the documentation to understand the different options at hand to refine the training of the model.\n",
    "\n",
    "Below, we display the trained parameters of the model, which inform us about the effect of each variable on the probability of default. For instance, households with a higher debt-to-income ratio have a greater probability of default, which is intuitive from an economic standpoint. However, other parameters may make less sense at first glance. For instance, older individuals, longer tenures, or university education seem positively associated with default. Notice that this dataset provides the default behavior of individuals *conditional* on that individual having obtained a loan. For instance, older individuals usually have lower levels of debt than younger individuals. Thus, old individuals that have loans outstanding are likely to be in a worse financial condition than relatively similar young individuals. We can make similar interpretations for the remaining trained parameters.\n",
    "\n",
    "In any case, our core task here is to develop a model that predicts well out of sample, rather than making sense of the relationship between each particular input and the probability of default. Thus, the trained parameters are of lesser importance if we can obtain a good predictive model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667758193583,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "oqaHfsvuH02c",
    "outputId": "e248478c-1be4-48df-e81c-54efc8905d63"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# parameters not specified, all are set to their defaults\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667758193918,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "phmddN6b0xkj",
    "outputId": "7adb6af8-5028-46a7-8d82-b81f8e31188e"
   },
   "outputs": [],
   "source": [
    "print(logisticRegr.intercept_)\n",
    "for cc in range(len(X.keys())):\n",
    "    print(X.keys()[cc], logisticRegr.coef_[0, cc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TTq89DC3r2e"
   },
   "source": [
    "Below, we display the accuracy of the trained model, which is roughly 92.5%. The average probability of default in the sample is 11.3%, so the model seems relatively able to discriminate some of the default cases in the test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnwKHUzbJ6_K"
   },
   "outputs": [],
   "source": [
    "predictions = logisticRegr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667758193918,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "xn5BIu3KJ8q6",
    "outputId": "df567739-6de4-4e4e-fbe5-8fd18688260b"
   },
   "outputs": [],
   "source": [
    "# Use score method to get accuracy of model\n",
    "score = logisticRegr.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SO6TvVF93xPm"
   },
   "source": [
    "Because analyzing the accuracy of the model may be misleading, we are going to obtain additional performance measures that we described in the previous lesson. \n",
    "\n",
    "First, we obtain and depict the confusion matrix. Notice now that the model fails quite terribly at predicting many of the default events. That is, the model is quite conservative and predicts in most cases that individuals will not default. This yields a low number of \"false positives\" but a large number of \"false negatives.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "executionInfo": {
     "elapsed": 439,
     "status": "ok",
     "timestamp": 1667758194355,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "m9Md8D8fJ_tq",
    "outputId": "f9a01eda-9ebb-4dbe-8a44-1d81d273c011"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "plt.figure(figsize=(9, 9))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=0.5, square=True, cmap=\"Blues_r\")\n",
    "plt.ylabel(\"Actual label\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "all_sample_title = \"Accuracy Score: {0}\".format(score)\n",
    "plt.title(all_sample_title, size=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8k2d_zjQ3zla"
   },
   "source": [
    "The precision of class 1 in the test set tells us out of all the default predictions made by the model, 98% were actually \"bad\" loan applicants. The recall of class 1 in the test set indicates the proportion of the defaulting loan applicants that our model has managed to identify as a \"bad\" loan applicants. So, the model only managed to identify 34% of \"bad\" loan applicants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667758194356,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "L14df-5aKL2Z",
    "outputId": "08c3c3a1-be4c-4fdc-c141-edca56e9811e"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3JOdAvLNy6B"
   },
   "source": [
    "Below, we depict the ROC curve, which tells us how the model's classification performs when we wish to increase recall, at the expense of increasing the proportion of true positives. The Area Under the Curve (AUC) is well below 1. Notice that a 0% of false positives can be obtained by only accepting around a 40% of true positives, which is close to what the trained model yields with threshold probabilities of 50%. Achieving a higher share of true positives can be obtained only at the cost of predicting more \"good\" individuals as bad. Thus, in an imperfect model like this, we can use our own discretion to choose if we prefer a model that over- or under-estimates the probability of default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "executionInfo": {
     "elapsed": 486,
     "status": "ok",
     "timestamp": 1667758194839,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "mGxUut7XKXPp",
    "outputId": "395f02eb-5bbe-4c4d-ea4f-4623fd1dae63"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "logit_roc_auc = roc_auc_score(y_test, logisticRegr.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logisticRegr.predict_proba(X_test)[:, 1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=\"Logistic Regression (area = %0.2f)\" % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUAqE6GF34r9"
   },
   "source": [
    "A simple alternative to improve the prediction of default by the model is to increase its complexity using powers of the input features, as we did in Lesson 1. Adding this, we can improve the fit of the model by taking into account non-linearities in the relationship between default and the input features. For instance, the probability of default may not increase or decrease monotonically with age; it may well be lower for middle-aged individuals relative to younger or older ones.\n",
    "\n",
    "In the block of code below, we train a logistic regression model where the input features include up to the 6th power of each input feature. In the optimization, we increase the maximum number of iterations relative to the default value of 100 due to the increased complexity of the model. Moreover, we also adjust the regularization parameter, setting \"C\" to 2.5. This parameter is an inverse indicator of the strength of regularization, the parameter $\\alpha$ in Lesson 2, meaning that we are regularizing less than in the default case where it takes the value of 1. Feel free to check the documentation of the function and play around with the parameters of the model to see its performance under different settings.\n",
    "\n",
    "The results we show below suggest a considerable improvement from adding powers of the input features. The recall increases up to 54%, which is still quite bad, but much better than the previous model. This suggests that non-linearities are relevant to improve the predictions of the model. \n",
    "\n",
    "The relevance of these non-linearities is difficult to gauge from direct observation of the data. Other tools in machine learning, such as neural networks, are able to \"identify\" them in a flexible manner without the need to strictly impose functional forms and improve upon the learning ability of the models. You will study these topics in later modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3519,
     "status": "ok",
     "timestamp": 1667758198347,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "DKCAtfPs1PfZ",
    "outputId": "4f9b54c1-3936-4973-f51d-8acc773b33c8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Redefine the input feature matrix to include powers of each feature\n",
    "Xpoly = X\n",
    "for pp in range(2, 6):\n",
    "    Xpoly = np.concatenate((Xpoly, np.power(X, pp)), axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    Xpoly, y, test_size=int(len(y) * 0.5), shuffle=False\n",
    ")\n",
    "# Scale the features\n",
    "scaler_input = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_input.fit(X_train)\n",
    "X_train = scaler_input.transform(X_train)\n",
    "X_test = scaler_input.transform(X_test)\n",
    "# Set up Logistic Regression\n",
    "logisticRegr = LogisticRegression(C=2.5, max_iter=500)\n",
    "logisticRegr.fit(X_train, y_train)\n",
    "# Display coefficients\n",
    "print(logisticRegr.intercept_)\n",
    "print(logisticRegr.coef_)\n",
    "# Compute accuracy\n",
    "predictions = logisticRegr.predict(X_test)\n",
    "score = logisticRegr.score(X_test, y_test)\n",
    "print(score)\n",
    "# Display confusion matrix and other indicators\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "plt.figure(figsize=(9, 9))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=0.5, square=True, cmap=\"Blues_r\")\n",
    "plt.ylabel(\"Actual label\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "all_sample_title = \"Accuracy Score: {0}\".format(score)\n",
    "plt.title(all_sample_title, size=15)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "executionInfo": {
     "elapsed": 631,
     "status": "ok",
     "timestamp": 1667758198974,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "KQ1jcSfNimmm",
    "outputId": "8ef17a33-a6d0-4996-dbf6-60bfaa7aa45b"
   },
   "outputs": [],
   "source": [
    "logit_roc_auc = roc_auc_score(y_test, logisticRegr.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logisticRegr.predict_proba(X_test)[:, 1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=\"Logistic Regression (area = %0.2f)\" % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsFUSVmBQKw8"
   },
   "source": [
    "## **4. Conclusion**\n",
    "\n",
    "We have covered a complete case study to predict the probability of default using the logistic model. In the next modules, you will learn further useful machine learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Copyright © 2022 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
