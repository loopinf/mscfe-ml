{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8pfxRMBEKxyK"
   },
   "source": [
    "## MACHINE LEARNING IN FINANCE\n",
    "MODULE 1 | LESSON 2\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2sewv-DaK1AF"
   },
   "source": [
    "# **SUPERVISED MODELS: REGRESSION AND HYPERPARAMETERS** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "XvJ0xrnPLlA0"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  60 minutes |\n",
    "|**Prior Knowledge** | Linear regression  \n",
    "|**Keywords** | Penalized regressions, linear models, hyperparameters  |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ddQvQRbldw65"
   },
   "source": [
    "*In this lesson, we will introduce in a linear regression framework the main techniques for parameter regularization, cross-validation, and hyperparameter tuning.*<span style='color: transparent; font-size:1%'>All rights reserved WQU WorldQuant University QQQQ</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "g6b1_uvWMu5_"
   },
   "source": [
    "## **1. Linear Models and Overfitting**\n",
    "\n",
    "In the linear regression context, a good way to reduce the overfitting of a model is to regularize (constrain) the parameters, sometimes also called \"weights.\" Regularization reduces the degrees of freedom of the model to fit the training data.\n",
    "\n",
    "In general form, let $J(\\theta)$ denote the cost function, during training, of a model with vector of parameters $\\theta$. Then, regularization is achieved by adding a penalty term to the MSE of the model, so that the cost function from linear regression becomes:\n",
    "$$\n",
    "\\begin{align*}\n",
    "J(\\theta) = MSE(\\theta) + Penalty(\\theta)\n",
    "\\end{align*} \n",
    "$$\n",
    "\n",
    "We next describe three methods--Ridge, Lasso, and Elastic Net--which specify penalty, or regularization, terms to constrain the weights. One important aspect is that, in any method, the penalty term should only be added to the cost function during training. In the validation or test samples, we are only interested in evaluating the unregularized performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "SE1_oFsJNFrs"
   },
   "source": [
    "### **1.1 Ridge**\n",
    "\n",
    "The Ridge method is called a *shrinkage* regularization technique that aims at keeping the smallest values possible for the parameters of the model. To achieve this, the regularization term is of the form\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\alpha\\sum_{i=1}^n \\theta_i^2\n",
    "\\end{align*}\n",
    "$$\n",
    "The hyperparameter $\\alpha$ determines how much we wish to regularize the model. If $\\alpha=0$, we are trivially not regularizing the model while, if $\\alpha$ is very large, then all parameters will be close to zero and our fit of the data will be a flat line through the average of the labels $y$ in the training sample (Notice that the regularization term above does not include the bias term $\\theta_0$). \n",
    "\n",
    "The cost function for a linear regression with a ridge penalty (Ridge Regression) is then given by\n",
    "$$\n",
    "\\begin{align*}\n",
    "J(\\theta) = MSE(\\theta) + \\frac{1}{2}\\alpha\\sum_{i=1}^n \\theta_i^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus, if we perform gradient descent, the update of the parameters will be given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\theta^{[next\\ step]}=\\theta^{[previous\\ step]} - \\eta \\nabla_{\\theta} MSE\\left(\\theta^{[previous\\ step]}\\right) - a \\odot \\theta^{[previous\\ step]}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $a$ is a column vector of $n$ elements all of which have the value $\\alpha$, except the first term that has a value of 0, in association with the term $\\theta_0$ not being regularized, and $\\odot$ refers to element-by-element multiplication. Finally, notice that the $1/2$ multiplying $\\alpha$ and the parameters in the penalty function is just to get a cleaner derivative once we compute the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "pQmeCo3gNJnj"
   },
   "source": [
    "### **1.2 Lasso**\n",
    "\n",
    "The Lasso (Least Absolute Shrinkage and Selection Operator) penalty is another regularization method that tends to eliminate the weights of the least important features, setting them to zero. The regularization term is of the form:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\alpha\\sum_{i=1}^n |\\theta_i|\n",
    "\\end{align*} \n",
    "$$\n",
    "\n",
    "Notice that this term is an $\\ell_1$ norm, while the Ridge version uses an $\\ell_2$ norm of the parameter vector. The hyperparameter $\\alpha$ again determines the overall importance of the regularization of parameters. The Lasso Regression cost function then reads as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "J(\\theta) = MSE(\\theta) + \\alpha\\sum_{i=1}^n |\\theta_i|\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The Lasso Regression performs a feature selection and outputs a sparse model with few non-zero parameters. Because the Lasso term is not differentiable at $\\theta_i=0$, the gradient descent algorithm needs to be adjusted properly using a *subgradient* method:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\theta^{[next\\ step]}=\\theta^{[previous\\ step]} - \\eta \\nabla_{\\theta} MSE\\left(\\theta^{[previous\\ step]}\\right) - a \\odot \\pmatrix{0 \\\\ sign(\\theta_1)\\\\ \\vdots \\\\ sign(\\theta_n)}\n",
    "\\end{align*} \n",
    "$$\n",
    "\n",
    "where \n",
    "$$\n",
    "\\begin{align*}\n",
    "sign(\\theta_i) = \\begin{cases} -1\\ \\text{if}\\ \\theta_i < 0 \\\\\n",
    "                                0\\ \\ \\ \\ \\text{if}\\ \\theta_i = 0 \\\\\n",
    "                                +1\\ \\text{if}\\ \\theta_i > 0 \\end{cases}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "HRdVSv7ZNLc1"
   },
   "source": [
    "### **1.3 Elastic Net**\n",
    "\n",
    "The Elastic Net combines Ridge and Lasso regularization in a single optimization, where a hyperparameter determines the relative combination of shrinkage and feature selection when training the model. The Elastic Net cost function is given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "J(\\theta) = MSE(\\theta) + r\\alpha\\sum_{i=1}^n |\\theta_i| + \\frac{1-r}{2}\\alpha\\sum_{i=1}^n \\theta_i^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The hyperparameter $\\alpha$ determines the overall importance of the regularization of parameters, while the hyperparameter $r$ balances the mix between feature selection and parameter shrinkage.\n",
    "\n",
    "Which of the three methods is preferable? Ridge is a good default, but we guess that only a few features are actually useful. You should opt for Lasso or Elastic Net because they remove the features that are useless. In general, a plain Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated. Thus, the Elastic Net is a good choice.\n",
    "\n",
    "Notice that in an Elastic Net Regression with Gradient Descent optimization, we would have, at least, three hyperparameters: The learning rate $\\eta$, the degree of overall regularization of the parameters $\\alpha$, and the relative weight of Lasso vs. Ridge regularization $r$. Next, we devise methods to obtain \"good\" values of those hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "CMSvEfhN-FWM"
   },
   "source": [
    "### **1.4 Elastic Net Regression and Times Series Momentum**\n",
    "\n",
    "To illustrate the workings of the Elastic Net Regression, we are going to return to our task of predicting stock returns based on past information. \n",
    "\n",
    "We first upload the necessary packages and define the input feature matrix $X$ and labels $y$ as in the previous lesson. We also divide the sample into a training set and a test set (we will introduce the validation set below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1665965992234,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "03909397029203160502"
     },
     "user_tz": 240
    },
    "id": "Fap4Yz2ngT4v",
    "outputId": "c2c743a9-63ad-4507-baa0-1cf6dc66e304"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Getting historical market data from SPY (ETF) (SPY)\n",
    "df = yf.download(\"SPY\", start=\"2000-01-01\", end=\"2022-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 6761,
     "status": "ok",
     "timestamp": 1665966001815,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "03909397029203160502"
     },
     "user_tz": 240
    },
    "id": "gc8jzxKhgU8c",
    "outputId": "56b7bce9-4d33-4a35-f60e-29314034cbdf"
   },
   "outputs": [],
   "source": [
    "df[\"Ret\"] = df[\"Adj Close\"].pct_change()\n",
    "\n",
    "name = \"Ret\"\n",
    "df[\"Ret10_i\"] = (\n",
    "    df[name].rolling(10).apply(lambda x: 100 * ((np.prod(1 + x)) ** (1 / 10) - 1))\n",
    ")\n",
    "df[\"Ret25_i\"] = (\n",
    "    df[name].rolling(25).apply(lambda x: 100 * ((np.prod(1 + x)) ** (1 / 25) - 1))\n",
    ")\n",
    "df[\"Ret60_i\"] = (\n",
    "    df[name].rolling(60).apply(lambda x: 100 * ((np.prod(1 + x)) ** (1 / 60) - 1))\n",
    ")\n",
    "df[\"Ret120_i\"] = (\n",
    "    df[name].rolling(120).apply(lambda x: 100 * ((np.prod(1 + x)) ** (1 / 120) - 1))\n",
    ")\n",
    "df[\"Ret240_i\"] = (\n",
    "    df[name].rolling(240).apply(lambda x: 100 * ((np.prod(1 + x)) ** (1 / 240) - 1))\n",
    ")\n",
    "\n",
    "del df[\"Open\"]\n",
    "del df[\"Close\"]\n",
    "del df[\"High\"]\n",
    "del df[\"Low\"]\n",
    "del df[\"Volume\"]\n",
    "del df[\"Adj Close\"]\n",
    "\n",
    "df = df.dropna()\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1665966012725,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "03909397029203160502"
     },
     "user_tz": 240
    },
    "id": "M-2o92HrgXT-",
    "outputId": "a92e30ad-6825-4669-f685-e61b80a0f4f4"
   },
   "outputs": [],
   "source": [
    "df[\"Ret25\"] = df[\"Ret25_i\"].shift(-25)\n",
    "df = df.dropna()\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 128,
     "status": "ok",
     "timestamp": 1665966015995,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "03909397029203160502"
     },
     "user_tz": 240
    },
    "id": "gTGsJKQugxn_",
    "outputId": "f44fb1ab-d0a9-4da3-d92c-4a3749909921"
   },
   "outputs": [],
   "source": [
    "X, y = df.iloc[:, 0:-1], df.iloc[:, -1]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 138,
     "status": "ok",
     "timestamp": 1665966017404,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "03909397029203160502"
     },
     "user_tz": 240
    },
    "id": "MhFU7cBXggh_",
    "outputId": "c537607b-93c8-4720-fd8b-a8a4ca90b815"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=int(len(y) * 0.5), shuffle=False\n",
    ")\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7o-AlgBXAnHp"
   },
   "source": [
    "We first study separately the performance of ridge regression and lasso regression on their own. Recall that ridge regression forces the learning algorithm to not only fit the data but also keep the model parameters $\\theta$ as small as possible. \n",
    "\n",
    "In the code below, we keep the regularization parameter $\\alpha$ fixed to 0.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "wfpZiePPg8id"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import ridge regression from sklearn library\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1665966024812,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "03909397029203160502"
     },
     "user_tz": 240
    },
    "id": "8SH70s2Hg-Pm",
    "outputId": "05b60d9b-94cf-4fc7-9b3e-d9404aca2900"
   },
   "outputs": [],
   "source": [
    "ridgeR = Ridge(alpha=0.7)\n",
    "ridgeR.fit(X_train, y_train)\n",
    "y_pred_ridge = ridgeR.predict(X_train)\n",
    "y_pred_test_ridge = ridgeR.predict(X_test)\n",
    "\n",
    "# calculate mean squared error\n",
    "mean_squared_error = np.mean((y_pred_test_ridge - y_test) ** 2)\n",
    "print(\"Mean squared error on test set\", mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1665966026837,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "03909397029203160502"
     },
     "user_tz": 240
    },
    "id": "VrhhRFkAjbOF",
    "outputId": "21d3c013-e0d6-4b25-a172-c28b93745c36"
   },
   "outputs": [],
   "source": [
    "# get ridge coefficient and print them\n",
    "ridge_coefficient = pd.DataFrame()\n",
    "ridge_coefficient[\"Columns\"] = X_train.columns\n",
    "ridge_coefficient[\"Coefficient Estimate\"] = pd.Series(ridgeR.coef_)\n",
    "print(ridge_coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 359,
     "status": "ok",
     "timestamp": 1665966029983,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "03909397029203160502"
     },
     "user_tz": 240
    },
    "id": "Qva3SyP-j6eW",
    "outputId": "92f7e503-db54-4019-d826-4191073ff45e"
   },
   "outputs": [],
   "source": [
    "# plotting the coefficient score\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "\n",
    "color = [\n",
    "    \"tab:gray\",\n",
    "    \"tab:blue\",\n",
    "    \"tab:orange\",\n",
    "    \"tab:green\",\n",
    "    \"tab:red\",\n",
    "    \"tab:purple\",\n",
    "    \"tab:brown\",\n",
    "    \"tab:pink\",\n",
    "    \"tab:gray\",\n",
    "    \"tab:olive\",\n",
    "    \"tab:cyan\",\n",
    "    \"tab:orange\",\n",
    "    \"tab:green\",\n",
    "    \"tab:blue\",\n",
    "    \"tab:olive\",\n",
    "]\n",
    "\n",
    "ax.bar(\n",
    "    ridge_coefficient[\"Columns\"], ridge_coefficient[\"Coefficient Estimate\"], color=color\n",
    ")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "D8ElhbAmEoIe"
   },
   "source": [
    "Check how the model parameters have changed from training the model with the ridge regularization relative to the simple linear regression model. The parameter with the highest absolute value in the trained linear regression, the one associated with the most recent daily return, is adjusted downwards (in absolute terms). In contrast, the parameters on the most distant lags increase their relative importance. However, ridge regularization seems to add little in terms of prediction ability relative to linear regression (compared with the results we obtained in Lesson 1).\n",
    "\n",
    "Let's see how we can implement Lasso regularization in our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1665966042497,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "03909397029203160502"
     },
     "user_tz": 240
    },
    "id": "ayy36EsBhKGW",
    "outputId": "991db2e4-4d13-478e-ce5a-13681359e5bc"
   },
   "outputs": [],
   "source": [
    "# import Lasso regression from sklearn library\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Train the model\n",
    "lasso = Lasso(alpha=0.0001)\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred_Lasso = lasso.predict(X_train)\n",
    "y_pred_test_lasso = lasso.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mean_squared_error = np.mean((y_pred_test_lasso - y_test) ** 2)\n",
    "print(\"Mean squared error on test set\", mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 129,
     "status": "ok",
     "timestamp": 1665966044288,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "03909397029203160502"
     },
     "user_tz": 240
    },
    "id": "LAzJA8lUjWim",
    "outputId": "45b9b736-719a-4db7-dab7-ad982432571f"
   },
   "outputs": [],
   "source": [
    "lasso_coeff = pd.DataFrame()\n",
    "lasso_coeff[\"Columns\"] = X_train.columns\n",
    "lasso_coeff[\"Coefficient Estimate\"] = pd.Series(lasso.coef_)\n",
    "\n",
    "print(lasso_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "pmyvnenZFucp"
   },
   "source": [
    "Notice how Lasso regularization shrinks further the magnitude of the parameters relative to ridge regression, but none reaches a value of zero. Thus, none of the input features of the model are considered as irrelevant by the algorithm. Still, the trained model does not fare better than the Ridge version.\n",
    "\n",
    "Let's try now with the Elastic Net Regression model that combines both regularization features. Notice that the parameter \"l1_ratio\" in the code below determines the relative importance of lasso against ridge regularization in the model.\n",
    "\n",
    "We can see from the out-of-sample MSE that the combination of both regularization methods contributes little to our prediction problem, as we would expect from observing their separate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1665966048066,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "03909397029203160502"
     },
     "user_tz": 240
    },
    "id": "eJnxqXJEhSVV",
    "outputId": "ea1816cd-37ad-4120-c9f0-5cfd95ea387e"
   },
   "outputs": [],
   "source": [
    "# import model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Train the model\n",
    "e_net = ElasticNet(alpha=0.0001, l1_ratio=0.1)\n",
    "e_net.fit(X_train, y_train)\n",
    "\n",
    "# calculate the prediction and mean square error\n",
    "y_pred_elastic = e_net.predict(X_test)\n",
    "mean_squared_error = np.mean((y_pred_elastic - y_test) ** 2)\n",
    "print(\"Mean Squared Error on test set\", mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1665070639302,
     "user": {
      "displayName": "Alvaro Remesal Martin",
      "userId": "13772153249100390070"
     },
     "user_tz": -120
    },
    "id": "ONEMGe4Ei9R9",
    "outputId": "2ff1b909-8b60-41ff-de5d-1b1e6609c83a"
   },
   "outputs": [],
   "source": [
    "e_net_coeff = pd.DataFrame()\n",
    "e_net_coeff[\"Columns\"] = X_train.columns\n",
    "e_net_coeff[\"Coefficient Estimate\"] = pd.Series(e_net.coef_)\n",
    "e_net_coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5yd0Y-h4L2D9"
   },
   "source": [
    "## **2. Hyperparameter Tuning**\n",
    "\n",
    "Let's assume that you now have a shortlist of promising models. You now need to fine-tune them. Let's look at a few ways you can do that.\n",
    "\n",
    "### **2.1 Cross-Validation: GridSearchCV**\n",
    "\n",
    "One way to check what the best values are for the hyperparameters would be to manually check the performance of the models with different hyperparameters, but this would be quite inefficient because we may not have time to explore many combinations.\n",
    "\n",
    "A more efficient technique exploits cross-validation: the training set is split into complementary subsets or \"folds.\" For each fold, the model is trained against the remaining folds using different hyperparatemers, where the fold is used as a validation set. This is called *$k$-fold cross-validation*. Once the model type and hyperparameters have been selected, a final model is trained using these hyperparameters on the full training set,\n",
    "and the generalized error is measured on the test set.\n",
    "\n",
    "Scikit-Learn's GridSearchCV will do the fine-tuning and the cross validation. All you need to do is tell it which hyperparameters you want it to experiment with and what values to try out and it will evaluate all the possible combinations of hyperparameter values, using cross-validation.\n",
    "\n",
    "### **2.2 Other Tuners**\n",
    "\n",
    "The grid search approach is fine when you are exploring relatively few combinations, but when the hyperparameter search space is large, it is\n",
    "often preferable to use *Randomized Search* through RandomizedSearchCV instead. This class can be used in much the same way as the GridSearchCV class, but instead of trying out all possible combinations, it evaluates a given number of random combinations of hyperparameters by selecting a random value for each hyperparameter at every iteration. An advantage over grid search is that it will explore different values of the hyperparameters at each run, instead of combinations of values set in advance.\n",
    "\n",
    "Another way to fine-tune your system is to try to combine the models that perform best. If you aggregate the predictions of a group of predictors (such as different regression models), you will often get better predictions than with the best individual predictor. The *Ensemble Methods* allows us to combine several predictors from different trained models to reach an even better predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "RilQDsqfNpZr"
   },
   "source": [
    "## **3. Performance with Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "uDbKdbRlI87g"
   },
   "source": [
    "The code below performs a grid search cross-validation exercise for our Elastic Net Regression to predict stock returns. The $k$-fold algorithm randomly splits the training set into 10 different subsamples or \"folds\", the parameter \"n_splits\" below. Then, each fold is used as a validation set where the model is trained using the other folds. Then, we have $k$ trained models and we can compute the performance of each fold estimation across them in the test set.\n",
    "\n",
    "Because the $k$ folds are split randomly, a small value of $k$ can lead to noisy results. This means that each time the procedure is run, a different split of the dataset into $k$-folds can be implemented, and in turn, the distribution of performance scores can be different, resulting in a different mean estimate of model performance. To solve this, we can repeat the $k$-fold cross-validation process multiple times, the parameter \"n_repeat\" below, and report the mean performance across all folds and all repeats. This approach is generally referred to as repeated $k$-fold cross-validation.\n",
    "\n",
    "*Note: The parameter \"random_state\" makes sure that the algorithm yields the same reproducible output across multiple function calls.*\n",
    "\n",
    "Once we set up the cross validation technique, we evaluate the model for a pre-determined grid of our hyperparameters in the Elastic Net Regression, the overall degree of regularization, $\\alpha$, and the relative importance of Lasso over Ridge regularization, $r$. We rank models across each repeated $k$-fold validation using their MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 10196,
     "status": "ok",
     "timestamp": 1665966114312,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "03909397029203160502"
     },
     "user_tz": 240
    },
    "id": "VQWrkTf_nfBS",
    "outputId": "2449d6b9-37ce-45ae-dbc1-925906e2f68a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RepeatedKFold\n",
    "\n",
    "model = ElasticNet()\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define grid\n",
    "grid = dict()\n",
    "grid[\"alpha\"] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]\n",
    "grid[\"l1_ratio\"] = [0, 0.01, 0.1, 0.2, 0.5, 0.7, 1]\n",
    "search = GridSearchCV(model, grid, scoring=\"neg_mean_squared_error\", cv=cv, n_jobs=-1)\n",
    "# perform the search\n",
    "results = search.fit(X_train, y_train)\n",
    "# summarize\n",
    "print(\"MSE: %.3f\" % results.best_score_)\n",
    "print(\"Config: %s\" % results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "tAhKsP6jNwJ_"
   },
   "source": [
    "It turns out that the best model from cross-validation is very close to plain linear regression! The model includes a small degree of ridge regularization and no Lasso regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1665966114313,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "03909397029203160502"
     },
     "user_tz": 240
    },
    "id": "JDUsO7SGpJPZ",
    "outputId": "35b560fa-d1a8-44e3-a36a-b92d6072c0bb"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "e_net = ElasticNet(alpha=0.001, l1_ratio=0)  # Using the above hyperparameters\n",
    "e_net.fit(X_train, y_train)\n",
    "\n",
    "# calculate the prediction and mean square error\n",
    "y_pred_elastic = e_net.predict(X_test)\n",
    "mean_squared_error = np.mean((y_pred_elastic - y_test) ** 2)\n",
    "print(\"Mean Squared Error on test set\", mean_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "_0Ng_iT2Ns6U"
   },
   "source": [
    "## **4. Conclusion**\n",
    "\n",
    "In this lesson, we have described the main methods for regularization and fine-tuning techniques to obtain the best combinations of hyperparameters in the linear regression model. \n",
    "\n",
    "In the next lesson, we will analyze the main features of classification problems in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "Copyright 2024 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
