{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "T6sRX_-CnqSJ"
   },
   "source": [
    "## MACHINE LEARNING\n",
    "\n",
    "\n",
    "MODULE 8 | LESSON 4\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7X4viCnAnwP4"
   },
   "source": [
    "# **Multilayer Perceptron: Timing Factors and Smart-Beta Strategies** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "QWNvKxXRn--s"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  50 minutes |\n",
    "|**Prior Knowledge** |  Linear Regression, Neural Network, Machine Learning  |\n",
    "|**Keywords** | Multilayer Perceptron, Deep learning, Neural Network, Momentum |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3Eng-zthw-4y"
   },
   "source": [
    "*In this lesson, we will revisit the momentum timing strategy of Lesson 2 using our newly acquired knowledge of neural networks. Multilayer perceptron networks (MLPs) will add non-linearity considerations to the prediction model, which will likely enhance its power-but let's check by how much.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "GV579XHu_mHe"
   },
   "source": [
    "## **1. MLP for Financial Market Predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "YkX6o8DqujVw"
   },
   "source": [
    "Multilayer perceptrons (MLPs) are one of the simplest forms of neural networks and thus have been widely used for many applications, including the prediction of financial market returns. There have been several applications, including in the academic literature, exploiting the potential of MLPs. Here, we share a few of these works (you can easily find more online) for you to check in case you are interested. In the forthcoming Deep Learning in Finance course, we will review many more models and applications, but the works presented next could be used as a reasonable baseline from which to grow our models. As you will see, most of them do not really achieve strong return predictability:\n",
    "\n",
    "- Guresen, Erkam, et al. \"Using Artificial Neural Network Models in Stock Market Index Prediction.\" *Expert Systems with Applications*, vol. 38, no. 8, 2011), pp. 10389-10397, https://www.sciencedirect.com/science/article/abs/pii/S0957417411002740?via%3Dihub\n",
    "\n",
    "- Rather, Akhter Mohiuddin, et al. \"Recurrent Neural Network and a Hybrid Model for Prediction of Stock Returns.\" *Expert Systems with Applications* vol. 42, no. 6, 2015, pp. 3234-3241, https://www.sciencedirect.com/science/article/abs/pii/S0957417414007684\n",
    "\n",
    "- Devadoss, A. Victor, and T. Antony Alphonnse Ligori. \"Forecasting of Stock Prices using Multi Layer Perceptron.\" *International Journal of Computing Algorithm*, vol. 2, no. 1, 2013, pp. 440-449, http://ijwebt.com/abstract_meta_author.php?id=V2-I2-P6\n",
    "\n",
    "- Namdari, Alireza, and Zhaojun Steven Li. \"Integrating Fundamental and Technical Analysis of Stock Market through Multi-Layer Perceptron.\" 2018 IEEE Technology and Engineering Management Conference (TEMSCON). *IEEE*, 2018, https://ieeexplore.ieee.org/abstract/document/8488440?casa_token=hbO9qp7fjdMAAAAA:QsTb7IcWieVNRxCzuw1O57Pq23SbKcmEQ652cULn2enCiDbt9fGx2XKkPVDc9mAK7V8abX5y\n",
    "\n",
    "\n",
    "- Anand, C. \"Comparison of Stock Price Prediction Models using Pre-Trained Neural Networks.\" *Journal of Ubiquitous Computing and Communication Technologies*, vol. 3, no. 2, 2021, pp. 122-134, https://irojournals.com/jucct/article/view/3/2/5\n",
    "\n",
    "Let's see how our MLP model is able to help in the prediction of momentum factor returns that we tackled in Lesson 2 using linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9jtMiQZ64en-"
   },
   "source": [
    "## **2. Timing Momentum with Multilayer Perceptron (MLPs)**\n",
    "\n",
    "### **2.1. Data**\n",
    "\n",
    "First, we proceed to load and treat all the necessary data to construct our first deep learning model. The different steps undertaken are virtually the same as the ones from the linear regression case from Lesson 2. We refer you to that notebook in case you have doubts on any of the steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1673884594631,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "A6Mq7CinlWT5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1673884594890,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "b_c_E_aCYZ00"
   },
   "outputs": [],
   "source": [
    "route = \"10_Portfolios_Prior_12_2_Daily.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1673884595110,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "HIvgk4uqrG4h",
    "outputId": "a0f7453d-21f7-4ccf-8ff9-5280235023ed"
   },
   "outputs": [],
   "source": [
    "# Read the csv file again with skipped rows\n",
    "df = pd.read_csv(\"10_Portfolios_Prior_12_2_Daily.csv\", index_col=0)\n",
    "# Format the date index\n",
    "df.index = pd.to_datetime(df.index, format=\"%Y%m%d\")\n",
    "# Build de MOM strategy: Long \"Hi PRIOR\" and Short \"Lo PRIOR\"\n",
    "df[\"Mom\"] = df[\"Hi PRIOR\"] - df[\"Lo PRIOR\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7L3ux3snoOaq"
   },
   "source": [
    "- **Inputs and outputs**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 160327,
     "status": "ok",
     "timestamp": 1673884755434,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "3_9Ig05DuinD"
   },
   "outputs": [],
   "source": [
    "df[\"Ret\"] = df[\"Mom\"]\n",
    "df[\"Ret10_MOMi\"] = df[\"Mom\"].rolling(10).apply(lambda x: np.prod(1 + x / 100) - 1)\n",
    "df[\"Ret25_MOMi\"] = df[\"Mom\"].rolling(25).apply(lambda x: np.prod(1 + x / 100) - 1)\n",
    "df[\"Ret60_MOMi\"] = df[\"Mom\"].rolling(60).apply(lambda x: np.prod(1 + x / 100) - 1)\n",
    "df[\"Ret120_MOMi\"] = df[\"Mom\"].rolling(120).apply(lambda x: np.prod(1 + x / 100) - 1)\n",
    "df[\"Ret240_MOMi\"] = df[\"Mom\"].rolling(240).apply(lambda x: np.prod(1 + x / 100) - 1)\n",
    "\n",
    "df[\"Ret10_hi\"] = df[\"Hi PRIOR\"].rolling(10).apply(lambda x: np.prod(1 + x / 100) - 1)\n",
    "df[\"Ret25_hi\"] = df[\"Hi PRIOR\"].rolling(25).apply(lambda x: np.prod(1 + x / 100) - 1)\n",
    "df[\"Ret60_hi\"] = df[\"Hi PRIOR\"].rolling(60).apply(lambda x: np.prod(1 + x / 100) - 1)\n",
    "df[\"Ret120_hi\"] = df[\"Hi PRIOR\"].rolling(120).apply(lambda x: np.prod(1 + x / 100) - 1)\n",
    "df[\"Ret240_hi\"] = df[\"Hi PRIOR\"].rolling(240).apply(lambda x: np.prod(1 + x / 100) - 1)\n",
    "\n",
    "df[\"Ret10_Low\"] = df[\"Lo PRIOR\"].rolling(10).apply(lambda x: np.prod(1 + x / 100) - 1)\n",
    "df[\"Ret25_Low\"] = df[\"Lo PRIOR\"].rolling(25).apply(lambda x: np.prod(1 + x / 100) - 1)\n",
    "df[\"Ret60_Low\"] = df[\"Lo PRIOR\"].rolling(60).apply(lambda x: np.prod(1 + x / 100) - 1)\n",
    "df[\"Ret120_Low\"] = df[\"Lo PRIOR\"].rolling(120).apply(lambda x: np.prod(1 + x / 100) - 1)\n",
    "df[\"Ret240_Low\"] = df[\"Lo PRIOR\"].rolling(240).apply(lambda x: np.prod(1 + x / 100) - 1)\n",
    "\n",
    "df[\"Ret60\"] = df[\"Ret60_MOMi\"].shift(-60)\n",
    "df = df.dropna()\n",
    "df.tail(10)\n",
    "\n",
    "df = df.drop(\n",
    "    [\n",
    "        \"Lo PRIOR\",\n",
    "        \"PRIOR 2\",\n",
    "        \"PRIOR 3\",\n",
    "        \"PRIOR 4\",\n",
    "        \"PRIOR 5\",\n",
    "        \"PRIOR 6\",\n",
    "        \"PRIOR 7\",\n",
    "        \"PRIOR 8\",\n",
    "        \"PRIOR 9\",\n",
    "        \"Hi PRIOR\",\n",
    "        \"Mom\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1673884755435,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "wN8HcI-yvlft",
    "outputId": "d25c5c24-4637-46f9-e75b-04b7a1796c22"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "mg1-pl4kote6"
   },
   "source": [
    "- **Train-Test samples and Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 1370,
     "status": "ok",
     "timestamp": 1673884756800,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "JZpTdBo9wnXY",
    "outputId": "5a647185-ec25-4408-e93c-d9c370a1ee5f"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={\"index\": \"Date\"}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1673884756801,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "puxPDItushiM",
    "outputId": "c498c0a5-3823-40b5-cafa-25a435e3281d"
   },
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "ts = int(0.4 * len(df))  # Number of observations in the test sample\n",
    "split_time = len(df) - ts  # From this data we are in the test sample\n",
    "test_time = df.iloc[split_time:, 0:1].values  # Keep the test sample dates\n",
    "Ret_vector = df.iloc[split_time:, 1:2].values\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 352,
     "status": "ok",
     "timestamp": 1673884757145,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "z69qkHmNrKc-",
    "outputId": "2bf6a23c-9955-4587-d537-ab1db18f34ea"
   },
   "outputs": [],
   "source": [
    "Xdf, ydf = df.iloc[:, 2:-1], df.iloc[:, -1]\n",
    "X = Xdf.astype(\"float32\")\n",
    "y = ydf.astype(\"float32\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=ts, shuffle=False\n",
    ")  # It is important to keep \"shuffle=False\"\n",
    "n_features = X_train.shape[1]\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1673884757146,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "ONAjMSJxw0ES"
   },
   "outputs": [],
   "source": [
    "# Scaling\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_input = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_input.fit(X_train)\n",
    "X_train = scaler_input.transform(X_train)\n",
    "X_test = scaler_input.transform(X_test)\n",
    "\n",
    "mean_ret = np.mean(y_train)  # Useful to compute the performance = R2\n",
    "\n",
    "scaler_output = MinMaxScaler(feature_range=(-1, 1))\n",
    "y_train = y_train.values.reshape(len(y_train), 1)\n",
    "y_test = y_test.values.reshape(len(y_test), 1)\n",
    "scaler_output.fit(y_train)\n",
    "y_train = scaler_output.transform(y_train)\n",
    "y_test = scaler_output.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "XnHTHlOVoyxC"
   },
   "source": [
    "### **2.2. MLP Model and Training**\n",
    "\n",
    "Now that we have all our data ready, let's build our first MLP model. To that end, we will define several things:\n",
    "\n",
    "- **Activation function** \n",
    "\n",
    "Although the different hidden layers may employ different activation functions, in this case, we will select the same one for all hidden layers: the rectified linear unit (**ReLU**).\n",
    "\n",
    "- **Hidden layers and units within layers**\n",
    "\n",
    "In this model, we will use a total of 3 hidden layers. Each of these layers will have 50, 30, and 10 units respectively in order from the input layer.\n",
    "\n",
    "- **Output layer**\n",
    "\n",
    "Of course, we need to define a final fully connected layer for the output\n",
    "\n",
    "- **Learning rate**\n",
    "\n",
    "As before, we will choose a learning rate of $10^{-5}$. (*Note that we will play around with this hyperparameter--i.e., tuning it--in the forthcoming Deep Learning in Finance course.*)\n",
    "\n",
    "- **Optimizer**\n",
    "\n",
    "We will choose the **Adam** optimizer, which you are already familiar with.\n",
    "\n",
    "- **Loss function**\n",
    "\n",
    "Different from the linear regression case, here we will select a loss function based on the **mean absolute error (MAE)**. This is:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    L(y, \\hat{y}) = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i| \n",
    "\\end{equation*}\n",
    "$$\n",
    "\\\n",
    "Note that, importantly, this error function has a different treatment of outliers than the MSE (mean squared error) function that we have used in linear regression (essentially because it does not compute the power of the differences!). This can be important, especially in the finance context, when we do not want to give too much importance to a few outliers that are simply \"extreme events,\" not representative of the overall sample. Of course, it would be a completely different story if our model aimed at predicting extreme events (e.g., firm bankruptcy, client default on credit card debt, etc.), since these extreme values will be the main focus of our investigation. \n",
    "\n",
    "\\\n",
    "***NOTE*, also, that we are setting our seed to be constant**. This is important to note, because different training could lead to different final outcomes due to a variety of things (e.g., random weight initialization in Keras). You are welcome to remove this seed and check how things change!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 3930,
     "status": "ok",
     "timestamp": 1673884761073,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "WVOFNUaG6sgc",
    "outputId": "8f3d1fbd-6501-4e21-ac9f-5205d1b6c211"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(12345)\n",
    "\n",
    "act_fun = \"relu\"  # Activation function\n",
    "hp_units = 50  # Units in the first hidden layer\n",
    "hp_units_2 = 30  # Units in the second hidden layer\n",
    "hp_units_3 = 10  # Units in the third hidden layer\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=hp_units, activation=act_fun))\n",
    "model.add(tf.keras.layers.Dense(units=hp_units_2, activation=act_fun))\n",
    "model.add(tf.keras.layers.Dense(units=hp_units_3, activation=act_fun))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "hp_lr = 1e-5\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(lr=hp_lr)\n",
    "model.compile(optimizer=adam, loss=\"mean_absolute_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "rFHhbZqnhNSw"
   },
   "source": [
    "Once we have defined our model, we can train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 42091,
     "status": "ok",
     "timestamp": 1673884803161,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "O3THLocE7tha",
    "outputId": "4f8d6f6f-1ade-4713-8b9b-6b512222bb80"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=30, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Liyeh8oihV5n"
   },
   "source": [
    "As before, we can get a feeling of how the model works by calling 'model.summary()':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1673884803161,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "oe92AMyApmW4",
    "outputId": "8bd7f838-8ef9-4e5e-b4b6-5a52333b6bac"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Xk0R2BURi7dn"
   },
   "source": [
    "Now that we have a few hidden layers with several units in each one, being able to interpret the architecture and understand how many parameters are trained at each step of the model becomes crucial. \n",
    "\n",
    "In this case, the 'summary()' option of Keras is already telling us how many parameters are trained in each layer. But where do these parameters come from?\n",
    "\n",
    "For example, in the first layer of the model there are 800 parameters. These are equal to the number of units in the layer (50) times the number of different inputs (15)-because there would be a weight associated with each input and unit in the layer-plus the bias terms $b$ for each unit (50). Thus, $15 \\times 50 + 50 = 800$.\n",
    "\n",
    "Where does the number of $1530$ parameters from the second layer come from?\n",
    "This is equal to the number of \"inputs\" to this layer (which is essentially the number of units in the previous layer (50) times the number of units in the layer (30) plus the bias term for each unit (30). Thus, $50 \\times 30 + 30 = 1530$.\n",
    "\n",
    "You can check the number of parameters in the other layers using the same logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "pBPbVMPgptw4"
   },
   "source": [
    "### **2.3. Validation and Early stopping**\n",
    "\n",
    "- **How many epochs should we use when training our models?**\n",
    "\n",
    "These are very legitimate questions to ask at this point. The choice of the number of epochs that we have used so far (30) is completely discretionary and not based at all on any relevant criteria. We are going to change this now by including **Early stopping** in our training.\n",
    "\n",
    "This means that we are going to instruct Keras to stop model training when some condition is met. This will be done via the **callback API** in Keras, as we will see shortly.\n",
    "\n",
    "- **When should we stop training?**\n",
    "\n",
    "There are multiple ways to define a stopping criterion. We are going to use one of the most common and focus on monitoring the loss function in a separate validation set. Once after each epoch of the training process, we will check if (and how much) the loss function in the validation set decreases. We will also define a parameter, **patience**, that indicates the number of epochs with no improvement in the validation set that we tolerate before Early stopping training. \n",
    "\n",
    "*More information on how **Callback API** and **Early stopping** in Keras works can be found here:* https://keras.io/api/callbacks/early_stopping/ \n",
    "\n",
    "Let's therefore implement Early stopping when training our algorithm. First, we need to define the characteristics of Early stopping, where we indicate:\n",
    "\n",
    "1. **The quantity/set to monitor**: in our case the validation set loss function (we will define the validation set in a minute).\n",
    "\n",
    "2. **The 'mode'**: by setting this to 'min' we ensure training will stop when the quantity set in (1) has stop decreasing.\n",
    "\n",
    "3. **Patience**: we will allow for 10 epochs with no improvement in minimizing the loss function of the validation set before we stop the training.\n",
    "\n",
    "4. **restore_best_weights**: due to the iteration process, it may be the case that the last iteration before stopping training does not yield the model weights that achieve the lowest loss function in validation. By setting this option to 'True' we ensure that we keep the weights that achieved the best loss function value (i.e., the lowest) in validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1673884803470,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "IQwZ-Hbpqlhy"
   },
   "outputs": [],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", mode=\"min\", verbose=1, patience=10, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "QFf-Qb-QMc8h"
   },
   "source": [
    "Finally, we can re-train our model with the Early stopping callback. Also, note that, because we need it for the Early stopping function, we define a validation set of 20\\% of the training set. Importantly, this split is performed by Keras with no shuffling of the data, and this set is kept apart (i.e., there is no training nor testing going on in the validation set). Lastly, note that we incorporate the callback feature 'es' defined before.\n",
    "\n",
    "We have selected here 100 epochs for training but included an Early stopping criterion. Would the model train for the whole 100 epochs? Let's see...<span style='color: transparent; font-size:1%'>All rights reserved WQU WorldQuant University QQQQ</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 8919,
     "status": "ok",
     "timestamp": 1673884812386,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "OSX896Q6rMzj",
    "outputId": "6a53d5bf-b4ff-461f-f6b5-96103f7c5b82"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=2,\n",
    "    callbacks=[es],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Rw8m5pbqOuqo"
   },
   "source": [
    "As you can see, given our new criteria, model training has stopped at epoch 11, when the Early stopping kicks in. \n",
    "\n",
    "Now, let's see if our simple deep learning model is able to produce a better prediction of the momentum factor returns than our linear regression did!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hARI1-u6tZEf"
   },
   "source": [
    "## **3. MLP Model Performance for Momentum Timing**\n",
    "\n",
    "As we did in the case of linear regression, we will::\n",
    "\n",
    "1. Evaluate the predictive performance of our MLP-based: Out-of-sample $R^2$\n",
    "2. Assess the viability of this by backtesting our trading strategy\n",
    "\n",
    "Let's start with the first one.\n",
    "\n",
    "### **3.1. Out-of-Sample Predictive Power ($R^2_{OS}$)**\n",
    "\n",
    "Let's first look at the out-of-sample explanatory power of our model via the $R^2_{OS}$ in Campbell and Thompson (2008). The construction of this measure is identical to what we did in Lesson 2 of the module. Please refer there if there are doubts in this regard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 792,
     "status": "ok",
     "timestamp": 1673884813164,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "eZxJ4PfemYIP",
    "outputId": "fb3dd282-938f-46a3-b751-31d1b27a0f23"
   },
   "outputs": [],
   "source": [
    "values = scaler_output.inverse_transform(y_test)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = scaler_output.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1673884813164,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "QD6kFyjcxkOx",
    "outputId": "230132d6-bf79-42b2-e9b9-089987691194"
   },
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1673884813165,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "zWmixBepwkdt",
    "outputId": "8259168b-d3d7-4172-d6d0-c44657ec30a6"
   },
   "outputs": [],
   "source": [
    "def R2_campbell(y_true, y_predicted, mean_ret):\n",
    "    y_predicted = y_predicted.reshape((-1,))\n",
    "    sse = sum((y_true - y_predicted) ** 2)\n",
    "    tse = sum((y_true - mean_ret) ** 2)\n",
    "    r2_score = 1 - (sse / tse)\n",
    "    return r2_score\n",
    "\n",
    "\n",
    "R2_Campbell = R2_campbell(values.flatten(), y_pred.flatten(), mean_ret)\n",
    "\n",
    "print(\"R2 (Campbell): \", R2_Campbell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7sfBqxR9QYAP"
   },
   "source": [
    "Now, our $R^2_{OS}$ is actually a big improvement over the linear regression case. Indeed, these kind of numbers are in the neighborhood of what is published in leading finance journals in terms of return predictability.\n",
    "\n",
    "**Note that there is some random weight initialization (among other things) going on when training the model, so if you run this model without setting the seed you will get different numbers. This will also impact the trading strategy returns. Feel free to try it!**\n",
    "\n",
    "Anyway, predictability is not a super-high number, so let's see if we can make actual money (over the buy-and-hold case) by following a trading strategy based on our models' predictions.\n",
    "\n",
    "### **3.2. Backtesting Momentum Timing**\n",
    "\n",
    "As before, this section is inherently identical in construction to the one from Lesson 2, with the only changes coming from the predictions delivered by our MLP-based model. If you have doubts on how to perform some of the steps in this section, please go back to Lesson 2 of the module.\n",
    "\n",
    "- **What do predicted versus real returns in a test set look like?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1673884813165,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "nefqSYFmk_Ab",
    "outputId": "7ac061e3-3036-4fe5-a82b-6da65ed542e0"
   },
   "outputs": [],
   "source": [
    "df_predictions = pd.DataFrame(\n",
    "    {\n",
    "        \"Date\": test_time.flatten(),\n",
    "        \"Pred\": y_pred.flatten(),\n",
    "        \"Ret\": (Ret_vector.flatten() / 100),\n",
    "        \"Values\": values.flatten(),\n",
    "    }\n",
    ")\n",
    "df_predictions.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1673884813165,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "xHmRmLCgqxco",
    "outputId": "b102c74b-f4b3-4d91-970e-d10cc5febbc9"
   },
   "outputs": [],
   "source": [
    "df_predictions.Date = pd.to_datetime(df_predictions.Date, format=\"%YYYY-%mm-%dd\")\n",
    "df = df_predictions\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 810,
     "status": "ok",
     "timestamp": 1673884813969,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "7ICmEutqyuLd",
    "outputId": "d57d849c-fc65-4903-ecbc-4a0a6794b195"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = plt.gca()\n",
    "df.plot(x=\"Date\", y=\"Values\", color=\"red\", label=\"Real Stock Return\", ax=ax)\n",
    "df.plot(x=\"Date\", y=\"Pred\", color=\"blue\", label=\"Predicted Returns\", ax=ax)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Stock Return\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "pGICQHFoSBZ4"
   },
   "source": [
    "As you can see, the predicted returns are much less volatile (noisy) in the case of the MLP model than in the linear regression case. This has to do with the enhanced predictive power of MLPs. Let's now see whether this predictive power is enough to effectively time momentum returns.\n",
    "\n",
    "- **Momentum timing strategy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 7968,
     "status": "ok",
     "timestamp": 1673884821933,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "Zhqkz1A7mA-w",
    "outputId": "a8bf6756-0ced-4c8e-bb90-bb44013f09f0"
   },
   "outputs": [],
   "source": [
    "df[\"Positions\"] = df[\"Pred\"].apply(np.sign)\n",
    "df[\"Strat_ret\"] = df[\"Positions\"].shift(1) * df[\"Ret\"]\n",
    "df[\"Positions_L\"] = df[\"Positions\"].shift(1)\n",
    "df[\"Positions_L\"][df[\"Positions_L\"] == -1] = 0\n",
    "df[\"Strat_ret_L\"] = df[\"Positions_L\"] * df[\"Ret\"]\n",
    "df[\"CumRet\"] = df[\"Strat_ret\"].expanding().apply(lambda x: np.prod(1 + x) - 1)\n",
    "df[\"CumRet_L\"] = df[\"Strat_ret_L\"].expanding().apply(lambda x: np.prod(1 + x) - 1)\n",
    "df[\"bhRet\"] = df[\"Ret\"].expanding().apply(lambda x: np.prod(1 + x) - 1)\n",
    "\n",
    "Final_Return_L = np.prod(1 + df[\"Strat_ret_L\"]) - 1\n",
    "Final_Return = np.prod(1 + df[\"Strat_ret\"]) - 1\n",
    "Buy_Return = np.prod(1 + df[\"Ret\"]) - 1\n",
    "\n",
    "print(\"Strat Return Long Only =\", Final_Return_L * 100, \"%\")\n",
    "print(\"Strat Return =\", Final_Return * 100, \"%\")\n",
    "print(\"Buy and Hold Return =\", Buy_Return * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 922,
     "status": "ok",
     "timestamp": 1673884822837,
     "user": {
      "displayName": "Ivan Blanco",
      "userId": "11863287364861133555"
     },
     "user_tz": -60
    },
    "id": "auSoPaKJmEfq",
    "outputId": "a3845b58-2a5c-4e6e-d4eb-0b639cad64cc"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = plt.gca()\n",
    "df.plot(x=\"Date\", y=\"bhRet\", label=\"Buy&Hold\", ax=ax)\n",
    "df.plot(x=\"Date\", y=\"CumRet_L\", label=\"Strat Only Long\", ax=ax)\n",
    "df.plot(x=\"Date\", y=\"CumRet\", label=\"Strat Long/Short\", ax=ax)\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"Cumulative Returns\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "51j-zvdSTMVx"
   },
   "source": [
    "Sadly, as you can see, our strategy is not really able to time momentum factor returns. Arguably, our MLPs do a much better job than the linear regression case, but we still need much to be able to outperform the buy-and-hold strategy. This is not really surprising, since momentum has been one of the most profitable factors ever!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gmhCkl44892Q"
   },
   "source": [
    "## **4. Conclusion**\n",
    "\n",
    "\\\n",
    "Hopefully, you are not too disappointed by the failure of our trading strategies. Some of you may already be familiar with deep learning applications in other fields, where DL models achieve a remarkable performance and predictive power. Unfortunately, predicting the stock market is not as easy a task for an algorithm as recognizing and classifying pictures into groups of cats and dogs. \n",
    "\n",
    "Luckily for us, there is still a lot that can be done to improve the predictive performance of our algorithms in a financial market setting: hyperparameter tuning, more dense and complex networks, feeding data from other different sources, etc. We will deal with all these issues (and more) in the upcoming Deep Learning in Finance course.\n",
    "\n",
    "See you there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "Copyright 2024 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
