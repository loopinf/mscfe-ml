{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNipAqQOfhvn"
   },
   "source": [
    "\n",
    "## MACHINE LEARNING IN FINANCE\n",
    "MODULE 6 | LESSON 1\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mcm1WcLPSvU"
   },
   "source": [
    "# **INTRODUCTION TO DEEP LEARNING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPfzglOWOlus"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  70 minutes |\n",
    "|**Prior Knowledge** | Machine learning  |\n",
    "|**Keywords** |Deep learning, activation function, forward propagation, backpropagation |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFGA6RqAOyZp"
   },
   "source": [
    "*In this lesson, we are going to give a theoretical background of neural networks that will give you the intuition needed to master deep learning beyond the module. Once you are done with this lesson, you should be able to understand where deep learning is applicable, why it is needed, and the building blocks of neural networks. To ensure the accuracy provided by the neural networks is high, the concept of backpropagation will be introduced. The lesson will set the momentum for upcoming lessons that will heavily rely on this lesson.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPS8sUqdpZHp"
   },
   "source": [
    "\n",
    "## **1. Deep Learning**\n",
    "\n",
    "Deep learning is a field of machine learning that has been inspired by the functionality of the human brain neural networks (Zhang et al). In recent years, deep learning has attracted immense use cases due to its strong generalization capabilities and robust training power. When the data is huge, it has been shown that deep learning performs much better than classical machine learning algorithms.\n",
    "\n",
    "Deep learning has successfully been applied in both classification and regression problems in computer visions, natural language processing, audio-visual recognition, and image processing. In the recent past, deep learning applications have been researched and implemented in the fields of finance, medicine, operations research, and banking. The applications of deep learning in finance have been fueled by the existence of vast financial data, which happens to be incomplete, uncertain, and inconsistent, thereby posing a huge challenge to the traditional statistical methods that were previously employed to perform financial data analysis.\n",
    "\n",
    "Applications of deep learning in the financial world include:\n",
    "- Credit risk modeling (Addo et al. 38; Modarres et al.)\n",
    "- Forecasting of option prices, future prices, and share prices (Sezer et al.; Yao et al. 455-466)\n",
    "- Predicting bankruptcy (Mai et al. 743-758; Qu et al. 895-899)\n",
    "\n",
    "The advancement of deep learning and machine learning in the areas of finance has largely been facilitated by:\n",
    "- Evolution of computing power.\n",
    "- Advancement of mathematical theory by machine learning researchers.\n",
    "- Accessibility of technology.\n",
    "- Breakthrough achieved in other domains of deep learning such as in games, natural language processing, image processing, etc.\n",
    "\n",
    "In financial engineering, the following problems can be solved using deep learning algorithms:\n",
    "- Deep hedging (Buehler et al. 1271-1291)\n",
    "- Deep calibration (Nixon et al.)\n",
    "- Deep simulation. (Maeda et al. 71)\n",
    "\n",
    "Now that we have introduced deep learning and its application areas, in this lesson, we will learn about the following topics:\n",
    "\n",
    "1. Deep learning\n",
    "2. Forward propagation\n",
    "3. Backward propagation\n",
    "\n",
    "\n",
    "Deep learning derives its name from artificial neural networks (ANN) having many layers where each layer is designed to learn the intricate patterns found in the data. As pointed out earlier, it is nowadays common to build neural networks with thousands of layers, which has been made possible by the advancement in computational power. Because of this fact where ANN tend to use deep layers to learn, the name deep learning was adopted for such algorithms (Zhang et al).\n",
    "\n",
    "We started by saying that deep learning is a branch of machine learning. The question we might then ask ourselves is, what differentiates the two?\n",
    "\n",
    "Whilst the performance of classical machine learning algorithms is highly dependent on the correct choice of features and feature engineering techniques applied to the dataset, a task that is no mean feat, in deep learning, this is well taken care of by the layers that have the capability to learn complex relationships of the data. When our dataset is small, the deep learning algorithms are not preferred as they tend to overfit on the dataset and thus provide bad generalization. In such cases, machine learning performs much better than deep learning models.\n",
    "\n",
    "Having discussed the definition of deep learning and where it is used, let us now learn the building blocks of deep learning algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WMefWFugmAI"
   },
   "source": [
    "### **1.1 Neurons**\n",
    "\n",
    "A neuron forms the fundamental computational unit of artificial neural networks. Below we examine the functionality of artificial neural networks, let us assume that the predictor variables are $x_1$, $x_2$, $x_3$ and $x_4$ and we are predicting $y$.\n",
    "\n",
    "Since the variables will usually not be equally important in predicting the output, we assign weight multipliers to all the inputs to signify importance. Therefore, summing the inputs that have been multiplied by the weights will yield\n",
    "$$x_1 \\times w_1 + x_2 \\times w_2 + x_3 \\times w_3 + x_4 \\times w_4$$\n",
    "By adding a bias term to the above equation we get\n",
    "$$z = (x_1 \\times w_1 + x_2 \\times w_2 + x_3 \\times w_3 + x_4 \\times w_4) + b$$\n",
    "\n",
    "The above equation resembles that of a linear regression. The difference between a neuron and a linear regression is the introduction of a non-linear function called an activation function that is applied to $z$ to predict the output,that is, $$y = f(z)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "id": "1eoD4ZpTpZHs",
    "outputId": "6167c518-25d5-4bca-de6c-0638e28792b6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nnv import NNV\n",
    "\n",
    "print(\"\\033[1m\" + \"Fig 1: A Neural Network\" + \"\\033[0m\")\n",
    "layersList = [\n",
    "    {\"title\": \"inputs \\n + bias\", \"units\": 5, \"color\": \"darkBlue\"},\n",
    "    {\"title\": \"$z$\", \"units\": 1},\n",
    "    {\"title\": \"$f(z)$\", \"units\": 1, \"edges_color\": \"red\", \"edges_width\": 2},\n",
    "    {\"title\": \"output\", \"units\": 1, \"color\": \"darkBlue\"},\n",
    "]\n",
    "\n",
    "NNV(layersList).render(save_to_file=\"my_example1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWpao3XIpZHt"
   },
   "source": [
    "So, in summary, given inputs $X$, a neuron will pair them with weights $W$ and a bias term $b$, which after adding them will be acted upon by an activation function to yield the output $y$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O47ewc8GgffC"
   },
   "source": [
    "### **1.2 Neural Network Layers**\n",
    "\n",
    "Nearly all tasks we will face in the industry will need us to stack more layers to learn the intricate relationship present in the data. The organization of layers in a neural network is such that it can allow information to flow from one layer to the next. The layers in a simple neural network are:\n",
    "- Input layer\n",
    "- Hidden layer\n",
    "- Output layer\n",
    "\n",
    "This can be shown in the diagram below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "ay37ulqKpZHt",
    "outputId": "42d4bd9a-2431-45ce-86e0-7ec1f9bb2e74"
   },
   "outputs": [],
   "source": [
    "print(\"\\033[1m\" + \"Fig 2: Two Layer Neural Network\" + \"\\033[0m\")\n",
    "layersList = [\n",
    "    {\"title\": \"input layer\", \"units\": 3, \"color\": \"darkBlue\"},\n",
    "    {\"title\": \"hidden layer\", \"units\": 4},\n",
    "    {\"title\": \"output layer\", \"units\": 1, \"color\": \"darkBlue\"},\n",
    "]\n",
    "\n",
    "NNV(layersList).render(save_to_file=\"my_example2.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQdS5RihpZHu"
   },
   "source": [
    "The input layer consists of the observed variables that are passed to the model. They can be raw and unprocessed data straight from the data source or featured engineered datasets.\n",
    "\n",
    "The hidden layer is the layer that processes data passed from the input layer to derive complex patterns present in the data. There can be thousands of hidden layers present in a neural network. The processed data is then passed to the output layer.\n",
    "\n",
    "The output layer gives the output of the modeling exercise. There can be more than one neuron in the output layer depending on the problem being solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1gF_I44gZxb"
   },
   "source": [
    "## **1.3 Activation Functions**\n",
    "\n",
    "As pointed out earlier, activation functions are used to introduce non-linearity to the input so as to learn the underlying relationship in the data, thus allowing for complex fitting. Without a non-linear transformation, the neural network is a linear regression model. \n",
    "\n",
    "We will now explore the most commonly used activation functions.\n",
    "\n",
    "### **1.3.1 Sigmoid Function (Logistic Function)**\n",
    "The sigmoid function is defined as $$f(x) = \\frac{1}{1+e^{-x}}$$\n",
    "It gives an $S$-shaped curve as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "uNoNd3sCpZHu",
    "outputId": "d58d85df-1671-4691-96f3-fd243cab6b6d"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    a = []\n",
    "    for item in x:\n",
    "        a.append(1 / (1 + math.exp(-item)))\n",
    "    return a\n",
    "\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.2)\n",
    "sig = sigmoid(x)\n",
    "\n",
    "print(\"\\033[1m\" + \"Fig 3: A sigmoid function curve\" + \"\\033[0m\")\n",
    "plt.plot(x, sig)\n",
    "# `plt.suptitle`(\"Fig. 3: A sigmoid function curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qfGz8_npZHu"
   },
   "source": [
    "The sigmoid function is differentiable and monotonic and it outputs values between $0$ and $1$. We can write the sigmoid function in Python as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nhq8Q3vipZHu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bL6ISpCDpZHv"
   },
   "source": [
    "#### **1.3.2 Tanh Function**\n",
    "The tanh function expressed as $$f(x) = \\frac{1 - e^{-2x}}{1+e^{-2x}}$$ resembles the sigmoid function above with the difference being that it outputs values between $-1$ and $1$ as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "kzx0wLb6pZHv",
    "outputId": "c5c7262c-3beb-42bd-ae46-50893456c863"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    a = []\n",
    "    for item in x:\n",
    "        a.append((1 - math.exp(-2 * item)) / (1 + math.exp(-2 * item)))\n",
    "    return a\n",
    "\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.2)\n",
    "tanva = tanh(x)\n",
    "\n",
    "print(\"\\033[1m\" + \"Fig 4: A tanh function curve\" + \"\\033[0m\")\n",
    "plt.plot(x, tanva)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bt69yvDXpZHv"
   },
   "source": [
    "It can be implemented in Python as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XyQDVxjpZHv"
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return (1 - np.exp(-2 * x)) / (1 + np.exp(-2 * x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77opZHvSpZHw"
   },
   "source": [
    "#### **1.3.3 Rectified Linear Unit (ReLU) Function**\n",
    "The ReLU function is mathematically expressed as\n",
    "$$\n",
    "f(x) = \\begin{cases} \n",
    "      0 & \\text{for }x< 0 \\\\\n",
    "      x & \\text{for }x\\geq 0\n",
    " \\end{cases}\n",
    " $$\n",
    " or $$f(x) = \\max\\{0, x\\}$$\n",
    " whose outputs are non-negative numbers.\n",
    " \n",
    " It can pictorially be visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "DmWcLfz0pZHw",
    "outputId": "7a2960d2-2d58-4127-b181-3e6e971cb52f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "y = np.maximum(0, x)\n",
    "\n",
    "print(\"\\033[1m\" + \"Fig 5: A ReLU function curve\" + \"\\033[0m\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x, y)\n",
    "plt.legend([\"Relu\"])\n",
    "# `plt.suptitle`(\"Fig. 5: A ReLU function curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EiForp1pZHw"
   },
   "source": [
    "The Python code for ReLU is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5MnZqWjapZHw"
   },
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    if x < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8WMy4HepZHw"
   },
   "source": [
    "#### **1.3.4 The Leakey ReLU Function**\n",
    "The Leakey ReLU is a modification of the ReLU function intended to tackle the dying ReLU problem. It is mathematically expressed as:\n",
    "$$\n",
    "f(x) = \\begin{cases} \n",
    "      \\alpha x & \\text{for }x< 0 \\\\\n",
    "      x & \\text{for }x\\geq 0\n",
    " \\end{cases}\n",
    " $$\n",
    " where $\\alpha$ usually takes the value $0.01$.\n",
    " \n",
    " It is implemented in Python as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1igO-sOfpZHw"
   },
   "outputs": [],
   "source": [
    "def LeakeyReLU(x, alpha=0.01):\n",
    "    if x < 0:\n",
    "        return alpha * x\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLq43XWnpZHx"
   },
   "source": [
    "Diagrammatically, we can visualize it as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "NwBLa0JTpZHx",
    "outputId": "46c083b1-87f8-4298-cbc9-114dfe6733b3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Leaky Rectified Linear Unit (leaky ReLU) Activation Function\n",
    "def leaky_ReLU(x):\n",
    "    data = [max(0.05 * value, value) for value in x]\n",
    "    return np.array(data, dtype=float)\n",
    "\n",
    "\n",
    "# Generating data For Graph\n",
    "x_data = np.linspace(-10, 10, 100)\n",
    "y_data = leaky_ReLU(x_data)\n",
    "\n",
    "# Graph\n",
    "print(\"\\033[1m\" + \"Fig 6: A Leakey ReLU function curve\" + \"\\033[0m\")\n",
    "plt.plot(x_data, y_data)\n",
    "plt.legend([\"leaky_ReLU\"])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_hhzWuApZHx"
   },
   "source": [
    "#### **1.3.5 The Softmax Function**\n",
    "The softmax function generalizes the sigmoid function, and it is used in multi-class classification problems while logistic function is suitable for binary classes. It is expressed as:\n",
    "$$f(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$ The Python code to produce the above equation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_hjMZNlpZHx"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0A0BdyJupZHx"
   },
   "source": [
    "Other commonly used activation functions are:\n",
    "\n",
    "a) The exponential linear unit function given by \n",
    "$$\n",
    "f(x) = \\begin{cases} \n",
    "      \\alpha (e^{x}-1) & \\text{for }x< 0 \\\\\n",
    "      x & \\text{for }x\\geq 0\n",
    " \\end{cases}\n",
    " $$\n",
    " b) The swish function given by $$f(x) = x\\sigma(x)$$ where $\\sigma(x)$ is the sigmoid function.\n",
    " \n",
    " Now that we have learned the founding blocks of deep learning, let us now apply them in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEAYiP5qpZHx"
   },
   "source": [
    "## **2. Forward Propagation**\n",
    "\n",
    "In the previous section, we said that neural networks learn by stacking neurons in layers. A neural network is said to have $n$-layers if the number of layers excluding the input layer are $n$. Therefore, a two-layer neural network will have one input layer $X$, one hidden layer $h$, and one output layer $y$ as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "BWgVSYYUpZHx",
    "outputId": "3453146c-f691-41ed-b368-04808ca8b223"
   },
   "outputs": [],
   "source": [
    "print(\"\\033[1m\" + \"Fig 7: A Two Layer Neural Network\" + \"\\033[0m\")\n",
    "layersList = [\n",
    "    {\"title\": \"input layer\", \"units\": 3, \"color\": \"darkBlue\"},\n",
    "    {\"title\": \"hidden layer\", \"units\": 4},\n",
    "    {\"title\": \"output layer\", \"units\": 1, \"color\": \"darkBlue\"},\n",
    "]\n",
    "\n",
    "NNV(layersList).render(save_to_file=\"my_example3.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYjCmJMkpZHx"
   },
   "source": [
    "Take a case where our input variables are three, that is, $x_1$, $x_2$ and $x_3$ as in the diagram above. The number of neurons in the input layer will therefore be three, and in this example, we will also assume the number of neurons in the hidden layer and output layer to be four and one respectively. As earlier indicated, the input variables will be multiplied by their corresponding weights, then we add a bias term before passing this sum to the hidden layer where the activation function will be used.\n",
    "\n",
    "The obvious question we might be asking ourselves is given the inputs, where do we get the corresponding weights? Practically, it will be impossible to know which of the inputs will be more important in prediction and by what proportion. We therefore assign weights and the bias term using random numbers that we will denote as $W_{xh}$ and $b_h$ respectively, where the dimensions of the weight matrix will have a number of columns equal to the number of neurons in the input layer and a number of rows equal to the number of neurons in the hidden layer so as to ensure matrix multiplication compatibility. Thus, in the presented example, the dimension of $W_{xh}$ is $3\\times 4$ and therefore $$z_1 = X W_{xh} + b_h.$$\n",
    "We pass this equation to the hidden layer where it is acted upon by the activation function. If the activation function in this case is the sigmoid function, then we have $$a_1 = \\sigma (z_1).$$\n",
    "The outputs $a_1$ is again multiplied by a new weight matrix and added to a new bias term that is now passed to the output layer. By denoting the weight matrix and bias term here by $W_{hy}$ and $b_y$ respectively, the dimensions of $W_{hy}$ will be $4\\times 1$ since the hidden layer has $4$ neurons and the output layer has one neuron.\n",
    "\n",
    "The result is given by $$z_2 = a_1 W_{hy} + b_y.$$\n",
    "\n",
    "Finally, we apply another activation function to $z_2$ to get the output value $$\\hat{y} = \\sigma(z_2).$$ The above process is what we call the forward propagation. \n",
    "\n",
    "In summary, the mathematical steps involved in a two-layer neural network is as follows:\n",
    "$$\n",
    "\\begin{cases}\n",
    "z_1 &= X W_{xh} + b_h\\\\\n",
    "a_1 &= \\sigma(z_1) \\\\\n",
    "z_2 &= a_1 W_{hy} + b_y \\\\\n",
    "\\hat{y} &= \\sigma(z_2)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "A Python function implementation of the above steps is as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v28E1KkGpZHx"
   },
   "outputs": [],
   "source": [
    "def forward_prop(X, b_h, b_y, W_x_h, W_h_y):\n",
    "    z_1 = np.dot(X, W_x_h) + b_h\n",
    "    a_1 = sigmoid(z_1)\n",
    "    z_2 = np.dot(a_1, W_h_y) + b_y\n",
    "    y_hat = sigmoid(z_2)\n",
    "\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcR3scyNpZHx"
   },
   "source": [
    "Are we done? Apparently not. We have to compare our approximated output with the actual output. How do we therefore know that our model is performing up to expectation? We define a cost/loss function. Just as their are many activation functions, there exist many cost functions. To build the intuition in the next section, we will use the cost function below\n",
    "$$J = \\frac{1}{n} \\sum_{n=1}^n (y_i - \\hat{y})^2$$\n",
    "the sum of the squared difference between the actual output and the predicted output where $n$ is the number of variables used in training.\n",
    "\n",
    "Now that we have learned how to predict the output of a neural network, in the next section, we are going to fine-tune this to get better approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iw3Rh57MpZHy"
   },
   "source": [
    "## **3. Backpropagation**\n",
    "\n",
    "At this time, the question that is lingering in our heads might be how does a neural network learn to predict the correct output? Where is the cost function coming in in all of this? A short answer to this is that we aim to have the cost function as small as possible, and therefore for any iteration we will encounter, we will seek to minimize the cost function. But why? If the cost function is large, then it means that the difference between the predicted and actual outputs is high.\n",
    "\n",
    "The only way we can minimize the cost function is to change some parameters, but we cannot change the input and output values. Therefore, we can only change the weight and bias terms since they were initially assigned random numbers. The role of backpropagation will be to help the neural network learn how to update the weights in such a manner so as to minimize the cost function thereby increasing the accuracy of our prediction. We use a method called gradient descent to update the weights.\n",
    "\n",
    "Gradient descent is an optimization algorithm commonly used in machine learning to find the optimal values of weights that will minimize the cost function or errors.\n",
    "\n",
    "To understand how gradient descent finds the optimal weights, let us provide an intuitive explanation. From its name \"Gradient descent,\" think of going down a slope. The technique tries to find a combination of parameters in a curve that will ensure the output gives a low-cost value.\n",
    "\n",
    "Since our interest is to find the optimal weight, consider the curve below as a function of $w$, that is, $E(w)$ as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "MvIbyW77pZHy",
    "outputId": "0d6bd62e-7a08-488d-e81d-fdf3fa96c704"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "w = np.linspace(-3, 3, num=100)\n",
    "\n",
    "Ew = []\n",
    "for i in range(len(w)):\n",
    "    Ew.append(w[i] ** 4 - 5 * w[i] ** 2 - 3 * w[i])\n",
    "\n",
    "print(\"\\033[1m\" + \"Fig 8: A Non-convex Curve\" + \"\\033[0m\")\n",
    "plt.plot(w, Ew)\n",
    "plt.grid()\n",
    "plt.axvline()\n",
    "plt.axhline()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaOC4KF6pZHy"
   },
   "source": [
    "Our ideal will be to hit the global minimum, but gradient (the lowest trough in the curve) descent will not necessarily get to this point, though it will get close. We start first by selecting initial values of $w$. The gradient descent will attempt to reach the local minimum point, and since it will only descend and not climb, by increasing or decreasing the value of $w$ gradually in small steps, we will see it go down the slope toward the global minimum. We refer to these steps as the learning rate denoted by $\\alpha$.\n",
    "\n",
    "By repeating this process over many iterations, the gradient descent will give us optimal weights that ensure the cost is at a minimum. Remember that by forward propagation we stopped at the output layer. By back-propagation, we will move in the reverse order, that is, from the output layer to the input layer while calculating the cost function's gradient that minimizes the error. Having calculated the gradients, we can now proceed to update the weights as follows: $$W_{new} = W - \\alpha \\frac{\\partial J}{\\partial W}$$\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "The learning rate guides the type of step to be taken, that is, a small learning rate infers we take a small step down the slope and therefore the gradient descent process becomes slow. The inverse is also true as shown in the diagrams below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoweGWBBpZHy"
   },
   "source": [
    "**Fig. 9**: Gradient Descent with Different Learning Rates\n",
    "\n",
    "<img src=\"https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png\" alt=\"Gradient Descent with Large steps\" width=\"1000\"/> \n",
    "\n",
    "##### Source: [Jeremy Jordan, Setting the Learning Rate of Your Neural Network, 2018](https://www.jeremyjordan.me/nn-learning-rate/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5o0S9YX7pZHy"
   },
   "source": [
    "This whole procedure of moving from the output layer to the input layer while updating the weights using an optimization algorithm is called back-propagation.\n",
    "\n",
    "In the two-layer neural network example we considered in the last section, we had two weights, $W_{xh}$ that feeds inputs to hidden layer and $W_{hy}$ that feeds to the output layer from the hidden layer. To find the optimal weights that gives us minimal errors, we will find the derivative of $J$ with respect to the weights.\n",
    "\n",
    "We will start with $W_{hy}$ since we are moving backwards, from the output layer towards the input layer. Recall that cost function $J$ is given by:\n",
    "$$J = \\frac{1}{n}\\sum_{i = 1}^{n} (y_i - \\hat{y})^2$$\n",
    "and $\\hat{y} = \\sigma(z_2)$ where $z_2 = a_1 W_{hy} + b_y$. To find the derivative of $J$ with respect to $W_{hy}$ we will use the chain rule, that is,\n",
    "$$\\frac{\\partial J}{\\partial W_{hy}} = \\frac{\\partial J}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial z_2} \\times \\frac{\\partial z_2}{\\partial W_{hy}} \\hspace{10mm} (1)$$\n",
    "\n",
    "Note that:\n",
    "$$\\frac{\\partial J}{\\partial \\hat{y}} = (y - \\hat{y})$$\n",
    "$$\\frac{\\partial \\hat{y}}{\\partial z_2} = \\sigma ' (z_2)$$\n",
    "where $\\sigma '$ denotes the derivative of the sigmoid activation function.\n",
    "\n",
    "The derivative of the sigmoid function $\\sigma (z) = \\frac{1}{1 + e^{-z}}$ is\n",
    "$$\\sigma ' (z) = \\frac{e^{-z}}{(1+e^{-z})^2}$$\n",
    "Finally, $$\\frac{\\partial z_2}{\\partial W_{hy}} = a_1$$\n",
    "Combining the derivatives back to the equation (1) above, we get\n",
    "$$\\frac{\\partial J}{\\partial W_{hy}} = (y - \\hat{y}) \\times \\sigma ' (z_2) \\times a_1 \\hspace{10mm} (2)$$\n",
    "\n",
    "Next, we now find the derivative of $J$ with respect to the weight $W_{xh}$. Recall that:\n",
    "$$\\hat{y} = \\sigma(z_2)$$\n",
    "$$z_2 = a_1 W_{hy} + b_y$$\n",
    "$$a_1 = \\sigma(z_1)$$\n",
    "$$z_1 = X W_{xh} + b$$\n",
    "By the chain rule, we get\n",
    "$$\\frac{\\partial J}{\\partial W_{xh}} = \\frac{\\partial J}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial z_2} \\times \\frac{\\partial z_2}{\\partial a_1} \\times \\frac{\\partial a_1}{\\partial z_1} \\times \\frac{\\partial z_1}{\\partial W_{xh}}\\hspace{10mm} (3)$$\n",
    "where:\n",
    "$$\\frac{\\partial z_2}{\\partial a_1} = W_{hy}$$\n",
    "$$\\frac{\\partial a_1}{\\partial z_1} = \\sigma ' (z_1)$$\n",
    "$$\\frac{\\partial z_1}{\\partial W_{xh}} = X$$\n",
    "\n",
    "Therefore:\n",
    "$$\\frac{\\partial J}{\\partial W_{xh}} = (y - \\hat{y}) \\times \\sigma ' (z_2) \\times W_{hy} \\times \\sigma ' (z_1) \\times X \\hspace{10mm} (4)$$\n",
    "\n",
    "Since we now have the gradients, we can now update the weights as:\n",
    "$$W_{hy} = W_{hy} - \\alpha \\frac{\\partial J}{\\partial W_{hy}} \\hspace{10mm} (5)$$\n",
    "$$W_{xh} = W_{xh} - \\alpha \\frac{\\partial J}{\\partial W_{xh}} \\hspace{10mm} (6)$$\n",
    "\n",
    "Let us now implement backpropagation in Python. We will define $d Why$ and $d Whx$ to represent $\\frac{\\partial J}{\\partial W_{hy}}$ and $\\frac{\\partial J}{\\partial W_{xh}}$ respectively.\n",
    "\n",
    "We first write a function for the derivative of the sigmoid function.<span style='color: transparent; font-size:1%'>All rights reserved WQU WorldQuant University QQQQ</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glFdkG04pZHy"
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(z):\n",
    "    return np.exp(-z) / ((1 + np.exp(-z)) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExIPBYJ_pZHy"
   },
   "source": [
    "Define $(y - \\hat{y}) \\sigma ' (z_2)$ as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvpjzarEpZHy"
   },
   "source": [
    "`delta2 = np.multiply((y_hat - y)`, `sigmoid_derivative(z_2))`\n",
    "\n",
    "The gradient with respect to $W_{hy}$ is therefore:\n",
    "\n",
    "`dWhy = np.dot(a1.T, delta2)`\n",
    "\n",
    "and with respect to $W_{xh}$ is:\n",
    "\n",
    "`delta1 = np.dot(delta2, Why.T) * sigmoid_derivative(z1)`\n",
    "\n",
    "`dWxh = np.dot(X.T, delta1)`\n",
    "\n",
    "We therefore update the weights accordingly:\n",
    "\n",
    "`Wxh = Wxh - alpha * dWxh`\n",
    "\n",
    "`Why = Why - alpha * dWhy`\n",
    "\n",
    "So, the function becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ooswJAhEpZHy"
   },
   "outputs": [],
   "source": [
    "def backward_prop(y_hat, z_1, a_1, z_2, alpha, W_x_h, W_h_y, X):\n",
    "    delta2 = np.multiply((y_hat - y), sigmoid_derivative(z_2))\n",
    "    d_W_h_y = np.dot(a_1.T, delta2)\n",
    "\n",
    "    delta1 = np.dot(delta2, W_h_y.T) * sigmoid_derivative(z_1)\n",
    "    d_W_x_h = np.dot(X.T, delta1)\n",
    "\n",
    "    W_x_h = W_x_h - alpha * d_W_x_h\n",
    "    W_h_y = W_h_y - alpha * d_W_h_y\n",
    "\n",
    "    return W_x_h, W_h_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtrygoeEPIhB"
   },
   "source": [
    "## **4. Conclusion**\n",
    "\n",
    "In this lesson, we started by introducing the concept of deep learning and its application areas in financial engineering. We started by discussing the fundamental building blocks of deep learning, after which we discussed how neural networks get the output predictions using forward prediction. Then, we fine-tuned the algorithm by introducing the concept of gradient descent in back-propagation.\n",
    "\n",
    "In the next lesson, we will put these concepts together as we work on examples and compare the performance of deep learning and other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmeoDucgpZHy"
   },
   "source": [
    "\n",
    "**References**\n",
    "\n",
    "1. Addo, Martey Peter et al. \"Credit Risk Analysis Using Machine and Deep Learning Models.\" *Risks*, vol. 6, no. 2, 2018, pp. 1-20.\n",
    "\n",
    "2. Buehler, Hans et al. \"Deep Hedging.\" *Quantitative Finance*, vol. 19, no. 8. 2019, pp. 1271-1291.\n",
    "\n",
    "3. Maeda, Iwao et al. \"Deep Reinforcement Learning in Agent Based Financial Market Simulation.\" *Journal of Risk and Financial Management*, vol. 13, no. 4, 2020.\n",
    "\n",
    "4. Mai, Feng, et al. \"Deep Learning Models for Bankruptcy Prediction Using Textual Disclosures.\" *European Journal of Operational Research*, vol. 274, no. 2, 2019, pp. 743-758.\n",
    "\n",
    "5. Modarres, Ceena et al. \"Towards Explainable Deep Learning for Credit Lending: A Case Study.\" *arXiv* preprint, arXiv:1811.06471, 2018.\n",
    "\n",
    "6. Nixon, Jeremy et al. \"Measuring Calibration in Deep Learning.\" *CVPR Workshops*, vol. 2, no. 7, 2019.\n",
    "\n",
    "7. Sezer, Omer Berat et al. \"Financial Time Series Forecasting with Deep Learning: A Systematic Literature Review: 2005-2019.\" *Applied Soft Computing*, vol. 90, 2020, 1-63.\n",
    "\n",
    "8. Yao, Jingtao et al. \"Option Price Forecasting Using Neural Networks.\" *Omega*, vol. 28, no. 4, 2000, pp. 455-466.\n",
    "\n",
    "9. Qu, Yi et al. \"Review of Bankruptcy Prediction Using Machine Learning and Deep Learning Techniques.\"* Procedia Computer Science*, vol. 162, 2019, pp. 895-899.\n",
    "\n",
    "10. Zhang, Aston et al. \"Dive into Deep Learning.\" *arXiv* preprint arXiv:2106.11342, 2021. https://d2l.ai/d2l-en.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "Copyright 2023 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
