{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_rwYPo6fW5B"
   },
   "source": [
    "\n",
    "## MACHINE LEARNING IN FINANCE\n",
    "MODULE 6 | LESSON 4\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcUcqGe8Tne0"
   },
   "source": [
    "# **CREDIT RISK MODELING: APPLICATION OF DEEP LEARNING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JW7QxDPfTrAJ"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  120 minutes |\n",
    "|**Prior Knowledge** | Introduction to machine learning, introduction to deep learning  |\n",
    "|**Keywords** |Expected Loss, credit risk, SMOTE |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmcPkR9gjRgl"
   },
   "source": [
    "*In the previous lessons of this module, we have built on the theory of deep learning. In this lesson, we will discuss one of the hottest topics in machine learning, that is, predicting loan default. We will use classical machine learning algorithms on a refined dataset and discuss the steps used to predict mortgage default.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGQnIx1_lG-R"
   },
   "source": [
    "## **1. Mortgage Delinquency**\n",
    "\n",
    "There has been an evolution in the business of lending money as the process has become increasingly complex due to the growing market demands and clients' increasing appetite for credit. These factors among others have led to an increase in regulation and oversight in the banking industry so as to make sure they act responsibly when issuing loans. In the recent past, the rate of digitalization globally has shot up with people in remote parts of the world having access to phones. This has made it possible for people to use mobile devices as a financial medium through which they can send and receive money to and from other people around the world. These transactions happen in a matter of seconds. Many fintechs have taken advantage of this to launch microloans to customers who are low risk. The fintechs use the interaction of the customers with their gadgets to build a credit score for each of the customers and determine the probability of the customer defaulting on a loan. This logic also applies to mortgages and the probability of mortgage default is called mortgage delinquency. Machine learning models are trained on the data we have processed and the decision making process of giving loans is automated.\n",
    "\n",
    "The machine learning models are used to assess the creditworthiness of a borrower. Before the advent of machine learning, lenders had an established guideline to measure creditworthiness. These guidelines were based on the five C's listed below:\n",
    "\n",
    "1. Character that looks at the borrower's repayment and credit record.\n",
    "2. Capacity that assess the borrower's ability to service the loan by looking at the debt-to-income ratio.\n",
    "3. Capital that looks at the down payment the borrower has paid. This is used to determine how serious the borrower is.\n",
    "4. Collateral, which is the asset provided to secure the mortgage, such as another home.\n",
    "5. Conditions of the borrower's environment, like the state of the economy.\n",
    "\n",
    "However, this has posed serious challenges to lenders as the number of features are limited in assessing customers' creditworthiness, with potentially credit-worthy clients being denied credit for failing certain criteria, and their inability to keep pace with the technological evolution that has been witnessed in the past decade.\n",
    "\n",
    "It is because of these limitations that machine learning models are now at the heart of assessing the creditworthiness of borrowers. However, recent research has shown that deep learning has the potential to eclipse machine learning for assessing credit risks. Deep neural networks are great at detecting risky borrowers when the data is unstructured and very complex.\n",
    "\n",
    "However, the risk of using deep learning models is that in most cases they are not explainable, that is, they are like black boxes and we are in most cases unable to know what happened for us to get a certain output. Currently, a lot of research is being done to address this by focusing on explainable artificial intelligence. In the next section, we are going to show a simple example of using a classical machine learning algorithm to assess credit risk. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxkK6UiEwEHh"
   },
   "source": [
    "## **2.Credit Risk**\n",
    "\n",
    "### **2.1 Data Sources**\n",
    "\n",
    "We downloaded our data from the [UCI marketing](https://archive.ics.uci.edu/ml/datasets/bank+marketing) dataset, and we did some engineering and cleaning to make the data ready for model training.\n",
    "\n",
    "In the cell below, we import packages we will use in this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zacu_I9muh4z"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# EDA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from imblearn.combine import SMOTETomek\n",
    "from scipy import stats\n",
    "\n",
    "# feature selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    brier_score_loss,\n",
    "    classification_report,\n",
    "    cohen_kappa_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0O1TcYq3Q2q"
   },
   "source": [
    "### **2.2 Loading the Dataset**\n",
    "\n",
    "We start by uploading this downloaded data and taking a rough look at the data. In this problem, we will skip the process of performing EDA and data preprocessing techniques as the data is already processed.\n",
    "\n",
    "- Person Age, Person Income, Person Employment Length, Loan Amount, Loan interest rate, Loan percent income and Person credit history length were transformed using min max scaler, that is, converting the values to range between 0 and 1.\n",
    "- The other columns were categorical and we therefore converted them to numerical data by one hot encoding the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gi1y-c9bu40o"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"credit_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "3XlFekLlv6ig",
    "outputId": "f1bec5e0-b00d-4acd-9b3b-f66e900a478f"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lrirzw0Bv8Lb",
    "outputId": "0ccef769-99d3-46c8-a60c-84ccd1c07e9f"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-15jksHMAPJ",
    "outputId": "b4f7535b-3452-4ed9-8637-c45e7fecebd2"
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ckymAGvrPC0v",
    "outputId": "3c78ae60-bce8-4ace-cbed-42106c0e4b29"
   },
   "outputs": [],
   "source": [
    "df.loan_status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kyLkiaA5Obf"
   },
   "source": [
    "Clearly, the data are highly imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "1EwHVM6q5k1P",
    "outputId": "6d3a78e6-f9f3-4c91-d916-43001cc778c4"
   },
   "outputs": [],
   "source": [
    "# plots count\n",
    "ax = sns.countplot(x=df[\"loan_status\"])\n",
    "\n",
    "# sets the figure size in inches\n",
    "ax.figure.set_size_inches(12, 6)\n",
    "\n",
    "# set plot features\n",
    "ax.set_title(\"Count for Loan Status\", fontsize=16)\n",
    "ax.set_ylabel(\"Count\", fontsize=14)\n",
    "ax.set_xlabel(\"Loan status\", fontsize=14)\n",
    "\n",
    "# set `xticks` labels\n",
    "plt.xticks([0, 1], [\"Non-Default\", \"Default\"])\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Fig. 1: Top: A bar plot of good versus bad loans. The graph shows that the dataset is highly imbalanced.\",\n",
    "    fontweight=\"bold\",\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "\n",
    "# displays plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AcfC6r6E56P7",
    "outputId": "f0bbcff3-f114-46e1-96dd-99bda2eb080e"
   },
   "outputs": [],
   "source": [
    "# separating the data set for easier analysis\n",
    "df_default = df[df[\"loan_status\"] == 1].copy()\n",
    "df_non_default = df[df[\"loan_status\"] == 0].copy()\n",
    "\n",
    "# counts the number of defaults and non-defaults\n",
    "total_default = df_default.shape[0]\n",
    "total_non_default = df_non_default.shape[0]\n",
    "total_loans = df.shape[0]\n",
    "\n",
    "print(\"Number of default cases:\", total_default)\n",
    "print(\n",
    "    \"This is equivalent to {:.2f}% of the total loans\".format(\n",
    "        (total_default / total_loans) * 100\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nNumber of non-defualt cases:\", total_non_default)\n",
    "print(\n",
    "    \"This is equivalent to {:.2f}% of the total loans\".format(\n",
    "        (total_non_default / total_loans) * 100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQPN675YQ0FI"
   },
   "source": [
    "### **2.3 Split Dataset into Train and Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCiNZ4oOQ6gP"
   },
   "outputs": [],
   "source": [
    "# creates the X and y data sets\n",
    "X = df.drop(\"loan_status\", axis=1).values\n",
    "y = df[\"loan_status\"].values\n",
    "\n",
    "# splits into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=2022, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slMI0NuuSAOz"
   },
   "source": [
    "### **2.4 Balancing Data for Training**\n",
    "\n",
    "Since we saw that the data was highly imbalanced, we performed oversampling with synthetic minority over-sampling technique (SMOTE) with the package below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6lpV5HcsRdtn",
    "outputId": "c97951ec-bfc7-43d9-e0df-95adfe550def"
   },
   "outputs": [],
   "source": [
    "# counts the number of classes before oversampling\n",
    "print(\"Before balancing:\", Counter(y_train))\n",
    "\n",
    "# defines the resampler\n",
    "resampler = SMOTETomek(random_state=2022, n_jobs=-1)\n",
    "\n",
    "# transforms the data set\n",
    "X_balanced, y_balanced = resampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# counts the number of classes after oversampling\n",
    "print(\"After balancing:\", Counter(y_balanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "aB62tUf_SMME",
    "outputId": "a6f0ebef-7ac4-4924-d024-ef9d6e80fe3b"
   },
   "outputs": [],
   "source": [
    "# plots before and after SMOTE\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x=y_train)\n",
    "plt.title(\"Before SMOTE\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x=y_balanced)\n",
    "plt.title(\"After SMOTE\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Fig. 2: Top: A plot of the defaulters versus non-defaulters before resampling and after resampling\",\n",
    "    fontweight=\"bold\",\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7dEbTTYFSpEY",
    "outputId": "3bb592ab-ee7f-4cfd-fa42-05ff3d8e383e"
   },
   "outputs": [],
   "source": [
    "print(\"Total records BEFORE:\", X_train.shape[0])\n",
    "print(\"Total records AFTER:\", X_balanced.shape[0])\n",
    "print(\"Difference =\", X_train.shape[0] - X_balanced.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0NBQr5D70FB"
   },
   "source": [
    "We now see that both the bad and good target variable are equal. Why  do we need to balance our dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vP83L2FQS885"
   },
   "source": [
    "### **2.5 Training the Models and Getting the Performance Metrics**\n",
    "\n",
    "As the process of model building affects the business, a discussion with business owners on the objective of the model will highly affect how we set thresholds. If the purpose is customer acquisition at the expense of risk, we can always lower the threshold of probability so as to get as many customers as possible. If we are interested in optimizing to get high profits or minimize high risks, then we can increase the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kHxxCtzSyH7"
   },
   "outputs": [],
   "source": [
    "# Set the threshold for defaults\n",
    "THRESHOLD = 0.50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWtxUOIU9xxM"
   },
   "source": [
    "In this exercise, we will use two classifiers, logistic regression that is highly used by banks to discriminate between defaulters and non-defaulters, because it is easy to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "htGwUcULTFdW"
   },
   "outputs": [],
   "source": [
    "# list of classifiers\n",
    "classifiers = [\n",
    "    LogisticRegression(max_iter=220, random_state=2022),\n",
    "    RandomForestClassifier(random_state=2022),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0N5j6xd-H5w"
   },
   "source": [
    "The function below trains the models on a dataset and evaluates their performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfQgPi1aW1zQ"
   },
   "outputs": [],
   "source": [
    "def calculate_model_metrics(model, X_test, y_test, model_probs, threshold):\n",
    "    \"\"\"\n",
    "    Calculates Accuracy, F1-Score, PR AUC\n",
    "    \"\"\"\n",
    "    # keeps probabilities for the positive outcome only\n",
    "    probs = pd.DataFrame(model_probs[:, 1], columns=[\"prob\"])\n",
    "\n",
    "    # applies the threshold\n",
    "    y_pred = probs[\"prob\"].apply(lambda x: 1 if x > threshold else 0)\n",
    "\n",
    "    # calculates f1-score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # calculates accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # calculates kappa score\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "    # calculates AUC\n",
    "    auc_score = roc_auc_score(y_test, probs)\n",
    "\n",
    "    # calculates the precision\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "\n",
    "    # calculates the recall\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy, kappa, f1, auc_score, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ErwtJWFDTRtg"
   },
   "outputs": [],
   "source": [
    "def get_classifiers_performance(\n",
    "    X_train, X_test, y_train, y_test, threshold, classifiers\n",
    "):\n",
    "    # creates empty data frame\n",
    "    df_performance = pd.DataFrame()\n",
    "\n",
    "    for clf in classifiers:\n",
    "        print(\"Training \" + type(clf).__name__ + \"...\")\n",
    "        # fits the classifier to training data\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # predict the probabilities\n",
    "        clf_probs = clf.predict_proba(X_test)\n",
    "\n",
    "        # calculates model metrics\n",
    "        (\n",
    "            clf_accuracy,\n",
    "            clf_kappa,\n",
    "            clf_f1,\n",
    "            clf_auc,\n",
    "            clf_precision,\n",
    "            clf_recall,\n",
    "        ) = calculate_model_metrics(clf, X_test, y_test, clf_probs, threshold)\n",
    "\n",
    "        # creates a dict\n",
    "        clf_dict = {\n",
    "            \"model\": [type(clf).__name__, \"---\"],\n",
    "            \"precision\": [clf_precision, np.nan],\n",
    "            \"recall\": [clf_recall, np.nan],\n",
    "            \"f1-Score\": [clf_f1, np.nan],\n",
    "            \"ROC AUC\": [clf_auc, np.nan],\n",
    "            \"accuracy\": [clf_accuracy, np.nan],\n",
    "            \"cohen kappa\": [clf_kappa, np.nan],\n",
    "        }\n",
    "\n",
    "        # concatenate Data Frames\n",
    "        df_performance = pd.concat([df_performance, pd.DataFrame(clf_dict)])\n",
    "\n",
    "    # resets Data Frame index\n",
    "    df_performance = df_performance.reset_index()\n",
    "\n",
    "    # drops index\n",
    "    df_performance.drop(\"index\", axis=1, inplace=True)\n",
    "\n",
    "    # gets only the odd numbered rows\n",
    "    rows_to_drop = np.arange(1, len(classifiers) * 2, 2)\n",
    "\n",
    "    # drops unwanted rows that have no data\n",
    "    df_performance.drop(rows_to_drop, inplace=True)\n",
    "\n",
    "    # returns performance summary\n",
    "    return df_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 149
    },
    "id": "YmgOFtlcWjAK",
    "outputId": "6e9054f6-f126-4e72-85a3-2e540f832a2c"
   },
   "outputs": [],
   "source": [
    "# calculates classifiers performance\n",
    "df_performances = get_classifiers_performance(\n",
    "    X_balanced, X_test, y_balanced, y_test, THRESHOLD, classifiers\n",
    ")\n",
    "# highlight max values for each column\n",
    "df_performances.style.highlight_max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1epML-z7-gTK"
   },
   "source": [
    "Clearly, the random forest classifier outperforms logistic regression in nearly all the metrics with the exception of recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5l4J1fMa2Fy"
   },
   "source": [
    "### **2.6 Probability Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njQLPU9_aBmp"
   },
   "outputs": [],
   "source": [
    "# instantiates the classifiers\n",
    "lr_clf = LogisticRegression(max_iter=220, random_state=2022)\n",
    "rf_clf = RandomForestClassifier(random_state=2022)\n",
    "\n",
    "# trains the classifiers\n",
    "lr_clf.fit(X_balanced, y_balanced)\n",
    "rf_clf.fit(X_balanced, y_balanced)\n",
    "\n",
    "# store the predicted probabilities for class 1\n",
    "y_pred_lr_prob = lr_clf.predict_proba(X_test)[:, 1]\n",
    "y_pred_rf_prob = rf_clf.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "Inweh4G5bHde",
    "outputId": "983f83f7-aca6-44f2-80a3-cb08f66c9ca8"
   },
   "outputs": [],
   "source": [
    "# sets plot size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plots\n",
    "sns.kdeplot(y_pred_lr_prob, label=\"Logistic Regression\")\n",
    "sns.kdeplot(y_pred_rf_prob, label=\"Random Forest\")\n",
    "\n",
    "# sets the plot features\n",
    "plt.title(\"Probability Density Plot\", fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Fig. 3: Top: A probability density plot for the predicted performance of customers\",\n",
    "    fontweight=\"bold\",\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "\n",
    "# displays the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4WsOSF8_VGs"
   },
   "source": [
    "We can observe in the density plot above that the largest concentration of probabilities is around 0, and logistic regression has moderate distributed probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "tdJEhjx1bStA",
    "outputId": "3c3a4f5f-e704-4fd6-e39e-ad0db5b8bff4"
   },
   "outputs": [],
   "source": [
    "# set axes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# set fig size\n",
    "ax.figure.set_size_inches(12, 8)\n",
    "\n",
    "# plot probability - Logistic Regression\n",
    "plt.subplot(2, 2, 1)\n",
    "ax = stats.probplot(y_pred_lr_prob, plot=plt)\n",
    "\n",
    "# plot probability - Random Forest\n",
    "plt.subplot(2, 2, 2)\n",
    "ax = stats.probplot(y_pred_rf_prob, plot=plt)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Fig. 4: Top: Comparing model performance and theoretical expectation\",\n",
    "    fontweight=\"bold\",\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "\n",
    "# displays the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enzOWPfoAZxr"
   },
   "source": [
    "The data seem to follow the theoretical distribution when we use the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p5RPsG5Mbl18",
    "outputId": "51416b7e-90f6-443e-cc22-00708448bd23"
   },
   "outputs": [],
   "source": [
    "# calculates the Brier Score Loss\n",
    "bsl_lr = brier_score_loss(y_test, y_pred_lr_prob, pos_label=1)\n",
    "bsl_rf = brier_score_loss(y_test, y_pred_rf_prob, pos_label=1)\n",
    "\n",
    "# prints the calculated Brier Score Loss for each algorithm probability\n",
    "print(f\"Brier Score Loss (Logistic Regression): {np.round(bsl_lr, 2)}\")\n",
    "print(f\"Brier Score Loss (Random Forest): {np.round(bsl_rf, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opV6qm1iBDcb"
   },
   "source": [
    "The smaller the value of the Brier score, the better.  The Brier score is made up of refinement loss and calibration loss. Clearly, we can see that random forest performs better since it give us a lower Brier score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OXMRQetbb_nY",
    "outputId": "c8fb41fc-3cb7-4538-e12b-305101863e06"
   },
   "outputs": [],
   "source": [
    "# makes predictions\n",
    "y_pred_lr = lr_clf.predict(X_test)\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "\n",
    "print(\"Classification Report for \" + type(lr_clf).__name__)\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "print(\"\\nClassification Report for \" + type(rf_clf).__name__)\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7jEcVmuBwWa"
   },
   "source": [
    "We observe that random forest performs better than logistic regression on unseen data. You are tasked to plot the receiver operation curve and the confusion matrix of the two models.\n",
    "\n",
    "In the next step we will train deep learning models to estimate probability of default.\n",
    "\n",
    "### **Deep Learning**\n",
    "\n",
    "We start by importing `keras` classifier to run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVvpGueU_mdx"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otraapKhHJsE"
   },
   "source": [
    "We then write a function with the model layers and activation functions at each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3ExiYq2DYyF"
   },
   "outputs": [],
   "source": [
    "def model_function(dropout_rate, verbose=0):\n",
    "    # Select a `keras` model, sequential function allows us to specify our neural network architecture\n",
    "    model = keras.Sequential()\n",
    "    # First Layer is dense implying that the features are connected to every single node in the first hidden layer\n",
    "    model.add(Dense(128, kernel_initializer=\"normal\", activation=\"relu\", input_dim=26))\n",
    "    model.add(Dense(64, kernel_initializer=\"normal\", activation=\"relu\"))\n",
    "    model.add(Dense(8, kernel_initializer=\"normal\", activation=\"relu\"))\n",
    "    # Drop out is added to ignore irrelevant neurons\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    # We add sigmoid to classify the target\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    # the loss function is binary cross entropy because the problem is a binary classification\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giGsXUwVQxKy"
   },
   "source": [
    "The next cell defines the model parameters and hyperparameters, train the model and validate the model on test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jc9VenRXD5K4"
   },
   "outputs": [],
   "source": [
    "model = KerasClassifier(\n",
    "    build_fn=model_function, dropout_rate=0.2, verbose=0, batch_size=50, epochs=100\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "y_predict_dl = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yp9ID-ezRJBh"
   },
   "source": [
    "We now evaluate the model using several metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G6vEuT7nRHbt",
    "outputId": "76ffaab6-07e8-4b6c-ce20-aa8c18b7f647"
   },
   "outputs": [],
   "source": [
    "ROC_AUC_metric = roc_auc_score(y_test, pd.DataFrame(y_predict_dl.flatten()))\n",
    "print(\"Deep Learning ROC AUC is {:.4f}\".format(ROC_AUC_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8bpQdG6CP3ot",
    "outputId": "c2fbd35c-f98b-4921-b6f5-debb10935e37"
   },
   "outputs": [],
   "source": [
    "print(\"\\nClassification Report for deep learning model\")\n",
    "print(classification_report(y_test, y_predict_dl.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2UnlQHdLQVoe",
    "outputId": "21d6df1e-dab5-4cc9-c270-5c0e184a1197"
   },
   "outputs": [],
   "source": [
    "print(\"\\nThe brier score is equal to \")\n",
    "brier_score_loss(y_test, y_predict_dl.flatten(), pos_label=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIHCVkjiRRQd"
   },
   "source": [
    "We can observe that the model performance is comparable to random forest model without much data preparation or data balancing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75qKfun0F166"
   },
   "source": [
    "The next step is to help a business make the right decision. The discussion for this is beyond the scope of this lesson and we will therefore only give a brief discussion of what to expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjwSMQDncbL-"
   },
   "source": [
    "### **2.7 Business Decision Making**\n",
    "\n",
    "The final step of modeling is to evaluate if our modeling met the set Key Performance Indicators (KPIs) by the institution. Among the metrics to measure are:\n",
    "\n",
    "#### **2.7.1 Acceptance Rate**\n",
    "\n",
    "The threshold for an acceptance rate is used for setting the percentage of new loans business are willing to disburse. We use test data as a proxy for fresh new loans to measure the performance. \n",
    "\n",
    "We use the threshold to assign new `loan_status` values. We then observe if the distribution of the bads and goods changes.\n",
    "\n",
    "#### **2.7.2 Bad Rates**\n",
    "\n",
    "Now that we have the acceptance rate, we do the analysis on bad rate percentages within the loans that met the threshold. This allows us to see the percentage of defaults that our model will accept.\n",
    "\n",
    "In summary, we set an acceptance rate so as to have fewer bad loans in our portfolio. \n",
    "\n",
    "#### **2.7.3 Strategy**\n",
    "\n",
    "At this point, a strategy meeting with project (business) owners is called where we brainstorm with them and strategize on the best approach for using the scorecard. This is done by building a strategy table that has simulations with possible outcomes.\n",
    "\n",
    "#### **2.7.4 Total Loss**\n",
    "\n",
    "Finally, we estimate the total expected loss having concrete decisions from the data science team and input from business owners. We define the expected loss as the amount of money a financial lender might lose by lending to a borrower. The components of expected loss (EL) are:\n",
    "\n",
    "- Probabilities of default (**PD**)\n",
    "- The loss given default (**LGD**) \n",
    "- The `loan_amnt`, which will be assumed to be the exposure at default (**EAD**).\n",
    "\n",
    "The factors that contribute to expected loss are categorized into:\n",
    "\n",
    "1. Borrower specific factors.\n",
    "2. Economic environment.\n",
    "\n",
    "The expected loss is calculated as shown below:\n",
    "\n",
    "### $\\text{Total Expected Loss} = \\sum_{x=1}^n PD_x * LGD_x * EAD_x.$\n",
    "\n",
    " `expected_loss = prob_default * lgd * loan_amnt`\n",
    "\n",
    "Probability of default as defined earlier is the inability of the borrower to repay their debt in full or on time/ schedule.\n",
    "\n",
    "Loss given default on the other hand is the proportion of the borrowed amount by the customer that the financial institution will be unable to recover once the customer defaults.\n",
    "\n",
    "Exposure at default is the total sum that a lender is exposed to at the time of default.\n",
    "\n",
    "Let us consider an example below.\n",
    "\n",
    "Consider the case of a mortgage worth $\\$ 1,000,000$ and the lender is willing to finance $80\\%$ of the mortgage value. The borrower pays back $\\$ 100,000$ and our model shows that the probability of default is $20\\%$. Further assume that the bank can recover some of the money by auctioning the house at $\\$ 560,000$ after the customer has defaulted. Then, this is how we will calculate the expected loss:\n",
    "\n",
    "- Cost of House = $\\$ 1,000,000$\n",
    "- Lender finances $80\\%$ of the cost, that is, loan amount = $\\$ 800,000$\n",
    "- If the borrower pays back $\\$100,000$ then exposure at default = $\\$800,000 - \\$ 100,000 = \\$ 700,000$\n",
    "- Probability of default = $0.2$\n",
    "- Loss given default = $\\frac{\\$ 700,000 - \\$ 560,000}{\\$ 700,000} = 20\\%$\n",
    "\n",
    "So the expected loss is:\n",
    "$$= \\text{PD}\\times \\text{LGD}\\times {EAD}\\\\\n",
    "= 0.2 \\times 0.2 \\times \\$700,000 = \\$ 28,000$$\n",
    "\n",
    "In this lesson, we have covered an application of machine learning in credit risk management. Can you think of how the same can be implemented using a deep learning algorithm?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIf5dBqHOPKs"
   },
   "source": [
    "## **3. Conclusion**\n",
    "\n",
    "This module introduced the concept of deep learning and its application in finance. In the next module, we will study hyperparameter tuning in machine learning models to improve their performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYJvDOYROWfU"
   },
   "source": [
    "**References**\n",
    "\n",
    "1. Cooper, Michael James. *A Deep Learning Prediction Model for Mortgage Default.* 2018. University of Bristol, Masters thesis.\n",
    "\n",
    "2. Giesecke, Kay. \"Credit Risk Modeling and Valuation: An Introduction.\" 2004. Available at SSRN 479323.\n",
    "\n",
    "3. Sirignano, Justin et al. \"Deep Learning for Mortgage Risk.\" *arXiv* preprint arXiv:1607.02470. 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Copyright © 2022 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n",
    "<span style='color: transparent; font-size:1%'>All rights reserved WQU WorldQuant University QQQQ</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1cc05c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "Copyright 2023 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
