{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8JSeOadQ51U"
   },
   "source": [
    "\n",
    "## MACHINE LEARNING IN FINANCE\n",
    "MODULE 4 | LESSON 4\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grgJcZUXQ-Z_"
   },
   "source": [
    "# **ENSEMBLE LEARNING COMPARISONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhhrLQ4mRXks"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  25 minutes |\n",
    "|**Prior Knowledge** | Boosting methodology, Adaptive Boosting, Ensemble learning, Derivatives  |\n",
    "|**Keywords** |Pseudo residuals, loss function, learning rate, gradient boosting  |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHkaHhZ8RQo_"
   },
   "source": [
    "*The previous lesson introduced the reader to the details of boosting, specifically adaptive boosting. This lesson will explore Gradient Boosting and compare all ensemble learning models covered in this module to a common predictive problem.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTcugHIXoJvo"
   },
   "source": [
    "## **1. Gradient Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvK-2aosoK-V"
   },
   "source": [
    "In the previous lesson we've looked at AdaBoost as it was among the first designed boosting algorithm with a particular loss function that can be sensitive to outliers. Gradient Boosting, however, is a generic algorithm that allows the optimization of an arbitrary loss function, thus making Gradient Boosting more flexible and robust to outliers than AdaBoost. The requirement of the loss function is that it is differentiable. We've seen that AdaBoost places more emphasis on predictions that were incorrect from the previous base learner although there is still some weight assigned to the correct predictions. With Gradient Boosting, however, there is only an emphasis on building the subsequent base learners based on the misclassifications from the previous learner. To illustrate this, we will go through an example to explain the process. Unlike the previous section, we will look at a regression example, i.e., a continuous target variable to illustrate boosting applied to a different type of problem. The dataset we use is the house price dataset from the UCI Machine Learning Repository. We will begin by importing the necessary libraries for this lesson along with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETGMAn4lvHaj"
   },
   "outputs": [],
   "source": [
    "# import all necessary libraries\n",
    "\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    "    StackingClassifier,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# for stacking model later\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "executionInfo": {
     "elapsed": 925,
     "status": "ok",
     "timestamp": 1667247027579,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "cLlbSCj5oTct",
    "outputId": "a7fb475c-5bf4-452e-d32c-ed7514c1ffd6"
   },
   "outputs": [],
   "source": [
    "loc = \"../../data\"  # specify location of dataset\n",
    "data4gradboost = pd.read_excel(loc + \"/Real estate valuation data set.xlsx\")\n",
    "\n",
    "data4gradboost.set_index(\"No\", drop=True, inplace=True)\n",
    "data4gradboost.drop(\"X1 transaction date\", axis=1, inplace=True)\n",
    "data4gradboost.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDDHGrloLltH"
   },
   "source": [
    "Above shows a snapshot of the dataset, which has 5 predictors and a target variable in the last column that represents the price per unit area. Refer to https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set for more details on this dataset. We start by separating the dataset into the features and target column for a train/test split. The size of the test set is not important for this example since we are only interested in understanding the gradient boosting algorithm methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbDp3pk_ogsp"
   },
   "outputs": [],
   "source": [
    "# Separate into X and Y\n",
    "X = data4gradboost.iloc[:, :-1]\n",
    "y = data4gradboost.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmnLGyonooLJ"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyIJpVY6MWlo"
   },
   "source": [
    "#### **1.1 Step 1: Fit Weak Learner**\n",
    "\n",
    "Now that we have the data to develop our gradient boosting model, we can begin the first step. Similarly, for the AdaBoost in the previous lesson, we use weak decision tree classifiers as the base learners. We choose a max_depth of 2 and then fit the base learner to the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667247036658,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "8aalRs96oq3g",
    "outputId": "e38e1d1c-3ae2-477a-ecb2-6287b54ba2d9"
   },
   "outputs": [],
   "source": [
    "tree_1 = DecisionTreeRegressor(max_depth=2, random_state=2)\n",
    "tree_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZ7kKKRONAbk"
   },
   "source": [
    "#### **1.2 Step 2: Calculate Residuals**\n",
    "\n",
    "Once we fit our first weak learner to the data, we then calculate the residuals. The residuals are simply given by\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\text{residuals} = y_{i} -\\hat y_{i}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $y$ is the actual outcome and $\\hat y$ is the predicted value for observation $i$. The residuals now become the new target data to train the next base learner. Gradient boost builds trees based on the residuals or errors of the previous tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyQ186pbEsgC"
   },
   "source": [
    "#### **1.3 Step 3: Train Next Base Learner on Residuals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBDw0zQqoq6n"
   },
   "outputs": [],
   "source": [
    "# predictions\n",
    "y_pred1 = tree_1.predict(X_train)\n",
    "# residuals become the next target data to train\n",
    "y2_train = y_train - y_pred1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ny4vkJC6BK2R"
   },
   "source": [
    "Once we have the new target values to train the next base learner, we continue this cycle, i.e., steps 1 to 3. In this example, we will do this for 3 base learners, i.e., 3 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aa_tvQ8ovoR"
   },
   "outputs": [],
   "source": [
    "# initialize new tree or 2nd base learner\n",
    "tree_2 = DecisionTreeRegressor(max_depth=2, random_state=2)\n",
    "tree_2.fit(X_train, y2_train)\n",
    "\n",
    "# predictions\n",
    "y_pred2 = tree_2.predict(X_train)\n",
    "# new target values\n",
    "y3_train = y2_train - y_pred2\n",
    "\n",
    "# initialize new tree or 3rd base learner\n",
    "tree_3 = DecisionTreeRegressor(max_depth=2, random_state=2)\n",
    "tree_3.fit(X_train, y3_train)\n",
    "\n",
    "# last set of predictions. Stop at 3rd base learner\n",
    "y_pred3 = tree_3.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyRvsMTRE47B"
   },
   "source": [
    "#### **1.4 Final Step: Calculate Final Predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPBNeUupBzLs"
   },
   "source": [
    "Finally, for predictions of unseen observations, we would add the predictions of each base learner as below. We assign the final predictions to `y_pred` and then calculate a performance metric, namely Root Mean Squared Error (RMSE). We obtain an RMSE of 9.9625."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 459,
     "status": "ok",
     "timestamp": 1667247045060,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "zKGa8h47wRcP",
    "outputId": "952ee74e-c168-433a-9625-ad6e24997d2b"
   },
   "outputs": [],
   "source": [
    "# Make predictions on test set for all 3 base learners\n",
    "y1_pred = tree_1.predict(X_test)\n",
    "y2_pred = tree_2.predict(X_test)\n",
    "y3_pred = tree_3.predict(X_test)\n",
    "\n",
    "y_pred = y1_pred + y2_pred + y3_pred\n",
    "\n",
    "# check MSE\n",
    "round(mean_squared_error(y_test, y_pred) ** 0.5, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8rg8y4vCkmY"
   },
   "source": [
    "Building the gradient boosting algorithm from scratch is quite tedious, and it is easy to make an error coding it. Fortunately, Python has the built-in `GradientBoostingRegressor` from sklearn to make things much easier. Below is the more elegant and efficient way to code this. Notice the hyperparameter `learning_rate` = 1, which we'll address later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d72LXkpPxea_"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnpJ_XPFxipR"
   },
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(\n",
    "    max_depth=2, n_estimators=3, random_state=10, learning_rate=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1667247051503,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "RRSsq7fMx39Z",
    "outputId": "489a2a2f-59b4-4d80-c4de-abcd7f465ee6"
   },
   "outputs": [],
   "source": [
    "gbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nr5RHKVDZ9G"
   },
   "source": [
    "The RMSE results are identical to our model built from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 441,
     "status": "ok",
     "timestamp": 1667247055879,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "SCiMkYRlyLV0",
    "outputId": "02e11b28-d4ed-41df-9561-d2ab1e6ac694"
   },
   "outputs": [],
   "source": [
    "gbr_pred = gbr.predict(X_test)\n",
    "round(mean_squared_error(y_test, gbr_pred) ** 0.5, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7h-FWMAz-LZ"
   },
   "source": [
    "The example above illustrates the procedure followed by a gradient boosting algorithm. Below will show how it works with a bit more attention to the mathematics behind it.<span style='color: transparent; font-size:1%'>All rights reserved WQU WorldQuant University QQQQ</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCTcMTUMEl5D"
   },
   "source": [
    "### **1.5 Gradient Boosting Reasoning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcrRPZ7gFZzi"
   },
   "source": [
    "The algorithm begins with a naive guess of using the average, say $\\left<y\\right>$, for all house prices or target values. The reasoning behind this is gradient boosting aims to minimize the loss function $L(y_i, \\hat y_i)$. A common $L$ used is of the form\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "L(y_i, \\hat y_i) = \\frac{1}{2}\\sum_{i}^{N} \\left(y_i - \\hat y_i \\right)^2 \n",
    "\\tag{1.1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "over $N$ observations. This type of loss function is an L2 Loss function.\n",
    "\n",
    "At the start, we assume $\\hat y_i$ is a constant function $F_{0}(x)$ over predictors $x$. From calculus, minimizing implies finding the derivative of (1.1) with respect to $F_{0}(x)$, equating to zero and solving for $F_{0}(x)$. If we do this, the solution $F_{0}^*(x)$ is,\n",
    "$$\n",
    "\\begin{equation}\n",
    "F_{0}^*(x) = \\frac{1}{N}\\sum_{i}^{N}y_i =\\bar{y}\n",
    "\\tag{1.2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "which is how we get the average as the first guess. Note, had we chosen a different $L$ this may not have been the case. For illustrative purposes, we will use the loss function in (1.1) to explain the concept. \n",
    "\n",
    "Remember from our example in section 1.1 that the next base learner would aim to predict the residuals of the previous base learner instead of the actual values. Starting off with predictions as the average of the actuals is a very simplistic estimation; however, the next iteration 1 will improve on the previous base learner predictions such that new predictions are\n",
    "$$\n",
    "\\begin{equation}\n",
    "F_{1}(x) = F_{0}(x) + h_1(x)\n",
    "\\tag{1.3}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $h_1(x)$ is an added estimator such that together with the previous estimate $F_{0}$, will improve the estimates. We hope that our new estimates would predict the actuals i.e.,\n",
    "$$\n",
    "\\begin{equation}\n",
    "F_{1}(x) = F_{0}(x) + h_1(x) = y\n",
    "\\tag{1.4}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "but rearranging (1.4) gives us $h_1(x) = y - F_{0}(x)$. This is just the residuals from the previous base learner's predictions and is what we were doing in section 1.1. So to get this estimator $h_1(x)$ we would provide as a target column $(y - F_{0}(x))$ and fit a base learner on the dataset $\\{(x_i , y_i - F_{0}(x))\\}_{i=1}^N$ or $\\{(x_i , r_{i1})\\}_{i=1}^N$ where $r_{i1}$ are the pseudo residuals at iteration 1. In general at iteration $j$, we have from (1.4),\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "F_{j}(x) = F_{j-1}(x) + h_j(x)\n",
    "\\tag{1.5}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "and we develop our estimator $h_j(x)$ in the same way. There is a parameter $\\nu$, that is not shown in (1.5) where,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "F_{j}(x) = F_{j-1}(x) + \\nu h_j(x)\n",
    "\\tag{1.6}\n",
    "\\end{equation} \n",
    "$$\n",
    "\n",
    "which we've set as $\\nu = 1$ but can actually take on values in the range $[0,1]$. This parameter is the *learning rate*, which improves accuracy in the long run. Think of it as a step size, and often many learners with small step sizes provide better results. From recursive substitution over all iterations, the last iteration $T$ will provide a final model such that for a test or unseen data point $x^*$ the prediction would be \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "F_{T}(x^*) = F_{0}+  \\sum_{j}^{T-1}\\nu h_{j}(x^*),\n",
    "\\tag{1.6}\n",
    "\\end{equation} \n",
    "$$\n",
    "\n",
    "which ties in to the last step for our gradient boosting model built from scratch. Classification is very similar to the regression process but differs mainly in the scoring. The reader is referred to Murphy (607) for further reading on classification. There is also a modified gradient boosting algorithm, which improves on what we've covered above called Extreme Gradient Boosting or XGBoost. One of the modifications is that XGBoost uses a 2nd order approximation of the loss function and adds a regularizer on the tree complexity. Refer to Murphy (615) for details on this.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JPOm5U5Da7m"
   },
   "source": [
    "## **2. Ensemble Learning Comparison**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpI8hQYhDlTg"
   },
   "source": [
    "This section will apply Bagging, Stacking, AdaBoost, and Gradient boosting to a common classification problem. We predict whether the Luxembourg index (LUXXX) will exceed a return of 0.25% in any direction. The predictors are a combination of country indices and technical indicators. We start by importing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADXr5DdQGr9h"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# loc = \"ENTER YOUR FULL PATH TO LOCATION OF DATA FILE HERE\"\n",
    "# data_df = pd.read_csv(loc+\"/MScFE 650 MLF GWP Data.csv\")\n",
    "loc = \"../../data\"\n",
    "data_df = pd.read_csv(loc + \"/MScFE 650 MLF GWP Data.csv\")\n",
    "# Convert string to datetime\n",
    "data_df[\"Date\"] = pd.to_datetime(data_df[\"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ww-W0BgYHDWY"
   },
   "source": [
    "Create the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NScysWJQHDjM"
   },
   "outputs": [],
   "source": [
    "# Set Target Index for predicting\n",
    "target_ETF = \"LUXXX\"\n",
    "\n",
    "# Use returns instead of prices for other Indices\n",
    "# Other Indices used as Index_features\n",
    "ETF_features = data_df.loc[:, ~data_df.columns.isin([\"Date\", target_ETF])].columns\n",
    "data_df[ETF_features] = data_df[ETF_features].pct_change()\n",
    "\n",
    "data_df[target_ETF + \"_returns\"] = data_df[target_ETF].pct_change()\n",
    "\n",
    "# Create Target Column.\n",
    "# Shift period for target column\n",
    "data_df[target_ETF + \"_returns\" + \"_Shift\"] = data_df[target_ETF + \"_returns\"].shift(-1)\n",
    "\n",
    "# Strategy to take long position for anticipated returns of 0.5%\n",
    "data_df[\"Target\"] = np.where(\n",
    "    (data_df[target_ETF + \"_returns_Shift\"].abs() > 0.025), 1, 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_B2uNCqcHMZm"
   },
   "source": [
    "Technical indicators included as predictors are slow to fast moving average ratio (SMA_ratio), Relative Strength Index (RSI), and Rate of Change (RC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ojYx3dbFHNUm"
   },
   "outputs": [],
   "source": [
    "# Four country indices used.\n",
    "feats = [\"MSCI KOREA\", \"MSCI DENMARK\", \"MSCI FRANCE\", \"MSCI NORWAY\"]\n",
    "\n",
    "# creating the technical indicators\n",
    "data_df[\"SMA_5\"] = data_df[target_ETF].rolling(5).mean()\n",
    "data_df[\"SMA_15\"] = data_df[target_ETF].rolling(15).mean()\n",
    "data_df[\"SMA_ratio\"] = data_df[\"SMA_15\"] / data_df[\"SMA_5\"]\n",
    "\n",
    "# Can drop SMA columns since not needed anymore.\n",
    "data_df.drop([\"SMA_5\", \"SMA_15\"], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# shift the price of the target by 1 unit previous in time\n",
    "data_df[\"Diff\"] = data_df[target_ETF] - data_df[target_ETF].shift(1)\n",
    "data_df[\"Up\"] = data_df[\"Diff\"]\n",
    "data_df.loc[(data_df[\"Up\"] < 0), \"Up\"] = 0\n",
    "\n",
    "data_df[\"Down\"] = data_df[\"Diff\"]\n",
    "data_df.loc[(data_df[\"Down\"] > 0), \"Down\"] = 0\n",
    "data_df[\"Down\"] = abs(data_df[\"Down\"])\n",
    "\n",
    "data_df[\"avg_5up\"] = data_df[\"Up\"].rolling(5).mean()\n",
    "data_df[\"avg_5down\"] = data_df[\"Down\"].rolling(5).mean()\n",
    "\n",
    "data_df[\"avg_15up\"] = data_df[\"Up\"].rolling(15).mean()\n",
    "data_df[\"avg_15down\"] = data_df[\"Down\"].rolling(15).mean()\n",
    "\n",
    "data_df[\"RS_5\"] = data_df[\"avg_5up\"] / data_df[\"avg_5down\"]\n",
    "data_df[\"RS_15\"] = data_df[\"avg_15up\"] / data_df[\"avg_15down\"]\n",
    "\n",
    "data_df[\"RSI_5\"] = 100 - (100 / (1 + data_df[\"RS_5\"]))\n",
    "data_df[\"RSI_15\"] = 100 - (100 / (1 + data_df[\"RS_15\"]))\n",
    "\n",
    "data_df[\"RSI_ratio\"] = data_df[\"RSI_5\"] / data_df[\"RSI_15\"]\n",
    "\n",
    "# Can drop RS Calc columns columns\n",
    "data_df.drop(\n",
    "    [\"Diff\", \"Up\", \"Down\", \"avg_5up\", \"avg_5down\", \"avg_15up\", \"avg_15down\"],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "data_df[\"RC\"] = data_df[target_ETF].pct_change(periods=15)\n",
    "\n",
    "# all_feats\n",
    "feats.append(\"SMA_ratio\")\n",
    "feats.append(\"RSI_ratio\")\n",
    "feats.append(\"RC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGUZK_23Jzsw"
   },
   "source": [
    "Now we can apply all our models to the data. Perform the train/test split of 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftYAsBNeKMNq"
   },
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "# Train/Test split. No NaNs in the data.\n",
    "NoNaN_df = data_df.dropna()\n",
    "X = NoNaN_df[feats]\n",
    "\n",
    "X = X.iloc[:, :]  # .values\n",
    "y = NoNaN_df.loc[:, \"Target\"]  # .values\n",
    "\n",
    "del NoNaN_df\n",
    "\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHKVuIT8KjuO"
   },
   "source": [
    "We will tune the boosting models and use the hyperparameters from Lesson 2 for the bagging model. We should note that AdaBoost by default has Decision Tree Classifiers as base learners with max_depth = 1 as default. For the gradient boosting classifier, we assume the default max_depth of 3. We only tune on learning rate and n_estimators to reduce computational time. We also do the same for the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vhj39iEUKfh_"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3791,
     "status": "ok",
     "timestamp": 1667247096267,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "VlBjUvr3KvJw",
    "outputId": "6e9e2363-b033-47c6-e22f-cf04763771da"
   },
   "outputs": [],
   "source": [
    "# parameters for AdaBoost\n",
    "param_grid = {\"n_estimators\": [10, 20, 50, 100], \"learning_rate\": [0.1, 0.25, 0.5, 1.0]}\n",
    "\n",
    "gridAdBoost = GridSearchCV(\n",
    "    AdaBoostClassifier(), param_grid, refit=True, verbose=3, cv=3\n",
    ")\n",
    "# fitting the model for grid search\n",
    "gridAdBoost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1667247102274,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "ECuON9TuMZSS",
    "outputId": "3650ea81-fad0-4b27-e3e0-b1cc5307f873"
   },
   "outputs": [],
   "source": [
    "# print best parameter after tuning\n",
    "print(gridAdBoost.best_params_)\n",
    "\n",
    "# print how our AdaBoost model looks after hyper-parameter tuning\n",
    "print(gridAdBoost.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2591,
     "status": "ok",
     "timestamp": 1667247110171,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "JUEnT_byMNRe",
    "outputId": "10b7e07f-ad3b-49fe-a7d2-a11b8ac4ec71"
   },
   "outputs": [],
   "source": [
    "# parameters for Gradient Boosting Classifier\n",
    "param_grid = {\"n_estimators\": [10, 20, 50, 100], \"learning_rate\": [0.1, 0.25, 0.5, 1.0]}\n",
    "\n",
    "GBgrid = GridSearchCV(\n",
    "    GradientBoostingClassifier(), param_grid, refit=True, verbose=3, cv=3\n",
    ")\n",
    "\n",
    "# fitting the model for grid search\n",
    "GBgrid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 412,
     "status": "ok",
     "timestamp": 1667247116480,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "6oYiF9GnOGT0",
    "outputId": "8f5f92a7-53a1-4d1d-a6c5-dd900063f9de"
   },
   "outputs": [],
   "source": [
    "# print best parameter after tuning\n",
    "print(GBgrid.best_params_)\n",
    "\n",
    "# print how our Gradient Boost model looks after hyper-parameter tuning\n",
    "print(GBgrid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09MnopW_R0CG"
   },
   "outputs": [],
   "source": [
    "# import library for XGBoost\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1051,
     "status": "ok",
     "timestamp": 1667247123331,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "3TE2LWj9R1_c",
    "outputId": "e2afdbb1-9337-4915-dd0e-7a13754c0224"
   },
   "outputs": [],
   "source": [
    "# parameters for XG Boosting Classifier\n",
    "param_grid = {\"n_estimators\": [10, 20, 50, 100], \"learning_rate\": [0.1, 0.25, 0.5, 1.0]}\n",
    "\n",
    "XGB_model = xgb.XGBClassifier()\n",
    "XGBgrid = GridSearchCV(XGB_model, param_grid, refit=True, verbose=3, cv=3)\n",
    "\n",
    "# fitting the model for grid search\n",
    "XGBgrid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 428,
     "status": "ok",
     "timestamp": 1667247128095,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "kI90f49QSbwx",
    "outputId": "da0244e7-cfd8-435d-bb8c-5c07bfab6255"
   },
   "outputs": [],
   "source": [
    "# print best parameter after tuning\n",
    "print(XGBgrid.best_params_)\n",
    "\n",
    "# print how our XGBoost model looks after hyper-parameter tuning\n",
    "print(XGBgrid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaBWUr4xKhuG"
   },
   "source": [
    "So we find that AdaBoost and Gradient Boosting have the same optimal hyperparameters; however, XGBoost has a different learning rate. We can train all of our ensemble models and then compare performance afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667247131391,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "aX7n4eUjOe8c",
    "outputId": "fefa3ad6-ce9e-4ea0-8adc-f8dca402772c"
   },
   "outputs": [],
   "source": [
    "# Train with Tuned Random Forest\n",
    "# Create a tuned RF Classifier\n",
    "bagmodel_tuned = RandomForestClassifier(\n",
    "    max_depth=2, min_samples_split=8, n_estimators=10, random_state=10\n",
    ")\n",
    "\n",
    "bagmodel_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meZeEQZEPLDd"
   },
   "source": [
    "This is the stacking model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 439,
     "status": "ok",
     "timestamp": 1667247135140,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "DP5DLYLrPKf2",
    "outputId": "0013eeb5-6743-4916-f4df-1ab34a28bf68"
   },
   "outputs": [],
   "source": [
    "clf1 = DecisionTreeClassifier()  # Decision Tree\n",
    "\n",
    "clf2 = SVC(kernel=\"rbf\")  # Support Vector Classifier\n",
    "\n",
    "clf3 = GaussianNB()  # Gaussian Naive Bayes\n",
    "\n",
    "est_rs = [(\"DTree\", clf1), (\"SVM\", clf2), (\"NB\", clf3)]\n",
    "# Meta model\n",
    "mylr = LogisticRegression()\n",
    "# creating a stacking classifier\n",
    "stackingCLF = StackingClassifier(\n",
    "    estimators=est_rs, final_estimator=mylr, stack_method=\"auto\", cv=3\n",
    ")\n",
    "stackingCLF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kWwDcj8Pau_"
   },
   "source": [
    "Fit all tuned boosting models to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 426,
     "status": "ok",
     "timestamp": 1667247137408,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "8uWlHXP_Pa56",
    "outputId": "3f403b31-64b6-4839-c9bb-2dba7201dcd6"
   },
   "outputs": [],
   "source": [
    "# Create a tuned AdaBoost Classifier\n",
    "AdaBoost_tuned = AdaBoostClassifier(learning_rate=0.1, n_estimators=10)\n",
    "\n",
    "# Create a tuned Gradient Boosting Classifier\n",
    "GB_tuned = GradientBoostingClassifier(learning_rate=0.1, n_estimators=10)\n",
    "\n",
    "# Create a tuned XGBoost Classifier\n",
    "XGB_tuned = xgb.XGBClassifier(learning_rate=0.25, n_estimators=10)\n",
    "\n",
    "# train boosting models\n",
    "AdaBoost_tuned.fit(X_train, y_train)\n",
    "GB_tuned.fit(X_train, y_train)\n",
    "XGB_tuned.fit(X_train, y_train)\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGd1OLi4TVEa"
   },
   "source": [
    "Lastly, we compare all models with an ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIj1VgwCTkYr"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667247142755,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "uZdIjxOaTVYO",
    "outputId": "6d42c99a-9b34-4273-d73b-ad1b429e37a7"
   },
   "outputs": [],
   "source": [
    "# predicted probabilities generated by models\n",
    "y_pred_probaStack = stackingCLF.predict_proba(X_test)  # stacking\n",
    "y_pred_probaRF = bagmodel_tuned.predict_proba(X_test)  # RF\n",
    "y_pred_probaAdB = AdaBoost_tuned.predict_proba(X_test)  # AdaBoost\n",
    "y_pred_probaGb = GB_tuned.predict_proba(X_test)  # `GradBoost`\n",
    "y_pred_probaXGB = XGB_tuned.predict_proba(X_test)  # XGBoost\n",
    "\n",
    "# Stacking ROC dependencies\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probaStack[:, 1])\n",
    "auc = round(roc_auc_score(y_test, y_pred_probaStack[:, 1]), 4)\n",
    "\n",
    "# RF ROC dependencies\n",
    "fpr_RF, tpr_RF, _ = roc_curve(y_test, y_pred_probaRF[:, 1])\n",
    "auc_RF = round(roc_auc_score(y_test, y_pred_probaRF[:, 1]), 4)\n",
    "\n",
    "# AdaBoost ROC dependencies\n",
    "fpr_AB, tpr_AB, _ = roc_curve(y_test, y_pred_probaAdB[:, 1])\n",
    "auc_AB = round(roc_auc_score(y_test, y_pred_probaAdB[:, 1]), 4)\n",
    "\n",
    "# Gradient Boost ROC dependencies\n",
    "fpr_GB, tpr_GB, _ = roc_curve(y_test, y_pred_probaGb[:, 1])\n",
    "auc_GB = round(roc_auc_score(y_test, y_pred_probaGb[:, 1]), 4)\n",
    "\n",
    "# XGB ROC dependencies\n",
    "fpr_XGB, tpr_XGB, _ = roc_curve(y_test, y_pred_probaXGB[:, 1])\n",
    "auc_XGB = round(roc_auc_score(y_test, y_pred_probaXGB[:, 1]), 4)\n",
    "\n",
    "# RF Model\n",
    "plt.plot(fpr_RF, tpr_RF, label=\"RF, auc=\" + str(auc_RF))\n",
    "# Stacking Model\n",
    "plt.plot(fpr, tpr, label=\"StackM, auc=\" + str(auc))\n",
    "# AdaBoost Model\n",
    "plt.plot(fpr_AB, tpr_AB, label=\"AdaB, auc=\" + str(auc_AB))\n",
    "# `GradBoost` Model\n",
    "plt.plot(fpr_GB, tpr_GB, label=\"GB, auc=\" + str(auc_GB))\n",
    "# XGBoost Model\n",
    "plt.plot(fpr_XGB, tpr_XGB, label=\"XGB, auc=\" + str(auc_XGB))\n",
    "\n",
    "# Random guess model\n",
    "plt.plot(fpr, fpr, \"--\", label=\"Random\")\n",
    "plt.title(\"ROC\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlabel(\"FPR\")\n",
    "\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srS8aWQ6VpLZ"
   },
   "source": [
    "Great! All models performed reasonably well with some models exceeding 70% AUC. Keep in mind that we did not explore all hyperparameters for models, but this is a good enough starting point to create your own models and explore the hyperparameter space and effect on model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09C5mFT1Wk_p"
   },
   "source": [
    "## **3. Conclusion**\n",
    "\n",
    "This lesson explored the last ensemble learning algorithm, namely gradient boosting in more detail after briefly mentioning it in Module 3. We compared all ensemble learning algorithms covered in this module in a common classification problem and obtained results significantly better than a no-skill model, thus showing the added value in using the combined predictive power of weak learners. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUpGiSBfz-ZI"
   },
   "source": [
    "**References**\n",
    "\n",
    "1. Murphy, Kevin P. *Probabilistic Machine Learning: An Introduction.* MIT Press, 2022.\n",
    "2. University of California, Irvine Machine Learning Repository. \"Real Estate Valuation Data Set.\" https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "Copyright 2023 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
