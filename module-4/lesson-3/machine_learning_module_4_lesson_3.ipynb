{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "S8JSeOadQ51U"
   },
   "source": [
    "\n",
    "## MACHINE LEARNING IN FINANCE\n",
    "MODULE 4 | LESSON 3\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "grgJcZUXQ-Z_"
   },
   "source": [
    "# **BOOSTING METHODS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dhhrLQ4mRXks"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  25 minutes |\n",
    "|**Prior Knowledge** | Classification, Decision Trees  |\n",
    "|**Keywords** |Sampling with replacement, Performance, Amount of Say  |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "LHkaHhZ8RQo_"
   },
   "source": [
    "*The previous lesson covered ensemble learning methods such as bagging and stacking and also applied these methods to a problem in finance. This lesson will cover another ensemble learning method called adaptive boosting in detail.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "v2L4DucfEgAC"
   },
   "source": [
    "## **1. Introduction**\n",
    "\n",
    "Boosting was briefly discussed in Module 3, Lesson 3, and we will add to that in this lesson. Boosting, like a random forest, uses many weak learners to improve the end result. Remember that a weak learner is a learner that performs slightly better than a no-skill model. The difference with boosting and a random forest model is that the weak learners are **not** independent. These learners are built sequentially, i.e., a weak learner \"improves\" upon the learner before it. This improvement happens with the next learner focusing on the previous weak learner's errors. This is the fundamental concept of boosting. This lesson will introduce the reader to *adaptive* boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "KYbv1BAHGuwU"
   },
   "source": [
    "## **2. Adaptive Boosting**\n",
    "\n",
    "Adaptive boosting or AdaBoost is a popular algorithm and was among the earliest boosting models introduced. The sequential development of this model occurs by adjusting the weights or importance of the previous learner's predictions. Higher weights are given to predictions that deviate from the actual value and lower weights to the predictions that are close to the actual value. In other words, more weighting is assigned to incorrect predictions, or errors are corrected iteratively. AdaBoost can work on both classification and regression problems. One should keep in mind that AdaBoost is not restricted to tree-based algorithms; however, the weak learners or base learners are homogeneous in that they must all be of the same family or class of models. As with random forests, the majority vote is the final decision. However, with AdaBoost, not all base learners have equal weight or \"say\" on the vote. We will use the Taiwan Credit Card default dataset to explain how AdaBoost works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "zdUI9KwbWct6"
   },
   "outputs": [],
   "source": [
    "# import any libraries needed to create dataset\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier  # , `DecisionTreeRegressor`\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ZLmSnBSKCaUE"
   },
   "source": [
    "The dataset is used to identify default payments for credit cards in Taiwan. The Target class is binary such that 1 refers to a client that defaults on their credit repayments and 0 otherwise. There are 22 predictors in this dataset. More information on this dataset can be found at https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients . We import the dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "byxKmHaoT1O3"
   },
   "outputs": [],
   "source": [
    "loc = \"../../data\"  # specify location of dataset\n",
    "data4lesson = pd.read_excel(loc + \"/Copy of default of credit card clients.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1657741019548,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "AQck1fxTyBin",
    "outputId": "d4b1b75c-6d1c-4942-993b-f6e206594a0a"
   },
   "outputs": [],
   "source": [
    "# Make ID column the index\n",
    "data4lesson.set_index(\"ID\", inplace=True, drop=True)\n",
    "data4lesson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "5APRZhb9WYs4"
   },
   "outputs": [],
   "source": [
    "# Rename the target column to 'Class'\n",
    "data4lesson.rename(columns={\"default payment next month\": \"Class\"}, inplace=True)\n",
    "\n",
    "# keep predictors in a separate list\n",
    "feats = list(data4lesson.columns[0 : data4lesson.shape[1] - 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "eLVQSMuIx-XH"
   },
   "source": [
    "Check the dimension of the data and the frequency of each target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 369,
     "status": "ok",
     "timestamp": 1657741046409,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "B8jpCaM4YL4X",
    "outputId": "a7d7fc1f-c3c6-4ccb-8a63-cd6c79d35625"
   },
   "outputs": [],
   "source": [
    "print(\"Rows {} columns {}\".format(data4lesson.shape[0], data4lesson.shape[1]))\n",
    "data4lesson[\"Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "KjiVVVGEjtaV"
   },
   "source": [
    "Let's begin the steps for AdaBoost.\n",
    "\n",
    "### **2.1 Step 1: Sample Weights**\n",
    "\n",
    "The first step for AdaBoost is to perform random sampling with replacement on our dataset. To do this, we initially assign each observation an equal weight, i.e., 1/N for N being the number of observations. To explain the process of random sampling, we will insert a cumulative column of the weights called `RangeSelect` to illustrate the idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 449,
     "status": "ok",
     "timestamp": 1657741099244,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "MoZPfJ6XYdSL",
    "outputId": "94f3d1ff-7cae-459a-ca37-0a90e98069aa"
   },
   "outputs": [],
   "source": [
    "# Assign equal weights to all samples\n",
    "N = len(data4lesson)\n",
    "data4lesson[\"probR1\"] = 1 / (1.0 * N)\n",
    "data4lesson[\"RangeSelect\"] = data4lesson[\"probR1\"].cumsum()\n",
    "# Show last 5 columns\n",
    "data4lesson[[\"PAY_AMT5\", \"PAY_AMT6\", \"Class\", \"probR1\", \"RangeSelect\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "KXSdSwyTzdrz"
   },
   "source": [
    "Now that we have the cumulative column, we are able to explain the method of sampling. \n",
    "\n",
    "### **2.2 Step 2: Sampling with Replacement**\n",
    "\n",
    "Think of each value in the `RangeSelect` column as an upper limit with the lower limit being the value in the observation, or row, before it. For example, the output above ID 2 would have a `RangeSelect` of $[0.000033, 0.000067)$. Now draw a random number in the range $[0, 1]$. Suppose this number is $0.000043$. We therefore add ID 2 to our sample since the random number of $0.000043$ is in the range $[0.000033, 0.000067)$, which corresponds to ID 2. If our random number drawn was $0.00014$, then ID 5 would be added to the sample since $0.00014$ is in the range $[0.000133, 0.000167)$. One may notice that observations with large ranges would have a higher chance of being selected. Hence, large weights or `probR1` values increase the chances of being sampled, which is crucial for the AdaBoost algorithm. Fortunately, Python has a more elegant and efficient way of performing random sampling with replacement, so we won't need the `RangeSelect` column anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "5Q1RSuWiC76z"
   },
   "outputs": [],
   "source": [
    "# Drop `RangeSelect` column\n",
    "data4lesson.drop(\"RangeSelect\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "mFHJgYmOFJDq"
   },
   "source": [
    "We'll use the pandas *sample* function to sample with replacement based on the weights column `probR1`. The amount of samples is the same as N, that is, the same number of observations as our original dataset. There will be duplicate observations in this new dataset since the sampling is done with replacement.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "68cUjBRZZ4b6"
   },
   "outputs": [],
   "source": [
    "# Random selection based on weights\n",
    "# specify seed in case we wish to replicate\n",
    "# the Random Sampling with replacement\n",
    "random.seed(10)\n",
    "\n",
    "data4lesson1 = data4lesson.sample(\n",
    "    len(data4lesson), replace=True, weights=data4lesson[\"probR1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "6JiLRGdkGvGn"
   },
   "source": [
    "### **2.3 Step 3: Fit a Base Learner**\n",
    "\n",
    "The next step in AdaBoost is to fit a weak classifier or base learner to the samples. For this example, we will use decision trees as our base learners. We'll call this learner *weak_l1* and fit it to the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1657741394555,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "ODWoXeWjHamJ",
    "outputId": "2f0bc47d-faf3-4376-f9d1-2239689909f5"
   },
   "outputs": [],
   "source": [
    "# Since it is a stump the max depth is 1\n",
    "weak_l1 = DecisionTreeClassifier(random_state=10, max_depth=1)\n",
    "\n",
    "X = data4lesson1.loc[:, feats]\n",
    "y = data4lesson1[\"Class\"]\n",
    "\n",
    "weak_l1.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7SFs80FUX35y"
   },
   "source": [
    "We should be able to view the flow diagram of the stump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 1979,
     "status": "ok",
     "timestamp": 1657741471022,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "0J2AUxq9X4Ot",
    "outputId": "1197da36-815b-4b7c-cee6-6a1aa665d588"
   },
   "outputs": [],
   "source": [
    "# visualize the stump\n",
    "# features we used\n",
    "\n",
    "fn = feats\n",
    "\n",
    "# labels of the target class\n",
    "\n",
    "cn = [\"0\", \"1\"]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(25, 20))\n",
    "_ = tree.plot_tree(weak_l1, feature_names=fn, class_names=cn, filled=True)\n",
    "\n",
    "# save the figure to file\n",
    "\n",
    "# fig.savefig('imagename.png')\n",
    "fig.savefig(\"decistion_tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "jyBUdwRnI8x6"
   },
   "source": [
    "**Figure 1: The Decision Stump of the Base Learner at the First Iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "0PYTzWYbZBzv"
   },
   "source": [
    "Notice the base learner found *Pay_0* to be the strongest predictor to split on. Let us compare the predictions to the actual values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "o0ZH8EUmI67A"
   },
   "outputs": [],
   "source": [
    "data4lesson1[\"pred1\"] = weak_l1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 581,
     "status": "ok",
     "timestamp": 1657741480806,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "_8lI2uZPJZiN",
    "outputId": "84fb8ab2-8ffb-43aa-d720-da7f84b9351d"
   },
   "outputs": [],
   "source": [
    "data4lesson1[[\"Class\", \"pred1\", \"probR1\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "z-EoA_FSKr4u"
   },
   "source": [
    "### **2.4 Step 4: Calculate Performance**\n",
    "\n",
    "The snapshot of just the first five rows shows one misclassification for ID 16973. The number of misclassifications will determine the *amount of say* or *performance* of this classifier. \n",
    "The formula for the amount of say is\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\text{Amount of say} = \\frac{1}{2}ln\\left( \\frac{\\text{1 - Total Error}}{\\text{Total Error}}\\right)\n",
    "\\tag{1.1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where the **Total Error** is simply the sum of weights of misclassified observations. The Total Error for the base classifier at our first iteration, i.e., weak_l1 is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 409,
     "status": "ok",
     "timestamp": 1657741622329,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "Ta8FLNatWrFr",
    "outputId": "68a365f6-2f8e-4b9e-8973-6ad160199ef4"
   },
   "outputs": [],
   "source": [
    "# Total error of weak_l1\n",
    "E_1 = round(\n",
    "    np.sum(\n",
    "        np.where(\n",
    "            data4lesson1[\"Class\"] != data4lesson1[\"pred1\"], data4lesson1[\"probR1\"], 0\n",
    "        )\n",
    "    ),\n",
    "    5,\n",
    ")\n",
    "\n",
    "E_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "DCOAwF2paO1H"
   },
   "source": [
    "The Amount of say in (1.1) against the total error in general is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 1048,
     "status": "ok",
     "timestamp": 1657741632874,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "2sYRSS26axod",
    "outputId": "7b3a30da-50b0-4a48-f7d6-3d282820e381"
   },
   "outputs": [],
   "source": [
    "n_pts = 100\n",
    "TE = 0.01 + np.arange(n_pts) * (1 - 0.01) / n_pts\n",
    "AoS = 0.5 * np.log((1 - TE) / TE)\n",
    "plt.plot(TE, AoS)\n",
    "plt.ylabel(\"Amount of Say\")\n",
    "plt.xlabel(\"Total Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "yKCc0i26c_8m"
   },
   "source": [
    "**Figure 2: Performance or Amount of Say against Total Error for a Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dhrhm5MMdLJP"
   },
   "source": [
    "From figure 2, we see that for close to perfect classifications, the base classifier will have a performance that tends to infinity. For almost all incorrect classifications, the learner will have performance that tends to negative infinity. For a Total Error of 0.5, this results in a performance of zero. If all samples had equal weights, this would be equivalent to a no-skill classifier. In practice, a small error term is added to to the Total Error to avoid the case when the Total Error is zero. Our first base learner has a Total Error of 0.1796, which results in a performance of 0.7596. Remember that AdaBoost focuses on the misclassifications by adjusting the weights in an iterative manner. Let us now look into this process of adjusting the weights. \n",
    "\n",
    "\n",
    "For the observations that were misclassified, the formula to adjust the weights is given by,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\text{Adjusted weight} = \\text{Current Weight} \\times e^{\\text{performance}}\n",
    "\\tag{1.2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "similarly the formula for adjusting the weights of the correct classification is,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\text{Adjusted weight} = \\text{Current Weight} \\times e^{\\text{-performance}}\n",
    "\\tag{1.3}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Notice the difference between (1.2) and (1.3) is the sign of the exponent. This implies that correct classifications are downweighted whereas misclassifications are adjusted to larger weights. Applying (1.2) and (1.3) to the weights creates the new weights shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "7qqjpo_Hc-E1"
   },
   "outputs": [],
   "source": [
    "# performance of weak_l1\n",
    "p1 = 0.5 * np.log((1 - E_1) / E_1)\n",
    "# Call the new weights `probR2` since it is the second iteration\n",
    "data4lesson1[\"probR2\"] = np.where(\n",
    "    data4lesson1[\"Class\"] != data4lesson1[\"pred1\"],\n",
    "    data4lesson1[\"probR1\"] * np.exp(p1),  # misclassifications (1.2)\n",
    "    data4lesson1[\"probR1\"] * np.exp(-p1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "TcWGDvHLtDZy"
   },
   "outputs": [],
   "source": [
    "data4lesson1[[\"Class\", \"pred1\", \"probR1\", \"probR2\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "HeehapuDio0y"
   },
   "source": [
    "We can see the correct classifications have new weights that are lower than the original weights of $3.3\\times 10^{-5}$ whereas the misclassifications have weights that have increased. Remember that an increase in weight would increase the chances of choosing that observation when sampling with replacement. One thing to take note of is that the new weights are normalized such that they sum to 1 as were the original weights in column `probR1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "ea3UWPK3nU1V"
   },
   "outputs": [],
   "source": [
    "# normalize the weights\n",
    "data4lesson1[\"probR2\"] = data4lesson1[\"probR2\"] / data4lesson1[\"probR2\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "h3c1E7cLk5I2"
   },
   "source": [
    "This brings us back to Step 1 with a new set of weights to repeat steps 2 to 4. The number of times we would repeat these cycles would depend on the number of weak classifiers we specify. This is the iterative process that AdaBoost follows to develop the final model. Ideally, we would want to continue this process until all observations are classified correctly, but this can allow for overfitting. Hyperparameter tuning should be taken into consideration. To illustrate the final prediction of the model, let's assume we had chosen 5 base learners and they each had the following performance or Amount of Say and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 421,
     "status": "ok",
     "timestamp": 1657742257656,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "ZUUn3obDmIDv",
    "outputId": "8333772f-7506-428f-83a3-2aa4a3daec2c"
   },
   "outputs": [],
   "source": [
    "dat = [[0.7596, 1], [0.7821, 1], [0.79019, 1], [0.8241, 0], [0.8410, 0]]\n",
    "model_summary = pd.DataFrame(data=dat, columns=[\"performance\", \"prediction\"])\n",
    "model_summary.index.name = \"iteration\"\n",
    "model_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "djXr2o3xIgRo"
   },
   "source": [
    "The hypothetical result above over 5 iterations shows 3 base learners predicting class 1 and 2 base learners predicting class 0. The final vote is based on the sum of performance for each class voted, i.e., instead of the counts being used, the sum of the performance is used. The calculation below shows that class 1 would be favored since the sum of the performance for base learners in favor of that class is 2.332, which is greater than the sum of the performance for base learners in favor of class 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 390,
     "status": "ok",
     "timestamp": 1657742138551,
     "user": {
      "displayName": "Darell Moodley",
      "userId": "09785417463054650992"
     },
     "user_tz": -120
    },
    "id": "-0rrfpIaohEh",
    "outputId": "b5737a0d-d959-497f-eb5e-0b2f1629b063"
   },
   "outputs": [],
   "source": [
    "class_0 = np.sum(\n",
    "    np.where(model_summary[\"prediction\"] == 0, model_summary[\"performance\"], 0)\n",
    ")\n",
    "class_1 = np.sum(\n",
    "    np.where(model_summary[\"prediction\"] == 1, model_summary[\"performance\"], 0)\n",
    ")\n",
    "\n",
    "print(\"Weight of Votes \")\n",
    "print(\"Class 0: {} Class 1: {}\".format(class_0, class_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b9GBu92_IdoM"
   },
   "source": [
    "Something to keep in mind is that the example above showed the updating procedure on the entire dataset whereas in practice we would be using the training set to update and develop AdaBoost. We've also shown the process using weak decision stumps, but there is evidence to support more advanced learners being effective (Chuan et al. 3). \n",
    "\n",
    "Boosting has found success in the data science field especially in Kaggle competitions. There is one more boosting technique we will cover in the next lesson called gradient boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "XPiD96W5o34H"
   },
   "source": [
    "## **3. Conclusion**\n",
    "\n",
    "This lesson focused on adaptive boosting and showed the iterative updating procedure to derive the final model and prediction. The procedure was shown for a classification problem, namely predictive credit card payment defaults. In the next lesson, we'll look at another boosting algorithm called gradient boosting and compare all ensemble learning methods covered in this module to a common problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8nJxFBlCRLID"
   },
   "source": [
    "**References**\n",
    "\n",
    "1. Chuan, Yijian, et al. \"The Success of AdaBoost and Its Application in Portfolio Management.\" *arXiv*, 2021, pp. 1–6.\n",
    "2. University of California, Irvine Machine Learning Repository. \"Default of Credit Card Clients.\" https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "Copyright © 2022 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n",
    "<span style='color: transparent; font-size:1%'>All rights reserved WQU WorldQuant University QQQQ</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da0c9a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "Copyright 2024 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
