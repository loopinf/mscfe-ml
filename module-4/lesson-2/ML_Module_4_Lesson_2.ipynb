{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8JSeOadQ51U"
   },
   "source": [
    "\n",
    "## MACHINE LEARNING IN FINANCE\n",
    "MODULE 4 | LESSON 2\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grgJcZUXQ-Z_"
   },
   "source": [
    "# **ENSEMBLE LEARNING IN PRACTICE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhhrLQ4mRXks"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  15 minutes |\n",
    "|**Prior Knowledge** | Bagging, Stacking, SVM, Decision Trees, Na√Øve Bayes, Logistic regression, ROC curve, AUC.  |\n",
    "|**Keywords** |Base Learners, Meta Model.  |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHkaHhZ8RQo_"
   },
   "source": [
    "*In the previous lesson, we introduced two ensemble learning methods, namely bagging and stacking. This lesson will put these two methods into practice using Python.*<span style='color: transparent; font-size:1%'>All rights reserved WQU WorldQuant University QQQQ</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibRNURi5jIFd"
   },
   "source": [
    "## **1. Introduction**\n",
    "\n",
    "In this predictive problem, we apply ensemble methods to predict whether the Luxembourg index (LUXXX) exceeds a return of 0.25% in any direction. The predictors are a combination of country indices and technical indicators. Let's begin by importing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "pIxEVFyLszGo",
    "outputId": "4bbfca4e-1149-4121-bf46-cf9fab821924"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# loc = \"ENTER YOUR FULL PATH TO LOCATION OF DATA FILE HERE\"\n",
    "\n",
    "data_df = pd.read_csv(\"../../data/MScFE 650 MLF GWP Data.csv\")\n",
    "# Convert string to datetime\n",
    "data_df[\"Date\"] = pd.to_datetime(data_df[\"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAN6RZhktYdU"
   },
   "source": [
    "We create the target variable as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r17LWpr0tXpY"
   },
   "outputs": [],
   "source": [
    "# Set Target Index for predicting\n",
    "target_ETF = \"LUXXX\"\n",
    "\n",
    "# Use returns instead of prices for other Indices\n",
    "# Other Indices used as Index_features\n",
    "ETF_features = data_df.loc[:, ~data_df.columns.isin([\"Date\", target_ETF])].columns\n",
    "data_df[ETF_features] = data_df[ETF_features].pct_change()\n",
    "\n",
    "data_df[target_ETF + \"_returns\"] = data_df[target_ETF].pct_change()\n",
    "\n",
    "# Create Target Column.\n",
    "# Shift period for target column\n",
    "data_df[target_ETF + \"_returns\" + \"_Shift\"] = data_df[target_ETF + \"_returns\"].shift(-1)\n",
    "\n",
    "# Strategy to take long position for anticipated returns of 0.5%\n",
    "data_df[\"Target\"] = np.where(\n",
    "    (data_df[target_ETF + \"_returns_Shift\"].abs() > 0.025), 1, 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xobah8rdtxKW"
   },
   "source": [
    "Create the predictors below, namely four country indices and three technical indicators: slow to fast moving average ratio (SMA_ratio), relative strength index (RSI), and rate of change (RC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ull3Cwmvtxh3"
   },
   "outputs": [],
   "source": [
    "# Four country indices used.\n",
    "feats = [\"MSCI KOREA\", \"MSCI DENMARK\", \"MSCI FRANCE\", \"MSCI NORWAY\"]\n",
    "\n",
    "# creating the technical indicators\n",
    "data_df[\"SMA_5\"] = data_df[target_ETF].rolling(5).mean()\n",
    "data_df[\"SMA_15\"] = data_df[target_ETF].rolling(15).mean()\n",
    "data_df[\"SMA_ratio\"] = data_df[\"SMA_15\"] / data_df[\"SMA_5\"]\n",
    "\n",
    "# Can drop SMA columns since not needed anymore.\n",
    "data_df.drop([\"SMA_5\", \"SMA_15\"], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# shift the price of the target by 1 unit previous in time\n",
    "data_df[\"Diff\"] = data_df[target_ETF] - data_df[target_ETF].shift(1)\n",
    "data_df[\"Up\"] = data_df[\"Diff\"]\n",
    "data_df.loc[(data_df[\"Up\"] < 0), \"Up\"] = 0\n",
    "\n",
    "data_df[\"Down\"] = data_df[\"Diff\"]\n",
    "data_df.loc[(data_df[\"Down\"] > 0), \"Down\"] = 0\n",
    "data_df[\"Down\"] = abs(data_df[\"Down\"])\n",
    "\n",
    "data_df[\"avg_5up\"] = data_df[\"Up\"].rolling(5).mean()\n",
    "data_df[\"avg_5down\"] = data_df[\"Down\"].rolling(5).mean()\n",
    "\n",
    "data_df[\"avg_15up\"] = data_df[\"Up\"].rolling(15).mean()\n",
    "data_df[\"avg_15down\"] = data_df[\"Down\"].rolling(15).mean()\n",
    "\n",
    "data_df[\"RS_5\"] = data_df[\"avg_5up\"] / data_df[\"avg_5down\"]\n",
    "data_df[\"RS_15\"] = data_df[\"avg_15up\"] / data_df[\"avg_15down\"]\n",
    "\n",
    "data_df[\"RSI_5\"] = 100 - (100 / (1 + data_df[\"RS_5\"]))\n",
    "data_df[\"RSI_15\"] = 100 - (100 / (1 + data_df[\"RS_15\"]))\n",
    "\n",
    "data_df[\"RSI_ratio\"] = data_df[\"RSI_5\"] / data_df[\"RSI_15\"]\n",
    "\n",
    "# Can drop RS Calc columns columns\n",
    "data_df.drop(\n",
    "    [\"Diff\", \"Up\", \"Down\", \"avg_5up\", \"avg_5down\", \"avg_15up\", \"avg_15down\"],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "data_df[\"RC\"] = data_df[target_ETF].pct_change(periods=15)\n",
    "\n",
    "# all_feats\n",
    "feats.append(\"SMA_ratio\")\n",
    "feats.append(\"RSI_ratio\")\n",
    "feats.append(\"RC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxilagldljNu"
   },
   "source": [
    "Now that we have our data, we can apply the ensemble methods beginning with bagging.\n",
    "\n",
    "## **2. Bagging with Random Forest**\n",
    "\n",
    "An example of a classifier that uses the bagging algorithm is the random forest classifier. Random forest is a tree-based algorithm; therefore, the weak learners are decision trees. The \"random\" applies more to the random features allocated to each decision tree; hence, not all features are used by the weak learners. The different subset of features and different samples used by the decision trees also result in uncorrelated trees, which improves the performance of the algorithm. The final outcome is simply a majority vote among the weak learners. For instance, if there are five decision trees in the random forest and three trees predict class 1 while two trees predict class 0, the random forest classification would be class 1 due to the majority prediction. We begin by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fxy8SnQ0z0n_"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OK2UUZDtzHXn"
   },
   "source": [
    "Perform the train/test split of 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ty6oHlkM3AHK"
   },
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "# Train/Test split. No NaNs in the data.\n",
    "NoNaN_df = data_df.dropna()\n",
    "X = NoNaN_df[feats]\n",
    "\n",
    "X = X.iloc[:, :]  # .values\n",
    "y = NoNaN_df.loc[:, \"Target\"]  # .values\n",
    "\n",
    "del NoNaN_df\n",
    "\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afP28X-EzHmo"
   },
   "source": [
    "We now create the random forest classifier. We call it our bag model since it is our bagging classifier. Among the hyperparameters specified is n_estimators, which specifies the number of trees. To reduce the time taken to train the classifier, we use a small number of 10 trees. A large number of trees will not lead to overfitting according to James (341)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHydx3PoyEU8"
   },
   "outputs": [],
   "source": [
    "bagmodel = RandomForestClassifier(n_estimators=10, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlbTgz_u3enz"
   },
   "source": [
    "We train the model and look at its accuracy on the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E7Y35fVgy59Y",
    "outputId": "c3c45563-0c96-42d3-89bc-4296907d8c50"
   },
   "outputs": [],
   "source": [
    "bagmodel.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on train set: %0.4f\" % (bagmodel.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: %0.4f\" % (bagmodel.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56ok1JLG_CTC"
   },
   "source": [
    "The big difference between the accuracies on the train and test set indicate over-fitting. We will look at optimal hyperparameter values to improve performance such as *min_samples_split*, which is the minimum number of samples required to split an internal node. We've mentioned *max_depth* in Lesson 3 of Module 3 for decision trees, which is the same for random forests. With limited computational power and quick turnaround times, we will explore just a few samples for these parameters. We use `GridSearchCV` as in Lesson 2 of Module 5 to find the optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_3RxUZR4KGw"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5FFFdpyR_Cm1",
    "outputId": "161d8f41-e230-4a64-d83b-0d111ed5e970"
   },
   "outputs": [],
   "source": [
    "# defining parameter range\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10],\n",
    "    \"max_depth\": [2, 3, 4],\n",
    "    \"min_samples_split\": [2, 4, 8],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=8), param_grid, refit=True, verbose=3, cv=3\n",
    ")\n",
    "\n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0z3HCcK5rSH",
    "outputId": "61e065a7-9518-4612-8d5e-9889dd2812ae"
   },
   "outputs": [],
   "source": [
    "# print best parameter after tuning\n",
    "print(grid.best_params_)\n",
    "\n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_S6gX754gr0"
   },
   "source": [
    "Apply optimal parameters and check if test score improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GEvnGfjL4ZYk",
    "outputId": "0d0f0d0a-e870-4d37-9487-0e18d342a735"
   },
   "outputs": [],
   "source": [
    "# Train with Tuned RF\n",
    "# Create a tuned RF Classifier\n",
    "bagmodel_tuned = RandomForestClassifier(\n",
    "    n_estimators=10,\n",
    "    max_depth=grid.best_params_[\"max_depth\"],\n",
    "    min_samples_split=grid.best_params_[\"min_samples_split\"],\n",
    ")\n",
    "\n",
    "bagmodel_tuned.fit(X_train, y_train)\n",
    "print(\"Accuracy on test set: %0.4f\" % (bagmodel_tuned.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWcZZ7x66Vi0"
   },
   "source": [
    "There is about a 3% increase in accuracy by just exploring a small hyperparameter space. The small hyperparameter space is considered to keep computational time within reasonable limits. Students are encouraged to explore more hyperparameter options. Another measure of model performance to explore is the out-of-bag error, which is essentially the test error of a bagged model (James 342). Keep in mind that accuracy is based on the predicted classes, whereas the ROC AUC is based on predicted scores. We therefore plot the ROC Curve as in the previous modules to compare to the random guess or no-skill model. From the output below, there is a clear benefit to the Random forest model over the no-skill model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "GyJiZ6s88RPe",
    "outputId": "1a8b905a-9140-4e5e-c288-bc888ee8cc8c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Performance\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# predicted probabilities generated by tuned classifier\n",
    "y_pred_proba = bagmodel_tuned.predict_proba(X_test)\n",
    "\n",
    "# RF ROC dependencies\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba[:, 1])\n",
    "auc = round(roc_auc_score(y_test, y_pred_proba[:, 1]), 4)\n",
    "\n",
    "# RF Model\n",
    "plt.plot(fpr, tpr, label=\"RF, auc=\" + str(auc))\n",
    "\n",
    "# Random guess model\n",
    "plt.plot(fpr, fpr, \"-\", label=\"Random\")\n",
    "plt.title(\"ROC\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlabel(\"FPR\")\n",
    "\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful feature from bagging is the *Out of Bag Score*. Refer to the video below for an explanation on this feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "\n",
    "VimeoVideo(\"785136013\", h=\"2e436e22bd\", width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Access Video Transcript here](https://drive.google.com/file/d/17OwwNQEQHVrRxQveR6_h9nSjw2BzEyNP/view?usp=share_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znTmzFWs5VM3"
   },
   "source": [
    "## **3. Stacking Model**\n",
    "\n",
    "The stacking model depends on the base models and meta model. For this illustration, we use the Gaussian Na√Øve Bayes, decision tree, and SVM classifiers as the base models. For the meta model, we use the simplistic logistic regression model. This is to show how we can leverage the logistic regression model using stacking. Let's import the necessary libraries for each of them. The names are well described and should be easy to match to the respective model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBaVeIW3zpXF"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTzTOdC64IOL"
   },
   "source": [
    "We define three base classifiers without any hyperparameter specifications, i.e., we use the default settings other than the kernel for SVC. This is to show the predictive power of stacking without optimizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVGJ8mqb4I30"
   },
   "outputs": [],
   "source": [
    "clf1 = DecisionTreeClassifier(random_state=2)  # Decision Tree\n",
    "\n",
    "clf2 = SVC(kernel=\"rbf\", random_state=2)  # Support Vector Classifier\n",
    "\n",
    "clf3 = GaussianNB()  # Gaussian Naive Bayes\n",
    "\n",
    "est_rs = [(\"DTree\", clf1), (\"SVM\", clf2), (\"NB\", clf3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vTTvAKr5AM2"
   },
   "source": [
    "Define the meta model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvr_ms6p4_h3"
   },
   "outputs": [],
   "source": [
    "mylr = LogisticRegression(random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gi4omz7M8bmh"
   },
   "outputs": [],
   "source": [
    "# creating a stacking classifier\n",
    "stackingCLF = StackingClassifier(\n",
    "    estimators=est_rs, final_estimator=mylr, stack_method=\"auto\", cv=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wrHUKa13JUU"
   },
   "source": [
    "The stacking model is ready to go, and now, we need to train it on the training data. We also show the training score since we want to compare it to each of its base learners below as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jfMsEpLA_5mu",
    "outputId": "ca02f675-673c-40fc-882f-2af09fd4ed05"
   },
   "outputs": [],
   "source": [
    "stackingCLF.fit(X_train, y_train)\n",
    "acc_score = stackingCLF.score(X_train, y_train)\n",
    "round(acc_score, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gaFSQdlZ9aqu",
    "outputId": "4f7d6037-1d4c-473c-af12-95a7cecf9f94"
   },
   "outputs": [],
   "source": [
    "# Let's run the individual base learners and stacking `clf` to compare\n",
    "for iterclf, iterlabel in zip([clf1, clf2, clf3], [\"DTree\", \"SVM\", \"NB\"]):\n",
    "    scores = model_selection.cross_val_score(\n",
    "        iterclf, X_train, y_train, cv=3, scoring=\"accuracy\"\n",
    "    )\n",
    "    print(\"accuracy: %0.3f %s \" % (scores.mean(), iterlabel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZunD4ng0cBa"
   },
   "source": [
    "It is quite evident that the stacking model outperforms all of its base learners and shows the benefit of ensemble learning in this case. It is worth comparing the performance of the two ensemble learning models used in this lesson; therefore, we look at the ROC curve and AUC of the stacking model versus the random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "3kWzFLfO9fQ5",
    "outputId": "58d50f65-70dc-4ffa-882b-0e080e8c4100"
   },
   "outputs": [],
   "source": [
    "# predicted probabilities generated by tuned classifier\n",
    "y_pred_probaStack = stackingCLF.predict_proba(X_test)\n",
    "\n",
    "# Stacking Model ROC dependencies\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probaStack[:, 1])\n",
    "auc = round(roc_auc_score(y_test, y_pred_probaStack[:, 1]), 4)\n",
    "\n",
    "# RF ROC dependencies\n",
    "fpr_RF, tpr_RF, _ = roc_curve(y_test, y_pred_proba[:, 1])\n",
    "auc_RF = round(roc_auc_score(y_test, y_pred_proba[:, 1]), 4)\n",
    "\n",
    "# RF Model\n",
    "plt.plot(fpr_RF, tpr_RF, label=\"RF, auc=\" + str(auc_RF))\n",
    "# Stacking Model\n",
    "plt.plot(fpr, tpr, label=\"StackM, auc=\" + str(auc))\n",
    "\n",
    "# Random guess model\n",
    "plt.plot(fpr, fpr, \"-\", label=\"Random\")\n",
    "plt.title(\"ROC\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlabel(\"FPR\")\n",
    "\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vHsaJ-H4L7r"
   },
   "source": [
    "From the plot above, it is clear that the stacking model slightly outperforms the random forest model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCDTTE2fCeqS"
   },
   "source": [
    "## **4. Conclusion**\n",
    "\n",
    "This lesson looked at bagging and stacking in practice to predict a change of the LUXXX index by a certain threshold, i.e., a classification problem. Both these methods show added value over a no-skill model. In the next lesson, we will cover another ensemble learning method namely boosting and adaptive boosting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjmUf8v26nkl"
   },
   "source": [
    "**References**\n",
    "\n",
    "James, Gareth et al. *An Introduction to Statistical Learning: With Applications in R.* 2nd ed., Springer, 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "Copyright 2023 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
